{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravitha/LabelRadiology/blob/master/MIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arW_6_PrkIlZ",
        "colab_type": "text"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthF7sh_UZX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dicom # some machines not install pydicom\n",
        "import scipy.misc\n",
        "import numpy as np \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle as cPickle\n",
        "#import matplotlib\n",
        "#import matplotlib.pyplot as plt \n",
        "from skimage.filters import threshold_otsu\n",
        "import os\n",
        "from os.path import join as join\n",
        "import csv\n",
        "import scipy.ndimage\n",
        "#import pydicom as dicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82DTupaeUZYI",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evR_WImPaUN0",
        "colab_type": "code",
        "outputId": "0c339964-ed4b-459e-f2a6-3ecfe008cf3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcQkq4ShkC-6",
        "colab_type": "text"
      },
      "source": [
        "# Compose Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzRMqmRi0fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/drive/My Drive/CO-PO/INBreast'\n",
        "TrDataset = np.load(join(path,'Train.npy'))\n",
        "TrLabel = np.load(join(path,'TrainL.npy'))\n",
        "ValDataset= np.load(join(path,'valid.npy'))\n",
        "ValLabel = np.load(join(path,'validL.npy'))\n",
        "TDataset = np.load(join(path,'Test.npy'))\n",
        "TLabel = np.load(join(path,'TestL.npy'))\n",
        "\n",
        "Dataset = np.concatenate((TrDataset,ValDataset,TDataset),axis = 0)\n",
        "Labels = np.concatenate((TrLabel,ValLabel,TLabel),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRB7_brfjY3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "adc6159b-dbdf-4dae-adbd-2a2f3283ce0c"
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "np.save('Dataset.npy', Dataset)\n",
        "np.save('Label.npy', Labels)\n",
        "\n",
        "np.unique(Labels, return_counts=True)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256)\n",
            "(410,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crX86A3FUZZI",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXWpwOAkOVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "nb_classes = 2\n",
        "Dataset = Dataset.reshape((410,256,256,1))\n",
        "Dataset_extend = np.zeros((Dataset.shape[0],256, 256,3))\n",
        "for i in range(Dataset.shape[0]):\n",
        "    rex = np.resize(Dataset[i,:,:,:], (256, 256))\n",
        "    Dataset_extend[i,:,:,0] = rex\n",
        "    Dataset_extend[i,:,:,1] = rex\n",
        "    Dataset_extend[i,:,:,2] = rex\n",
        "Dataset = Dataset_extend\n",
        "Labels = np_utils.to_categorical(Labels, nb_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqsOUAXIlSL1",
        "colab_type": "text"
      },
      "source": [
        "# Print Data Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNitNFUBlQXT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ff98bdf2-206f-4636-deb2-a22ed19c9081"
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256, 3)\n",
            "(410, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4qn6IkmSC1",
        "colab_type": "text"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKoXcmbmUCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model1():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssCxcR7lutw",
        "colab_type": "text"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFdleZCluLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ef110c5c-6ebe-4181-9e2c-ef0472a6470d"
      },
      "source": [
        "\n",
        "# Necessary Imports\n",
        "import numpy as np \n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "print('[INFO:Applying CV]')\n",
        "skf = KFold(n_splits=5)\n",
        "skf.get_n_splits(Dataset)\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(Dataset):\n",
        "    print(['FOLD:'], fold)\n",
        "    # Construct data from indexes\n",
        "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
        "    y_train, y_test = Labels[train_index], Labels[test_index]\n",
        "    \n",
        "    # Model Construction\n",
        "    model = model1()\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "    datagen.fit(X_train)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.00001),\n",
        "              metrics=['accuracy'])\n",
        "    Stopping_Criterion = EarlyStopping(patience=30, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint('./SEfold'+str(fold)+'{epoch:08d}{val_accuracy:05f}.hdf5', monitor='loss',verbose=1,save_best_only=True)\n",
        "    model.fit_generator(datagen.flow(X_train, y_train,batch_size=10),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_test , y_test ),callbacks=[model_checkpoint, Stopping_Criterion])\n",
        "    datafilename = 'TestFold'+str(fold)+'.npy'\n",
        "    labelfilename = 'TestLabelFold'+str(fold)+'.npy'\n",
        "    np.save(datafilename,X_test)\n",
        "    np.save(labelfilename,y_test)\n",
        "    fold+=1"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO:Applying CV]\n",
            "['FOLD:'] 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 22s 672ms/step - loss: 1.0496 - accuracy: 0.2439 - val_loss: 0.9401 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.05095, saving model to ./SEfold1000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 1.0344 - accuracy: 0.2439 - val_loss: 1.0149 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: loss improved from 1.05095 to 1.03574, saving model to ./SEfold1000000020.243902.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 1.0047 - accuracy: 0.2439 - val_loss: 1.0412 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: loss improved from 1.03574 to 1.00408, saving model to ./SEfold1000000030.243902.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.9614 - accuracy: 0.2439 - val_loss: 0.9867 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: loss improved from 1.00408 to 0.96129, saving model to ./SEfold1000000040.243902.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.8855 - accuracy: 0.2470 - val_loss: 1.0283 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: loss improved from 0.96129 to 0.88624, saving model to ./SEfold1000000050.243902.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.8074 - accuracy: 0.2927 - val_loss: 0.9685 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00006: loss improved from 0.88624 to 0.80729, saving model to ./SEfold1000000060.243902.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.7539 - accuracy: 0.3933 - val_loss: 0.9210 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00007: loss improved from 0.80729 to 0.75384, saving model to ./SEfold1000000070.243902.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.6682 - accuracy: 0.6067 - val_loss: 0.9669 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00008: loss improved from 0.75384 to 0.66856, saving model to ./SEfold1000000080.243902.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.6315 - accuracy: 0.7043 - val_loss: 0.9035 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00009: loss improved from 0.66856 to 0.63184, saving model to ./SEfold1000000090.243902.hdf5\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.6116 - accuracy: 0.7165 - val_loss: 0.8893 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00010: loss improved from 0.63184 to 0.61060, saving model to ./SEfold1000000100.243902.hdf5\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5759 - accuracy: 0.7805 - val_loss: 0.8888 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00011: loss improved from 0.61060 to 0.57574, saving model to ./SEfold1000000110.243902.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.5537 - accuracy: 0.8079 - val_loss: 0.8387 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00012: loss improved from 0.57574 to 0.55381, saving model to ./SEfold1000000120.256098.hdf5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5524 - accuracy: 0.7988 - val_loss: 0.8095 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00013: loss improved from 0.55381 to 0.55123, saving model to ./SEfold1000000130.231707.hdf5\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5339 - accuracy: 0.8140 - val_loss: 0.8128 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00014: loss improved from 0.55123 to 0.53377, saving model to ./SEfold1000000140.231707.hdf5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5151 - accuracy: 0.8323 - val_loss: 0.8311 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00015: loss improved from 0.53377 to 0.51520, saving model to ./SEfold1000000150.231707.hdf5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.5105 - accuracy: 0.8384 - val_loss: 0.8536 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00016: loss improved from 0.51520 to 0.50974, saving model to ./SEfold1000000160.243902.hdf5\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.5102 - accuracy: 0.8354 - val_loss: 0.8790 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.50974\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4958 - accuracy: 0.8323 - val_loss: 0.8973 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00018: loss improved from 0.50974 to 0.49647, saving model to ./SEfold1000000180.256098.hdf5\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4984 - accuracy: 0.8232 - val_loss: 0.9422 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.49647\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4772 - accuracy: 0.8750 - val_loss: 0.9439 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00020: loss improved from 0.49647 to 0.47734, saving model to ./SEfold1000000200.243902.hdf5\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.4643 - accuracy: 0.8628 - val_loss: 0.9576 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00021: loss improved from 0.47734 to 0.46465, saving model to ./SEfold1000000210.243902.hdf5\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4604 - accuracy: 0.8750 - val_loss: 0.9470 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00022: loss improved from 0.46465 to 0.46046, saving model to ./SEfold1000000220.243902.hdf5\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4606 - accuracy: 0.8750 - val_loss: 0.9036 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00023: loss improved from 0.46046 to 0.46006, saving model to ./SEfold1000000230.243902.hdf5\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4526 - accuracy: 0.8841 - val_loss: 0.8875 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00024: loss improved from 0.46006 to 0.45196, saving model to ./SEfold1000000240.256098.hdf5\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4580 - accuracy: 0.8750 - val_loss: 0.8599 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.45196\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4474 - accuracy: 0.8720 - val_loss: 0.8352 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00026: loss improved from 0.45196 to 0.44749, saving model to ./SEfold1000000260.243902.hdf5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4306 - accuracy: 0.8963 - val_loss: 0.8197 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00027: loss improved from 0.44749 to 0.43008, saving model to ./SEfold1000000270.268293.hdf5\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4463 - accuracy: 0.8872 - val_loss: 0.8176 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.43008\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.4472 - accuracy: 0.8811 - val_loss: 0.8236 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.43008\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4413 - accuracy: 0.8780 - val_loss: 0.8097 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.43008\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4385 - accuracy: 0.8841 - val_loss: 0.8083 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.43008\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.4131 - accuracy: 0.9146 - val_loss: 0.7693 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00032: loss improved from 0.43008 to 0.41280, saving model to ./SEfold1000000320.426829.hdf5\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4318 - accuracy: 0.8994 - val_loss: 0.7399 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.41280\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4253 - accuracy: 0.9024 - val_loss: 0.7057 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.41280\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4172 - accuracy: 0.9024 - val_loss: 0.6609 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.41280\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4279 - accuracy: 0.8994 - val_loss: 0.6081 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.41280\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4136 - accuracy: 0.9055 - val_loss: 0.5889 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.41280\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4054 - accuracy: 0.9177 - val_loss: 0.5838 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00038: loss improved from 0.41280 to 0.40578, saving model to ./SEfold1000000380.804878.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4095 - accuracy: 0.9055 - val_loss: 0.5994 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.40578\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4103 - accuracy: 0.9116 - val_loss: 0.5840 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.40578\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3918 - accuracy: 0.9329 - val_loss: 0.5872 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00041: loss improved from 0.40578 to 0.39217, saving model to ./SEfold1000000410.743902.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3986 - accuracy: 0.9177 - val_loss: 0.5661 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.39217\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3998 - accuracy: 0.9238 - val_loss: 0.5592 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.39217\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4024 - accuracy: 0.9238 - val_loss: 0.5427 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.39217\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3938 - accuracy: 0.9238 - val_loss: 0.5382 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.39217\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4031 - accuracy: 0.9146 - val_loss: 0.5323 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.39217\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3953 - accuracy: 0.9329 - val_loss: 0.5130 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.39217\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3745 - accuracy: 0.9573 - val_loss: 0.5066 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00048: loss improved from 0.39217 to 0.37459, saving model to ./SEfold1000000480.817073.hdf5\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3973 - accuracy: 0.9207 - val_loss: 0.4819 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.37459\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3809 - accuracy: 0.9421 - val_loss: 0.4764 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.37459\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3831 - accuracy: 0.9543 - val_loss: 0.4591 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.37459\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3765 - accuracy: 0.9451 - val_loss: 0.4616 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.37459\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3826 - accuracy: 0.9390 - val_loss: 0.4622 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.37459\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3644 - accuracy: 0.9604 - val_loss: 0.4689 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00054: loss improved from 0.37459 to 0.36459, saving model to ./SEfold1000000540.841463.hdf5\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3709 - accuracy: 0.9512 - val_loss: 0.4670 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.36459\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 215ms/step - loss: 0.3788 - accuracy: 0.9451 - val_loss: 0.4684 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.36459\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.3784 - accuracy: 0.9390 - val_loss: 0.4672 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.36459\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 209ms/step - loss: 0.3696 - accuracy: 0.9512 - val_loss: 0.4596 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.36459\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3722 - accuracy: 0.9482 - val_loss: 0.4502 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.36459\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3697 - accuracy: 0.9512 - val_loss: 0.4643 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.36459\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3680 - accuracy: 0.9512 - val_loss: 0.4478 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.36459\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3567 - accuracy: 0.9726 - val_loss: 0.4434 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00062: loss improved from 0.36459 to 0.35686, saving model to ./SEfold1000000620.853659.hdf5\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3558 - accuracy: 0.9665 - val_loss: 0.4502 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00063: loss improved from 0.35686 to 0.35553, saving model to ./SEfold1000000630.865854.hdf5\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3508 - accuracy: 0.9695 - val_loss: 0.4522 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00064: loss improved from 0.35553 to 0.35097, saving model to ./SEfold1000000640.865854.hdf5\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3653 - accuracy: 0.9573 - val_loss: 0.4503 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.35097\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3627 - accuracy: 0.9543 - val_loss: 0.4809 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.35097\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3563 - accuracy: 0.9695 - val_loss: 0.4755 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.35097\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3481 - accuracy: 0.9726 - val_loss: 0.4571 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00068: loss improved from 0.35097 to 0.34823, saving model to ./SEfold1000000680.853659.hdf5\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3585 - accuracy: 0.9573 - val_loss: 0.4610 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.34823\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3561 - accuracy: 0.9634 - val_loss: 0.4804 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.34823\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3487 - accuracy: 0.9726 - val_loss: 0.4698 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.34823\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3566 - accuracy: 0.9604 - val_loss: 0.4844 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.34823\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3436 - accuracy: 0.9787 - val_loss: 0.4900 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00073: loss improved from 0.34823 to 0.34305, saving model to ./SEfold1000000730.817073.hdf5\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3478 - accuracy: 0.9726 - val_loss: 0.4665 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.34305\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3474 - accuracy: 0.9726 - val_loss: 0.4750 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.34305\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3558 - accuracy: 0.9604 - val_loss: 0.4923 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.34305\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3437 - accuracy: 0.9726 - val_loss: 0.4670 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.34305\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3430 - accuracy: 0.9726 - val_loss: 0.4686 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00078: loss improved from 0.34305 to 0.34220, saving model to ./SEfold1000000780.829268.hdf5\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3474 - accuracy: 0.9665 - val_loss: 0.4707 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.34220\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3758 - accuracy: 0.9329 - val_loss: 0.4390 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.34220\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.3394 - accuracy: 0.9848 - val_loss: 0.4402 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00081: loss improved from 0.34220 to 0.33934, saving model to ./SEfold1000000810.890244.hdf5\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3393 - accuracy: 0.9756 - val_loss: 0.4443 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.33934\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 7s 210ms/step - loss: 0.3434 - accuracy: 0.9756 - val_loss: 0.4606 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.33934\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 7s 210ms/step - loss: 0.3357 - accuracy: 0.9817 - val_loss: 0.4699 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00084: loss improved from 0.33934 to 0.33578, saving model to ./SEfold1000000840.817073.hdf5\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3413 - accuracy: 0.9817 - val_loss: 0.4705 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.33578\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 7s 207ms/step - loss: 0.3418 - accuracy: 0.9756 - val_loss: 0.4829 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.33578\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3398 - accuracy: 0.9787 - val_loss: 0.4726 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.33578\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3309 - accuracy: 0.9909 - val_loss: 0.4723 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00088: loss improved from 0.33578 to 0.33100, saving model to ./SEfold1000000880.829268.hdf5\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3320 - accuracy: 0.9817 - val_loss: 0.4685 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.33100\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3379 - accuracy: 0.9817 - val_loss: 0.4654 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.33100\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3336 - accuracy: 0.9848 - val_loss: 0.4543 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.33100\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3369 - accuracy: 0.9817 - val_loss: 0.4728 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.33100\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3419 - accuracy: 0.9726 - val_loss: 0.4936 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.33100\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3272 - accuracy: 0.9909 - val_loss: 0.4970 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00094: loss improved from 0.33100 to 0.32705, saving model to ./SEfold1000000940.817073.hdf5\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3332 - accuracy: 0.9848 - val_loss: 0.4697 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.32705\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3527 - accuracy: 0.9604 - val_loss: 0.4599 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.32705\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3333 - accuracy: 0.9848 - val_loss: 0.4548 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.32705\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3372 - accuracy: 0.9787 - val_loss: 0.4295 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.32705\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3245 - accuracy: 0.9939 - val_loss: 0.4355 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00099: loss improved from 0.32705 to 0.32456, saving model to ./SEfold1000000990.865854.hdf5\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3251 - accuracy: 0.9909 - val_loss: 0.4365 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.32456\n",
            "['FOLD:'] 2\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 23s 683ms/step - loss: 1.0275 - accuracy: 0.2439 - val_loss: 0.7955 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.02799, saving model to ./SEfold2000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.9929 - accuracy: 0.2439 - val_loss: 0.8407 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: loss improved from 1.02799 to 0.99271, saving model to ./SEfold2000000020.243902.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.9493 - accuracy: 0.2439 - val_loss: 0.9549 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: loss improved from 0.99271 to 0.94881, saving model to ./SEfold2000000030.243902.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.8881 - accuracy: 0.2500 - val_loss: 0.9460 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: loss improved from 0.94881 to 0.88729, saving model to ./SEfold2000000040.243902.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.8197 - accuracy: 0.2957 - val_loss: 0.9406 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: loss improved from 0.88729 to 0.81975, saving model to ./SEfold2000000050.243902.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.7507 - accuracy: 0.4268 - val_loss: 0.9203 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00006: loss improved from 0.81975 to 0.75051, saving model to ./SEfold2000000060.243902.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.7065 - accuracy: 0.5274 - val_loss: 0.8755 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00007: loss improved from 0.75051 to 0.70648, saving model to ./SEfold2000000070.243902.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.6568 - accuracy: 0.6433 - val_loss: 0.8869 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00008: loss improved from 0.70648 to 0.65653, saving model to ./SEfold2000000080.243902.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.6277 - accuracy: 0.7073 - val_loss: 0.8458 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00009: loss improved from 0.65653 to 0.62751, saving model to ./SEfold2000000090.243902.hdf5\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.6017 - accuracy: 0.7348 - val_loss: 0.8299 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00010: loss improved from 0.62751 to 0.60204, saving model to ./SEfold2000000100.243902.hdf5\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.5701 - accuracy: 0.7805 - val_loss: 0.7895 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00011: loss improved from 0.60204 to 0.57062, saving model to ./SEfold2000000110.280488.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.5508 - accuracy: 0.8018 - val_loss: 0.7626 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00012: loss improved from 0.57062 to 0.55004, saving model to ./SEfold2000000120.280488.hdf5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.5395 - accuracy: 0.8201 - val_loss: 0.7288 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00013: loss improved from 0.55004 to 0.53895, saving model to ./SEfold2000000130.426829.hdf5\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5313 - accuracy: 0.8232 - val_loss: 0.7237 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00014: loss improved from 0.53895 to 0.53115, saving model to ./SEfold2000000140.475610.hdf5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.5204 - accuracy: 0.8171 - val_loss: 0.7415 - val_accuracy: 0.3537\n",
            "\n",
            "Epoch 00015: loss improved from 0.53115 to 0.52067, saving model to ./SEfold2000000150.353659.hdf5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.5165 - accuracy: 0.8232 - val_loss: 0.7251 - val_accuracy: 0.4146\n",
            "\n",
            "Epoch 00016: loss improved from 0.52067 to 0.51682, saving model to ./SEfold2000000160.414634.hdf5\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4877 - accuracy: 0.8567 - val_loss: 0.6972 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00017: loss improved from 0.51682 to 0.48806, saving model to ./SEfold2000000170.524390.hdf5\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5038 - accuracy: 0.8323 - val_loss: 0.6777 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.48806\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.4975 - accuracy: 0.8506 - val_loss: 0.6896 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.48806\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 7s 211ms/step - loss: 0.4761 - accuracy: 0.8689 - val_loss: 0.6868 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00020: loss improved from 0.48806 to 0.47629, saving model to ./SEfold2000000200.597561.hdf5\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4861 - accuracy: 0.8384 - val_loss: 0.6875 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.47629\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 7s 207ms/step - loss: 0.4732 - accuracy: 0.8598 - val_loss: 0.7040 - val_accuracy: 0.5122\n",
            "\n",
            "Epoch 00022: loss improved from 0.47629 to 0.47342, saving model to ./SEfold2000000220.512195.hdf5\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4721 - accuracy: 0.8598 - val_loss: 0.7189 - val_accuracy: 0.5122\n",
            "\n",
            "Epoch 00023: loss improved from 0.47342 to 0.47251, saving model to ./SEfold2000000230.512195.hdf5\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4555 - accuracy: 0.8780 - val_loss: 0.7000 - val_accuracy: 0.5610\n",
            "\n",
            "Epoch 00024: loss improved from 0.47251 to 0.45575, saving model to ./SEfold2000000240.560976.hdf5\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4415 - accuracy: 0.8841 - val_loss: 0.7012 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00025: loss improved from 0.45575 to 0.44196, saving model to ./SEfold2000000250.585366.hdf5\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4565 - accuracy: 0.8750 - val_loss: 0.6914 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.44196\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4411 - accuracy: 0.8902 - val_loss: 0.6867 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00027: loss improved from 0.44196 to 0.44026, saving model to ./SEfold2000000270.634146.hdf5\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4319 - accuracy: 0.9085 - val_loss: 0.6885 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00028: loss improved from 0.44026 to 0.43185, saving model to ./SEfold2000000280.609756.hdf5\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4466 - accuracy: 0.8841 - val_loss: 0.7015 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.43185\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.4365 - accuracy: 0.8963 - val_loss: 0.6864 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.43185\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4262 - accuracy: 0.8963 - val_loss: 0.6773 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00031: loss improved from 0.43185 to 0.42668, saving model to ./SEfold2000000310.621951.hdf5\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4215 - accuracy: 0.9085 - val_loss: 0.6681 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00032: loss improved from 0.42668 to 0.42198, saving model to ./SEfold2000000320.548781.hdf5\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4149 - accuracy: 0.9146 - val_loss: 0.6624 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00033: loss improved from 0.42198 to 0.41529, saving model to ./SEfold2000000330.646341.hdf5\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4254 - accuracy: 0.8994 - val_loss: 0.6759 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.41529\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4180 - accuracy: 0.9055 - val_loss: 0.6680 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.41529\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4135 - accuracy: 0.9177 - val_loss: 0.6637 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00036: loss improved from 0.41529 to 0.41318, saving model to ./SEfold2000000360.621951.hdf5\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4077 - accuracy: 0.9238 - val_loss: 0.6401 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00037: loss improved from 0.41318 to 0.40732, saving model to ./SEfold2000000370.682927.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4044 - accuracy: 0.9238 - val_loss: 0.6315 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00038: loss improved from 0.40732 to 0.40464, saving model to ./SEfold2000000380.695122.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4020 - accuracy: 0.9238 - val_loss: 0.6273 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00039: loss improved from 0.40464 to 0.40223, saving model to ./SEfold2000000390.743902.hdf5\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3997 - accuracy: 0.9299 - val_loss: 0.6058 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00040: loss improved from 0.40223 to 0.39933, saving model to ./SEfold2000000400.731707.hdf5\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4020 - accuracy: 0.9207 - val_loss: 0.5864 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.39933\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4122 - accuracy: 0.9024 - val_loss: 0.5937 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.39933\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4104 - accuracy: 0.9085 - val_loss: 0.5825 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.39933\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3850 - accuracy: 0.9421 - val_loss: 0.5617 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00044: loss improved from 0.39933 to 0.38261, saving model to ./SEfold2000000440.768293.hdf5\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4056 - accuracy: 0.9207 - val_loss: 0.5538 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.38261\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3959 - accuracy: 0.9268 - val_loss: 0.5357 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.38261\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3935 - accuracy: 0.9268 - val_loss: 0.5263 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.38261\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3834 - accuracy: 0.9421 - val_loss: 0.5246 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.38261\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4022 - accuracy: 0.9146 - val_loss: 0.5248 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.38261\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3802 - accuracy: 0.9482 - val_loss: 0.5233 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00050: loss improved from 0.38261 to 0.38057, saving model to ./SEfold2000000500.780488.hdf5\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3776 - accuracy: 0.9451 - val_loss: 0.5252 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00051: loss improved from 0.38057 to 0.37764, saving model to ./SEfold2000000510.768293.hdf5\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3831 - accuracy: 0.9482 - val_loss: 0.5280 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.37764\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3774 - accuracy: 0.9451 - val_loss: 0.5357 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.37764\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 213ms/step - loss: 0.3645 - accuracy: 0.9573 - val_loss: 0.5295 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00054: loss improved from 0.37764 to 0.36471, saving model to ./SEfold2000000540.756098.hdf5\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3767 - accuracy: 0.9329 - val_loss: 0.5162 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.36471\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 208ms/step - loss: 0.3688 - accuracy: 0.9604 - val_loss: 0.4975 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.36471\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3729 - accuracy: 0.9543 - val_loss: 0.4985 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.36471\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3754 - accuracy: 0.9421 - val_loss: 0.5096 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.36471\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3718 - accuracy: 0.9543 - val_loss: 0.5200 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.36471\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3759 - accuracy: 0.9482 - val_loss: 0.5288 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.36471\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3651 - accuracy: 0.9573 - val_loss: 0.5203 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.36471\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3646 - accuracy: 0.9573 - val_loss: 0.5267 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.36471\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3684 - accuracy: 0.9573 - val_loss: 0.5122 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.36471\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3871 - accuracy: 0.9329 - val_loss: 0.5164 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.36471\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3630 - accuracy: 0.9512 - val_loss: 0.5189 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00065: loss improved from 0.36471 to 0.36323, saving model to ./SEfold2000000650.768293.hdf5\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3639 - accuracy: 0.9573 - val_loss: 0.5308 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.36323\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.3624 - accuracy: 0.9512 - val_loss: 0.5230 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00067: loss improved from 0.36323 to 0.36225, saving model to ./SEfold2000000670.792683.hdf5\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3544 - accuracy: 0.9665 - val_loss: 0.5156 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00068: loss improved from 0.36225 to 0.35445, saving model to ./SEfold2000000680.792683.hdf5\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3592 - accuracy: 0.9634 - val_loss: 0.5046 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.35445\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3513 - accuracy: 0.9726 - val_loss: 0.4935 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00070: loss improved from 0.35445 to 0.35150, saving model to ./SEfold2000000700.829268.hdf5\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3657 - accuracy: 0.9573 - val_loss: 0.5180 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.35150\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3541 - accuracy: 0.9695 - val_loss: 0.5150 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.35150\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3571 - accuracy: 0.9604 - val_loss: 0.5179 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.35150\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3626 - accuracy: 0.9512 - val_loss: 0.5147 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.35150\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3405 - accuracy: 0.9817 - val_loss: 0.5061 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00075: loss improved from 0.35150 to 0.34066, saving model to ./SEfold2000000750.804878.hdf5\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3566 - accuracy: 0.9604 - val_loss: 0.5013 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.34066\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3407 - accuracy: 0.9787 - val_loss: 0.5032 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00077: loss improved from 0.34066 to 0.34062, saving model to ./SEfold2000000770.792683.hdf5\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3534 - accuracy: 0.9665 - val_loss: 0.5063 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.34062\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3495 - accuracy: 0.9665 - val_loss: 0.5004 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.34062\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3418 - accuracy: 0.9756 - val_loss: 0.5117 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.34062\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 207ms/step - loss: 0.3371 - accuracy: 0.9848 - val_loss: 0.4963 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00081: loss improved from 0.34062 to 0.33722, saving model to ./SEfold2000000810.817073.hdf5\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3343 - accuracy: 0.9848 - val_loss: 0.4997 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00082: loss improved from 0.33722 to 0.33434, saving model to ./SEfold2000000820.804878.hdf5\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3464 - accuracy: 0.9756 - val_loss: 0.5075 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.33434\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3366 - accuracy: 0.9817 - val_loss: 0.5290 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.33434\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3401 - accuracy: 0.9726 - val_loss: 0.5459 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.33434\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3437 - accuracy: 0.9756 - val_loss: 0.5551 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.33434\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3512 - accuracy: 0.9604 - val_loss: 0.5318 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.33434\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3497 - accuracy: 0.9604 - val_loss: 0.5150 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.33434\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3388 - accuracy: 0.9787 - val_loss: 0.5104 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.33434\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3480 - accuracy: 0.9695 - val_loss: 0.5439 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.33434\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3410 - accuracy: 0.9726 - val_loss: 0.5506 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.33434\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3482 - accuracy: 0.9695 - val_loss: 0.5122 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.33434\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3374 - accuracy: 0.9848 - val_loss: 0.4962 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.33434\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3430 - accuracy: 0.9756 - val_loss: 0.5194 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.33434\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3390 - accuracy: 0.9756 - val_loss: 0.5148 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.33434\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3380 - accuracy: 0.9817 - val_loss: 0.5002 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.33434\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3430 - accuracy: 0.9695 - val_loss: 0.4924 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.33434\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3359 - accuracy: 0.9817 - val_loss: 0.4978 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.33434\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3392 - accuracy: 0.9756 - val_loss: 0.5087 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.33434\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3460 - accuracy: 0.9695 - val_loss: 0.5071 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.33434\n",
            "['FOLD:'] 3\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 22s 661ms/step - loss: 1.0661 - accuracy: 0.2439 - val_loss: 1.0367 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.06607, saving model to ./SEfold3000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 1.0603 - accuracy: 0.2439 - val_loss: 1.0507 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: loss improved from 1.06607 to 1.06181, saving model to ./SEfold3000000020.243902.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 1.0513 - accuracy: 0.2439 - val_loss: 1.0473 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: loss improved from 1.06181 to 1.05052, saving model to ./SEfold3000000030.243902.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 1.0302 - accuracy: 0.2439 - val_loss: 1.0448 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: loss improved from 1.05052 to 1.02876, saving model to ./SEfold3000000040.243902.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.9946 - accuracy: 0.2439 - val_loss: 1.0091 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: loss improved from 1.02876 to 0.99380, saving model to ./SEfold3000000050.243902.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.9118 - accuracy: 0.2439 - val_loss: 0.9314 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00006: loss improved from 0.99380 to 0.91245, saving model to ./SEfold3000000060.243902.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.8231 - accuracy: 0.3140 - val_loss: 0.8893 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00007: loss improved from 0.91245 to 0.82223, saving model to ./SEfold3000000070.243902.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.7317 - accuracy: 0.4329 - val_loss: 0.8609 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00008: loss improved from 0.82223 to 0.73166, saving model to ./SEfold3000000080.243902.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.6473 - accuracy: 0.6677 - val_loss: 0.8590 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00009: loss improved from 0.73166 to 0.64748, saving model to ./SEfold3000000090.243902.hdf5\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5982 - accuracy: 0.7744 - val_loss: 0.8436 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00010: loss improved from 0.64748 to 0.59825, saving model to ./SEfold3000000100.243902.hdf5\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5666 - accuracy: 0.7927 - val_loss: 0.8214 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00011: loss improved from 0.59825 to 0.56679, saving model to ./SEfold3000000110.243902.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5648 - accuracy: 0.7805 - val_loss: 0.7942 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00012: loss improved from 0.56679 to 0.56467, saving model to ./SEfold3000000120.243902.hdf5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.5298 - accuracy: 0.8171 - val_loss: 0.7915 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00013: loss improved from 0.56467 to 0.53020, saving model to ./SEfold3000000130.256098.hdf5\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5269 - accuracy: 0.8171 - val_loss: 0.7385 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00014: loss improved from 0.53020 to 0.52729, saving model to ./SEfold3000000140.304878.hdf5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.5150 - accuracy: 0.8262 - val_loss: 0.7233 - val_accuracy: 0.3537\n",
            "\n",
            "Epoch 00015: loss improved from 0.52729 to 0.51441, saving model to ./SEfold3000000150.353659.hdf5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5064 - accuracy: 0.8293 - val_loss: 0.7493 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00016: loss improved from 0.51441 to 0.50616, saving model to ./SEfold3000000160.304878.hdf5\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4874 - accuracy: 0.8476 - val_loss: 0.7532 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00017: loss improved from 0.50616 to 0.48809, saving model to ./SEfold3000000170.268293.hdf5\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4862 - accuracy: 0.8415 - val_loss: 0.7479 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00018: loss improved from 0.48809 to 0.48628, saving model to ./SEfold3000000180.292683.hdf5\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4760 - accuracy: 0.8567 - val_loss: 0.7415 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00019: loss improved from 0.48628 to 0.47567, saving model to ./SEfold3000000190.341463.hdf5\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4683 - accuracy: 0.8598 - val_loss: 0.7763 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00020: loss improved from 0.47567 to 0.46856, saving model to ./SEfold3000000200.280488.hdf5\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4660 - accuracy: 0.8780 - val_loss: 0.7958 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00021: loss improved from 0.46856 to 0.46503, saving model to ./SEfold3000000210.292683.hdf5\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4535 - accuracy: 0.8811 - val_loss: 0.8154 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00022: loss improved from 0.46503 to 0.45384, saving model to ./SEfold3000000220.280488.hdf5\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4538 - accuracy: 0.8780 - val_loss: 0.8413 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.45384\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4535 - accuracy: 0.8811 - val_loss: 0.8819 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.45384\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4491 - accuracy: 0.8841 - val_loss: 0.9044 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00025: loss improved from 0.45384 to 0.44963, saving model to ./SEfold3000000250.256098.hdf5\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4406 - accuracy: 0.8841 - val_loss: 0.8904 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00026: loss improved from 0.44963 to 0.44106, saving model to ./SEfold3000000260.231707.hdf5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4381 - accuracy: 0.8902 - val_loss: 0.8665 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00027: loss improved from 0.44106 to 0.43791, saving model to ./SEfold3000000270.256098.hdf5\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4411 - accuracy: 0.8872 - val_loss: 0.8711 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.43791\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4354 - accuracy: 0.8902 - val_loss: 0.8619 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00029: loss improved from 0.43791 to 0.43549, saving model to ./SEfold3000000290.268293.hdf5\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4222 - accuracy: 0.9055 - val_loss: 0.8275 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00030: loss improved from 0.43549 to 0.42248, saving model to ./SEfold3000000300.280488.hdf5\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4264 - accuracy: 0.8994 - val_loss: 0.7960 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.42248\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4245 - accuracy: 0.8994 - val_loss: 0.7485 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.42248\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4182 - accuracy: 0.9024 - val_loss: 0.7161 - val_accuracy: 0.5610\n",
            "\n",
            "Epoch 00033: loss improved from 0.42248 to 0.41800, saving model to ./SEfold3000000330.560976.hdf5\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.4172 - accuracy: 0.8963 - val_loss: 0.6968 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00034: loss improved from 0.41800 to 0.41707, saving model to ./SEfold3000000340.548781.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4152 - accuracy: 0.9085 - val_loss: 0.6740 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00035: loss improved from 0.41707 to 0.41502, saving model to ./SEfold3000000350.597561.hdf5\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4166 - accuracy: 0.8994 - val_loss: 0.6549 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.41502\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4128 - accuracy: 0.9085 - val_loss: 0.6339 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00037: loss improved from 0.41502 to 0.41131, saving model to ./SEfold3000000370.682927.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4064 - accuracy: 0.9207 - val_loss: 0.6232 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00038: loss improved from 0.41131 to 0.40655, saving model to ./SEfold3000000380.707317.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4161 - accuracy: 0.9024 - val_loss: 0.6049 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.40655\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3983 - accuracy: 0.9207 - val_loss: 0.5929 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00040: loss improved from 0.40655 to 0.39845, saving model to ./SEfold3000000400.768293.hdf5\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4087 - accuracy: 0.9146 - val_loss: 0.5834 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.39845\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3964 - accuracy: 0.9207 - val_loss: 0.5859 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00042: loss improved from 0.39845 to 0.39674, saving model to ./SEfold3000000420.743902.hdf5\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3916 - accuracy: 0.9299 - val_loss: 0.5917 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00043: loss improved from 0.39674 to 0.39167, saving model to ./SEfold3000000430.743902.hdf5\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3856 - accuracy: 0.9360 - val_loss: 0.5872 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00044: loss improved from 0.39167 to 0.38602, saving model to ./SEfold3000000440.780488.hdf5\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3959 - accuracy: 0.9238 - val_loss: 0.5982 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.38602\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3944 - accuracy: 0.9299 - val_loss: 0.6184 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.38602\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3931 - accuracy: 0.9238 - val_loss: 0.6218 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.38602\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3878 - accuracy: 0.9360 - val_loss: 0.5927 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.38602\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3728 - accuracy: 0.9543 - val_loss: 0.5723 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00049: loss improved from 0.38602 to 0.37296, saving model to ./SEfold3000000490.780488.hdf5\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3655 - accuracy: 0.9482 - val_loss: 0.5616 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00050: loss improved from 0.37296 to 0.36560, saving model to ./SEfold3000000500.792683.hdf5\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3820 - accuracy: 0.9390 - val_loss: 0.5596 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.36560\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3745 - accuracy: 0.9421 - val_loss: 0.5502 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.36560\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3776 - accuracy: 0.9482 - val_loss: 0.5450 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.36560\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 211ms/step - loss: 0.3773 - accuracy: 0.9421 - val_loss: 0.5314 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.36560\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3793 - accuracy: 0.9390 - val_loss: 0.5067 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.36560\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3731 - accuracy: 0.9482 - val_loss: 0.5021 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.36560\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3655 - accuracy: 0.9573 - val_loss: 0.5088 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.36560\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3736 - accuracy: 0.9451 - val_loss: 0.5145 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.36560\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3688 - accuracy: 0.9512 - val_loss: 0.4988 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.36560\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3702 - accuracy: 0.9451 - val_loss: 0.4862 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.36560\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3625 - accuracy: 0.9573 - val_loss: 0.5019 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00061: loss improved from 0.36560 to 0.36227, saving model to ./SEfold3000000610.792683.hdf5\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3652 - accuracy: 0.9604 - val_loss: 0.4854 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.36227\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3536 - accuracy: 0.9604 - val_loss: 0.4825 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00063: loss improved from 0.36227 to 0.35376, saving model to ./SEfold3000000630.829268.hdf5\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3551 - accuracy: 0.9634 - val_loss: 0.4881 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.35376\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3463 - accuracy: 0.9756 - val_loss: 0.4889 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00065: loss improved from 0.35376 to 0.34643, saving model to ./SEfold3000000650.829268.hdf5\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3562 - accuracy: 0.9604 - val_loss: 0.4650 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.34643\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3554 - accuracy: 0.9634 - val_loss: 0.4796 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.34643\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3472 - accuracy: 0.9726 - val_loss: 0.4842 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.34643\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3430 - accuracy: 0.9756 - val_loss: 0.4865 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00069: loss improved from 0.34643 to 0.34232, saving model to ./SEfold3000000690.829268.hdf5\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3488 - accuracy: 0.9695 - val_loss: 0.4684 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.34232\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3461 - accuracy: 0.9756 - val_loss: 0.4632 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.34232\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3567 - accuracy: 0.9573 - val_loss: 0.4579 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.34232\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3519 - accuracy: 0.9726 - val_loss: 0.4750 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.34232\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3416 - accuracy: 0.9817 - val_loss: 0.4827 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00074: loss improved from 0.34232 to 0.34102, saving model to ./SEfold3000000740.829268.hdf5\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3444 - accuracy: 0.9726 - val_loss: 0.4911 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.34102\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3445 - accuracy: 0.9726 - val_loss: 0.4888 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.34102\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3394 - accuracy: 0.9817 - val_loss: 0.4857 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00077: loss improved from 0.34102 to 0.33912, saving model to ./SEfold3000000770.829268.hdf5\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3391 - accuracy: 0.9726 - val_loss: 0.4824 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.33912\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3479 - accuracy: 0.9665 - val_loss: 0.4824 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.33912\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.3418 - accuracy: 0.9787 - val_loss: 0.4916 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.33912\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3391 - accuracy: 0.9756 - val_loss: 0.4773 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00081: loss improved from 0.33912 to 0.33906, saving model to ./SEfold3000000810.841463.hdf5\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3435 - accuracy: 0.9756 - val_loss: 0.4916 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.33906\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3479 - accuracy: 0.9695 - val_loss: 0.4875 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.33906\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3373 - accuracy: 0.9817 - val_loss: 0.4870 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00084: loss improved from 0.33906 to 0.33701, saving model to ./SEfold3000000840.841463.hdf5\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3308 - accuracy: 0.9878 - val_loss: 0.4878 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00085: loss improved from 0.33701 to 0.33089, saving model to ./SEfold3000000850.817073.hdf5\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3424 - accuracy: 0.9726 - val_loss: 0.4784 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.33089\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.3296 - accuracy: 0.9909 - val_loss: 0.4872 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00087: loss improved from 0.33089 to 0.32972, saving model to ./SEfold3000000870.829268.hdf5\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3343 - accuracy: 0.9848 - val_loss: 0.4832 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.32972\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3344 - accuracy: 0.9817 - val_loss: 0.4892 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.32972\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3301 - accuracy: 0.9909 - val_loss: 0.4841 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.32972\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3397 - accuracy: 0.9756 - val_loss: 0.5011 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.32972\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3307 - accuracy: 0.9878 - val_loss: 0.4954 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.32972\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3360 - accuracy: 0.9848 - val_loss: 0.4875 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.32972\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3291 - accuracy: 0.9878 - val_loss: 0.4926 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00094: loss improved from 0.32972 to 0.32919, saving model to ./SEfold3000000940.817073.hdf5\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3343 - accuracy: 0.9817 - val_loss: 0.4864 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.32919\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3336 - accuracy: 0.9787 - val_loss: 0.4841 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.32919\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3373 - accuracy: 0.9817 - val_loss: 0.4878 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.32919\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3343 - accuracy: 0.9817 - val_loss: 0.4823 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.32919\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3371 - accuracy: 0.9756 - val_loss: 0.4826 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.32919\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.3443 - accuracy: 0.9695 - val_loss: 0.4802 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.32919\n",
            "['FOLD:'] 4\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 23s 698ms/step - loss: 1.0472 - accuracy: 0.2439 - val_loss: 0.7593 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.04666, saving model to ./SEfold4000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 1.0268 - accuracy: 0.2439 - val_loss: 0.8034 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: loss improved from 1.04666 to 1.02677, saving model to ./SEfold4000000020.243902.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.9883 - accuracy: 0.2470 - val_loss: 0.8540 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: loss improved from 1.02677 to 0.98845, saving model to ./SEfold4000000030.243902.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.9325 - accuracy: 0.2470 - val_loss: 0.9274 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: loss improved from 0.98845 to 0.93420, saving model to ./SEfold4000000040.243902.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.8883 - accuracy: 0.2591 - val_loss: 0.8507 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: loss improved from 0.93420 to 0.88881, saving model to ./SEfold4000000050.243902.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.8149 - accuracy: 0.3232 - val_loss: 0.8531 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00006: loss improved from 0.88881 to 0.81530, saving model to ./SEfold4000000060.243902.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.7448 - accuracy: 0.4482 - val_loss: 0.8329 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00007: loss improved from 0.81530 to 0.74389, saving model to ./SEfold4000000070.243902.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.6772 - accuracy: 0.5915 - val_loss: 0.8102 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00008: loss improved from 0.74389 to 0.67763, saving model to ./SEfold4000000080.243902.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.6243 - accuracy: 0.7287 - val_loss: 0.8221 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00009: loss improved from 0.67763 to 0.62386, saving model to ./SEfold4000000090.243902.hdf5\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5917 - accuracy: 0.7561 - val_loss: 0.8207 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00010: loss improved from 0.62386 to 0.59222, saving model to ./SEfold4000000100.256098.hdf5\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5743 - accuracy: 0.7713 - val_loss: 0.8389 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00011: loss improved from 0.59222 to 0.57405, saving model to ./SEfold4000000110.256098.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5450 - accuracy: 0.7988 - val_loss: 0.8156 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00012: loss improved from 0.57405 to 0.54566, saving model to ./SEfold4000000120.256098.hdf5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.5339 - accuracy: 0.8110 - val_loss: 0.7732 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00013: loss improved from 0.54566 to 0.53394, saving model to ./SEfold4000000130.317073.hdf5\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5081 - accuracy: 0.8476 - val_loss: 0.7207 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00014: loss improved from 0.53394 to 0.50871, saving model to ./SEfold4000000140.475610.hdf5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5055 - accuracy: 0.8415 - val_loss: 0.7145 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00015: loss improved from 0.50871 to 0.50548, saving model to ./SEfold4000000150.426829.hdf5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5098 - accuracy: 0.8384 - val_loss: 0.6804 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.50548\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4914 - accuracy: 0.8506 - val_loss: 0.6868 - val_accuracy: 0.5610\n",
            "\n",
            "Epoch 00017: loss improved from 0.50548 to 0.49143, saving model to ./SEfold4000000170.560976.hdf5\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5044 - accuracy: 0.8262 - val_loss: 0.6784 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.49143\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4826 - accuracy: 0.8445 - val_loss: 0.7118 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00019: loss improved from 0.49143 to 0.48322, saving model to ./SEfold4000000190.524390.hdf5\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4779 - accuracy: 0.8689 - val_loss: 0.6982 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00020: loss improved from 0.48322 to 0.47807, saving model to ./SEfold4000000200.597561.hdf5\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4776 - accuracy: 0.8659 - val_loss: 0.7030 - val_accuracy: 0.5122\n",
            "\n",
            "Epoch 00021: loss improved from 0.47807 to 0.47762, saving model to ./SEfold4000000210.512195.hdf5\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4578 - accuracy: 0.8720 - val_loss: 0.6797 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00022: loss improved from 0.47762 to 0.45679, saving model to ./SEfold4000000220.573171.hdf5\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.4621 - accuracy: 0.8628 - val_loss: 0.6722 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.45679\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4495 - accuracy: 0.8841 - val_loss: 0.7079 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00024: loss improved from 0.45679 to 0.45016, saving model to ./SEfold4000000240.548781.hdf5\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4533 - accuracy: 0.8689 - val_loss: 0.7414 - val_accuracy: 0.4634\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.45016\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4428 - accuracy: 0.8963 - val_loss: 0.7954 - val_accuracy: 0.4024\n",
            "\n",
            "Epoch 00026: loss improved from 0.45016 to 0.44325, saving model to ./SEfold4000000260.402439.hdf5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4543 - accuracy: 0.8750 - val_loss: 0.8113 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.44325\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4443 - accuracy: 0.8872 - val_loss: 0.8097 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.44325\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4434 - accuracy: 0.8780 - val_loss: 0.8208 - val_accuracy: 0.3537\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.44325\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4309 - accuracy: 0.8872 - val_loss: 0.8053 - val_accuracy: 0.3537\n",
            "\n",
            "Epoch 00030: loss improved from 0.44325 to 0.43139, saving model to ./SEfold4000000300.353659.hdf5\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4394 - accuracy: 0.8811 - val_loss: 0.8061 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.43139\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4328 - accuracy: 0.8963 - val_loss: 0.7935 - val_accuracy: 0.4146\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.43139\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4380 - accuracy: 0.8841 - val_loss: 0.7818 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.43139\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4298 - accuracy: 0.9024 - val_loss: 0.7796 - val_accuracy: 0.4024\n",
            "\n",
            "Epoch 00034: loss improved from 0.43139 to 0.43015, saving model to ./SEfold4000000340.402439.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4369 - accuracy: 0.8933 - val_loss: 0.7562 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.43015\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4314 - accuracy: 0.8872 - val_loss: 0.7214 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.43015\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4186 - accuracy: 0.9055 - val_loss: 0.6799 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00037: loss improved from 0.43015 to 0.41832, saving model to ./SEfold4000000370.634146.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4009 - accuracy: 0.9268 - val_loss: 0.6432 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00038: loss improved from 0.41832 to 0.40046, saving model to ./SEfold4000000380.682927.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4073 - accuracy: 0.9207 - val_loss: 0.6100 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.40046\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4091 - accuracy: 0.9116 - val_loss: 0.6067 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.40046\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4054 - accuracy: 0.9177 - val_loss: 0.5965 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.40046\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4038 - accuracy: 0.9177 - val_loss: 0.5795 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.40046\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4086 - accuracy: 0.9177 - val_loss: 0.5757 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.40046\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4023 - accuracy: 0.9085 - val_loss: 0.5551 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.40046\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4094 - accuracy: 0.9116 - val_loss: 0.5523 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.40046\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3917 - accuracy: 0.9360 - val_loss: 0.5555 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00046: loss improved from 0.40046 to 0.39193, saving model to ./SEfold4000000460.768293.hdf5\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3904 - accuracy: 0.9299 - val_loss: 0.5471 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00047: loss improved from 0.39193 to 0.39025, saving model to ./SEfold4000000470.792683.hdf5\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.4000 - accuracy: 0.9207 - val_loss: 0.5517 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.39025\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3803 - accuracy: 0.9421 - val_loss: 0.5479 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00049: loss improved from 0.39025 to 0.38068, saving model to ./SEfold4000000490.792683.hdf5\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3943 - accuracy: 0.9268 - val_loss: 0.5391 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.38068\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3879 - accuracy: 0.9360 - val_loss: 0.5353 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.38068\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3766 - accuracy: 0.9421 - val_loss: 0.5171 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00052: loss improved from 0.38068 to 0.37671, saving model to ./SEfold4000000520.817073.hdf5\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3937 - accuracy: 0.9329 - val_loss: 0.5068 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.37671\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3836 - accuracy: 0.9390 - val_loss: 0.4901 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.37671\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3575 - accuracy: 0.9604 - val_loss: 0.4797 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00055: loss improved from 0.37671 to 0.35769, saving model to ./SEfold4000000550.853659.hdf5\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3727 - accuracy: 0.9482 - val_loss: 0.4759 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.35769\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3803 - accuracy: 0.9421 - val_loss: 0.4666 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.35769\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3816 - accuracy: 0.9360 - val_loss: 0.4599 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.35769\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3802 - accuracy: 0.9329 - val_loss: 0.4488 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.35769\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3740 - accuracy: 0.9451 - val_loss: 0.4440 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.35769\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3700 - accuracy: 0.9512 - val_loss: 0.4459 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.35769\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3623 - accuracy: 0.9634 - val_loss: 0.4505 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.35769\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3746 - accuracy: 0.9421 - val_loss: 0.4763 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.35769\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3679 - accuracy: 0.9482 - val_loss: 0.4726 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.35769\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3639 - accuracy: 0.9573 - val_loss: 0.4536 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.35769\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3643 - accuracy: 0.9604 - val_loss: 0.4642 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.35769\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3682 - accuracy: 0.9543 - val_loss: 0.4435 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.35769\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3561 - accuracy: 0.9604 - val_loss: 0.4432 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00068: loss improved from 0.35769 to 0.35568, saving model to ./SEfold4000000680.878049.hdf5\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3601 - accuracy: 0.9573 - val_loss: 0.4463 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.35568\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3715 - accuracy: 0.9482 - val_loss: 0.4512 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.35568\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3580 - accuracy: 0.9634 - val_loss: 0.4421 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.35568\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3515 - accuracy: 0.9726 - val_loss: 0.4456 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00072: loss improved from 0.35568 to 0.35166, saving model to ./SEfold4000000720.865854.hdf5\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3554 - accuracy: 0.9634 - val_loss: 0.4548 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.35166\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3505 - accuracy: 0.9665 - val_loss: 0.4563 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00074: loss improved from 0.35166 to 0.35071, saving model to ./SEfold4000000740.865854.hdf5\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3559 - accuracy: 0.9665 - val_loss: 0.4475 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.35071\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3602 - accuracy: 0.9573 - val_loss: 0.4509 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.35071\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3452 - accuracy: 0.9756 - val_loss: 0.4592 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00077: loss improved from 0.35071 to 0.34492, saving model to ./SEfold4000000770.841463.hdf5\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3440 - accuracy: 0.9756 - val_loss: 0.4496 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00078: loss improved from 0.34492 to 0.34399, saving model to ./SEfold4000000780.865854.hdf5\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3455 - accuracy: 0.9726 - val_loss: 0.4535 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.34399\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3463 - accuracy: 0.9726 - val_loss: 0.4537 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.34399\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3441 - accuracy: 0.9695 - val_loss: 0.4680 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.34399\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3506 - accuracy: 0.9604 - val_loss: 0.4768 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.34399\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3450 - accuracy: 0.9756 - val_loss: 0.5071 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.34399\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3454 - accuracy: 0.9756 - val_loss: 0.4382 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.34399\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3416 - accuracy: 0.9756 - val_loss: 0.4633 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00085: loss improved from 0.34399 to 0.34172, saving model to ./SEfold4000000850.841463.hdf5\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3493 - accuracy: 0.9634 - val_loss: 0.4739 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.34172\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3510 - accuracy: 0.9695 - val_loss: 0.5240 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.34172\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3401 - accuracy: 0.9756 - val_loss: 0.4577 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00088: loss improved from 0.34172 to 0.34017, saving model to ./SEfold4000000880.865854.hdf5\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3426 - accuracy: 0.9756 - val_loss: 0.4448 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.34017\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3453 - accuracy: 0.9695 - val_loss: 0.4736 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.34017\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3409 - accuracy: 0.9787 - val_loss: 0.4576 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.34017\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3315 - accuracy: 0.9909 - val_loss: 0.4540 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00092: loss improved from 0.34017 to 0.33156, saving model to ./SEfold4000000920.865854.hdf5\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3370 - accuracy: 0.9848 - val_loss: 0.4463 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.33156\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3357 - accuracy: 0.9817 - val_loss: 0.4416 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.33156\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3313 - accuracy: 0.9878 - val_loss: 0.4394 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00095: loss improved from 0.33156 to 0.33119, saving model to ./SEfold4000000950.890244.hdf5\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3355 - accuracy: 0.9787 - val_loss: 0.4493 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.33119\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3283 - accuracy: 0.9909 - val_loss: 0.4444 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00097: loss improved from 0.33119 to 0.32833, saving model to ./SEfold4000000970.890244.hdf5\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3318 - accuracy: 0.9878 - val_loss: 0.4604 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.32833\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3268 - accuracy: 0.9909 - val_loss: 0.4693 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00099: loss improved from 0.32833 to 0.32668, saving model to ./SEfold4000000990.829268.hdf5\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3315 - accuracy: 0.9848 - val_loss: 0.4866 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.32668\n",
            "['FOLD:'] 5\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 22s 680ms/step - loss: 1.0470 - accuracy: 0.2439 - val_loss: 1.0653 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.04697, saving model to ./SEfold5000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 1.0293 - accuracy: 0.2439 - val_loss: 1.0685 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: loss improved from 1.04697 to 1.02856, saving model to ./SEfold5000000020.243902.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.9993 - accuracy: 0.2439 - val_loss: 1.0679 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: loss improved from 1.02856 to 0.99923, saving model to ./SEfold5000000030.243902.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.9355 - accuracy: 0.2439 - val_loss: 1.0674 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: loss improved from 0.99923 to 0.93650, saving model to ./SEfold5000000040.243902.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.8888 - accuracy: 0.2591 - val_loss: 1.0668 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: loss improved from 0.93650 to 0.88888, saving model to ./SEfold5000000050.243902.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.8207 - accuracy: 0.2988 - val_loss: 1.0667 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00006: loss improved from 0.88888 to 0.82019, saving model to ./SEfold5000000060.243902.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.7455 - accuracy: 0.4451 - val_loss: 1.0653 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00007: loss improved from 0.82019 to 0.74562, saving model to ./SEfold5000000070.243902.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.6727 - accuracy: 0.5915 - val_loss: 1.0626 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00008: loss improved from 0.74562 to 0.67234, saving model to ./SEfold5000000080.243902.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.6322 - accuracy: 0.6921 - val_loss: 1.0533 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00009: loss improved from 0.67234 to 0.63223, saving model to ./SEfold5000000090.243902.hdf5\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.6009 - accuracy: 0.7409 - val_loss: 1.0521 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00010: loss improved from 0.63223 to 0.60111, saving model to ./SEfold5000000100.243902.hdf5\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5616 - accuracy: 0.7866 - val_loss: 1.0431 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00011: loss improved from 0.60111 to 0.56167, saving model to ./SEfold5000000110.243902.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.5506 - accuracy: 0.8049 - val_loss: 1.0349 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00012: loss improved from 0.56167 to 0.55010, saving model to ./SEfold5000000120.243902.hdf5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.5292 - accuracy: 0.8262 - val_loss: 1.0153 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00013: loss improved from 0.55010 to 0.52930, saving model to ./SEfold5000000130.243902.hdf5\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5136 - accuracy: 0.8354 - val_loss: 0.9940 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00014: loss improved from 0.52930 to 0.51353, saving model to ./SEfold5000000140.243902.hdf5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.5081 - accuracy: 0.8201 - val_loss: 0.9817 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00015: loss improved from 0.51353 to 0.50828, saving model to ./SEfold5000000150.243902.hdf5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4890 - accuracy: 0.8567 - val_loss: 0.9660 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00016: loss improved from 0.50828 to 0.48895, saving model to ./SEfold5000000160.243902.hdf5\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4839 - accuracy: 0.8537 - val_loss: 0.9654 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00017: loss improved from 0.48895 to 0.48436, saving model to ./SEfold5000000170.243902.hdf5\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4795 - accuracy: 0.8598 - val_loss: 0.9496 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00018: loss improved from 0.48436 to 0.47848, saving model to ./SEfold5000000180.243902.hdf5\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4727 - accuracy: 0.8506 - val_loss: 0.9309 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00019: loss improved from 0.47848 to 0.47285, saving model to ./SEfold5000000190.243902.hdf5\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.4557 - accuracy: 0.8872 - val_loss: 0.9321 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00020: loss improved from 0.47285 to 0.45625, saving model to ./SEfold5000000200.243902.hdf5\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4643 - accuracy: 0.8659 - val_loss: 0.9161 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.45625\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4470 - accuracy: 0.8872 - val_loss: 0.8946 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00022: loss improved from 0.45625 to 0.44692, saving model to ./SEfold5000000220.243902.hdf5\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4487 - accuracy: 0.8841 - val_loss: 0.8715 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.44692\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4445 - accuracy: 0.8720 - val_loss: 0.8321 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00024: loss improved from 0.44692 to 0.44381, saving model to ./SEfold5000000240.243902.hdf5\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.4399 - accuracy: 0.8872 - val_loss: 0.8004 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00025: loss improved from 0.44381 to 0.44033, saving model to ./SEfold5000000250.256098.hdf5\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4351 - accuracy: 0.8902 - val_loss: 0.7851 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00026: loss improved from 0.44033 to 0.43564, saving model to ./SEfold5000000260.256098.hdf5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4202 - accuracy: 0.9085 - val_loss: 0.7775 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00027: loss improved from 0.43564 to 0.42008, saving model to ./SEfold5000000270.256098.hdf5\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4370 - accuracy: 0.8811 - val_loss: 0.7789 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.42008\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.4207 - accuracy: 0.9085 - val_loss: 0.7387 - val_accuracy: 0.3537\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.42008\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4291 - accuracy: 0.8963 - val_loss: 0.7306 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.42008\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4238 - accuracy: 0.9024 - val_loss: 0.7274 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.42008\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4274 - accuracy: 0.8841 - val_loss: 0.7301 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.42008\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4236 - accuracy: 0.9024 - val_loss: 0.7241 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.42008\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4072 - accuracy: 0.9146 - val_loss: 0.7579 - val_accuracy: 0.3902\n",
            "\n",
            "Epoch 00034: loss improved from 0.42008 to 0.40693, saving model to ./SEfold5000000340.390244.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4133 - accuracy: 0.9024 - val_loss: 0.7733 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.40693\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4073 - accuracy: 0.9146 - val_loss: 0.7288 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.40693\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3987 - accuracy: 0.9146 - val_loss: 0.6905 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00037: loss improved from 0.40693 to 0.39916, saving model to ./SEfold5000000370.524390.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3981 - accuracy: 0.9177 - val_loss: 0.6562 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00038: loss improved from 0.39916 to 0.39827, saving model to ./SEfold5000000380.646341.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3989 - accuracy: 0.9329 - val_loss: 0.6461 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.39827\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4160 - accuracy: 0.8994 - val_loss: 0.6249 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.39827\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3916 - accuracy: 0.9329 - val_loss: 0.6148 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00041: loss improved from 0.39827 to 0.39181, saving model to ./SEfold5000000410.756098.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3786 - accuracy: 0.9451 - val_loss: 0.6018 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00042: loss improved from 0.39181 to 0.37848, saving model to ./SEfold5000000420.731707.hdf5\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3832 - accuracy: 0.9360 - val_loss: 0.5954 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.37848\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3847 - accuracy: 0.9390 - val_loss: 0.5915 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.37848\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3886 - accuracy: 0.9238 - val_loss: 0.5769 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.37848\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3747 - accuracy: 0.9482 - val_loss: 0.5878 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00046: loss improved from 0.37848 to 0.37497, saving model to ./SEfold5000000460.719512.hdf5\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 6s 192ms/step - loss: 0.3787 - accuracy: 0.9543 - val_loss: 0.5859 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.37497\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.3776 - accuracy: 0.9451 - val_loss: 0.5854 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.37497\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3773 - accuracy: 0.9329 - val_loss: 0.5940 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.37497\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3638 - accuracy: 0.9573 - val_loss: 0.5959 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00050: loss improved from 0.37497 to 0.36282, saving model to ./SEfold5000000500.682927.hdf5\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3655 - accuracy: 0.9604 - val_loss: 0.5867 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.36282\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3559 - accuracy: 0.9665 - val_loss: 0.5490 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00052: loss improved from 0.36282 to 0.35611, saving model to ./SEfold5000000520.804878.hdf5\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3576 - accuracy: 0.9695 - val_loss: 0.5315 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.35611\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3630 - accuracy: 0.9573 - val_loss: 0.5221 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.35611\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3701 - accuracy: 0.9451 - val_loss: 0.5345 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.35611\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3506 - accuracy: 0.9665 - val_loss: 0.5150 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00056: loss improved from 0.35611 to 0.35064, saving model to ./SEfold5000000560.804878.hdf5\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3595 - accuracy: 0.9665 - val_loss: 0.5061 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.35064\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3537 - accuracy: 0.9695 - val_loss: 0.5048 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.35064\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3571 - accuracy: 0.9604 - val_loss: 0.4894 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.35064\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3615 - accuracy: 0.9543 - val_loss: 0.4895 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.35064\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3496 - accuracy: 0.9726 - val_loss: 0.4940 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00061: loss improved from 0.35064 to 0.34981, saving model to ./SEfold5000000610.829268.hdf5\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3401 - accuracy: 0.9848 - val_loss: 0.5031 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00062: loss improved from 0.34981 to 0.34021, saving model to ./SEfold5000000620.817073.hdf5\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3519 - accuracy: 0.9665 - val_loss: 0.5144 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.34021\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3571 - accuracy: 0.9604 - val_loss: 0.5128 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.34021\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3453 - accuracy: 0.9665 - val_loss: 0.5161 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.34021\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3430 - accuracy: 0.9817 - val_loss: 0.5139 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.34021\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3451 - accuracy: 0.9756 - val_loss: 0.5030 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.34021\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3392 - accuracy: 0.9787 - val_loss: 0.4991 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00068: loss improved from 0.34021 to 0.33919, saving model to ./SEfold5000000680.817073.hdf5\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3493 - accuracy: 0.9695 - val_loss: 0.5040 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.33919\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.3385 - accuracy: 0.9787 - val_loss: 0.5000 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00070: loss improved from 0.33919 to 0.33858, saving model to ./SEfold5000000700.829268.hdf5\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.3335 - accuracy: 0.9878 - val_loss: 0.5293 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00071: loss improved from 0.33858 to 0.33349, saving model to ./SEfold5000000710.768293.hdf5\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.3423 - accuracy: 0.9756 - val_loss: 0.5305 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.33349\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3440 - accuracy: 0.9756 - val_loss: 0.5173 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.33349\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3350 - accuracy: 0.9848 - val_loss: 0.5109 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.33349\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3570 - accuracy: 0.9573 - val_loss: 0.5241 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.33349\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3445 - accuracy: 0.9726 - val_loss: 0.5327 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.33349\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3409 - accuracy: 0.9756 - val_loss: 0.5168 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.33349\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3454 - accuracy: 0.9695 - val_loss: 0.5302 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.33349\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3381 - accuracy: 0.9817 - val_loss: 0.5332 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.33349\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3419 - accuracy: 0.9726 - val_loss: 0.5270 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.33349\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3396 - accuracy: 0.9726 - val_loss: 0.5244 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.33349\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3304 - accuracy: 0.9939 - val_loss: 0.4928 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00082: loss improved from 0.33349 to 0.33049, saving model to ./SEfold5000000820.829268.hdf5\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.3215 - accuracy: 1.0000 - val_loss: 0.5180 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00083: loss improved from 0.33049 to 0.32153, saving model to ./SEfold5000000830.804878.hdf5\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3321 - accuracy: 0.9878 - val_loss: 0.5233 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.32153\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3332 - accuracy: 0.9909 - val_loss: 0.5297 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.32153\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3275 - accuracy: 0.9909 - val_loss: 0.5301 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.32153\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3323 - accuracy: 0.9848 - val_loss: 0.5279 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.32153\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3226 - accuracy: 0.9970 - val_loss: 0.5155 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.32153\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.3314 - accuracy: 0.9817 - val_loss: 0.5287 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.32153\n",
            "Epoch 00089: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0J3YG2DUZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "import sklearn\n",
        "\n",
        "def classifyEval(testX, testy,model):\n",
        "  # predict probabilities for test set  \n",
        "  yhat_probs = model.predict(testX, verbose=0)\n",
        "  yhat = np.max(yhat_probs,axis=1)\n",
        "  yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "  x=1\n",
        "  print(testy[:,1])\n",
        "  print(yhat_classes)\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(testy[:,x], yhat_classes)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(testy[:,x], yhat_classes)\n",
        "  print('Precision: %f' % precision)\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(testy[:,x], yhat_classes)\n",
        "  print('Recall: %f' % recall)\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(testy[:,x], yhat_classes)\n",
        "  print('F1 score: %f' % f1)\n",
        " \n",
        "  # kappa\n",
        "  kappa = cohen_kappa_score(testy[:,x], yhat_classes)\n",
        "  print('Cohens kappa: %f' % kappa)\n",
        "  # ROC AUC\n",
        "  auc = roc_auc_score(testy[:,x], yhat)\n",
        "  print('ROC AUC: %f' % auc)\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(testy[:,x], yhat_classes)\n",
        "  print(matrix)\n",
        "\n",
        "  fpr_keras, tpr_keras, thresholds_keras = sklearn.metrics.roc_curve(testy[:,x], yhat)\n",
        "  plt.figure(1)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.plot(fpr_keras, tpr_keras, label='CV1 (area = {:.3f})'.format(auc))\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxXwIdDaTncb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "c79be8dd-835d-4da2-c56f-20b33b19276c"
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold5.npy')\n",
        "Y_test = np.load('/content/TestLabelFold5.npy')\n",
        "model = model1()\n",
        "model.load_weights('/content/SEfold5000000820.829268.hdf5')\n",
        "classifyEval(X_test_extend, Y_test, model)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
            "[0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 0 0 0 0 0 1 1]\n",
            "Accuracy: 0.829268\n",
            "Precision: 0.875000\n",
            "Recall: 0.350000\n",
            "F1 score: 0.500000\n",
            "Cohens kappa: 0.419028\n",
            "ROC AUC: 0.605645\n",
            "[[61  1]\n",
            " [13  7]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuERoQ6CUZZ0",
        "colab_type": "text"
      },
      "source": [
        "## Attention Model for Multi Instance Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtK_lHXrUZZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featureextractor(data):\n",
        "    conv = Conv2D(20, 5, activation = None, padding = 'same')(data)\n",
        "    conv_a = Activation('relu')(BatchNormalization()(conv))\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv_a)\n",
        "    conv1 = Conv2D(50, 5, activation = None, padding = 'same')(pool1)\n",
        "    conv1_a = Activation('relu')(BatchNormalization()(conv1))\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
        "    return pool2\n",
        "\n",
        "def featureextractorx2(data):\n",
        "    dense = Dense(500,activation='relu')(data)\n",
        "    return dense\n",
        "\n",
        "def Attention(data):\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    return dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3miMxRUZZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "l1factor=1e-2\n",
        "l2factor=0\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "prediction = MaxPool(axis=1)(dense_1)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbRDRdVcYkP",
        "colab_type": "code",
        "outputId": "fab69d65-a6a7-48f0-f900-18355494f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIHWT6X6fPv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUaTaZWirf0",
        "colab_type": "code",
        "outputId": "c155dade-a1b1-43bb-cb6f-7c13d55aadec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}\n",
        "Features = []\n",
        "\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "\n",
        "logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "dense_2 = Flatten()(logistic1)\n",
        "\n",
        "logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "dense_3 = Flatten()(logistic2)\n",
        "\n",
        "logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "dense_4 = Flatten()(logistic3)\n",
        "\n",
        "final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "prediction = MaxPool(axis=1)(final)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FwRbn0eiwib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "noises=50\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "datagen.fit(X_train_extend)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bg_PaepkBhP",
        "colab_type": "code",
        "outputId": "9567289f-268b-42bd-9eda-a9d2e29f67ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.00001),\n",
        "              metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(X_train_extend, Y_train,batch_size=2),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_valid_extend, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., epochs=100)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134/134 [==============================] - 35s 264ms/step - loss: 0.6514 - accuracy: 0.7127 - val_loss: 0.8838 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.6267 - accuracy: 0.7425 - val_loss: 0.7636 - val_accuracy: 0.4333\n",
            "Epoch 3/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.6024 - accuracy: 0.7836 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.5672 - accuracy: 0.8507 - val_loss: 0.5682 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5584 - accuracy: 0.8433 - val_loss: 0.5442 - val_accuracy: 0.8167\n",
            "Epoch 6/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.5589 - accuracy: 0.8097 - val_loss: 0.5946 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5288 - accuracy: 0.8545 - val_loss: 0.5800 - val_accuracy: 0.7667\n",
            "Epoch 8/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5151 - accuracy: 0.8582 - val_loss: 0.5377 - val_accuracy: 0.8500\n",
            "Epoch 9/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5261 - accuracy: 0.8358 - val_loss: 0.5270 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5130 - accuracy: 0.8470 - val_loss: 0.5216 - val_accuracy: 0.8500\n",
            "Epoch 11/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4840 - accuracy: 0.8731 - val_loss: 0.5267 - val_accuracy: 0.8167\n",
            "Epoch 12/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4953 - accuracy: 0.8619 - val_loss: 0.5123 - val_accuracy: 0.8167\n",
            "Epoch 13/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5000 - accuracy: 0.8433 - val_loss: 0.4984 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4907 - accuracy: 0.8507 - val_loss: 0.4877 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4814 - accuracy: 0.8694 - val_loss: 0.4697 - val_accuracy: 0.8833\n",
            "Epoch 16/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4705 - accuracy: 0.8806 - val_loss: 0.4655 - val_accuracy: 0.8833\n",
            "Epoch 17/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4634 - accuracy: 0.8843 - val_loss: 0.4711 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4425 - accuracy: 0.9030 - val_loss: 0.4611 - val_accuracy: 0.8833\n",
            "Epoch 19/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4562 - accuracy: 0.8843 - val_loss: 0.4742 - val_accuracy: 0.8500\n",
            "Epoch 20/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4466 - accuracy: 0.8881 - val_loss: 0.4702 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4490 - accuracy: 0.8843 - val_loss: 0.4764 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4406 - accuracy: 0.8993 - val_loss: 0.4772 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9067 - val_loss: 0.4578 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4504 - accuracy: 0.8881 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4518 - accuracy: 0.8806 - val_loss: 0.5093 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4471 - accuracy: 0.8769 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9104 - val_loss: 0.4399 - val_accuracy: 0.9000\n",
            "Epoch 28/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4114 - accuracy: 0.9254 - val_loss: 0.4405 - val_accuracy: 0.8833\n",
            "Epoch 29/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4215 - accuracy: 0.9067 - val_loss: 0.4320 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4280 - accuracy: 0.8918 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4124 - accuracy: 0.9179 - val_loss: 0.4489 - val_accuracy: 0.8833\n",
            "Epoch 32/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4201 - accuracy: 0.9067 - val_loss: 0.4370 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4097 - accuracy: 0.9104 - val_loss: 0.4642 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4228 - accuracy: 0.8993 - val_loss: 0.4591 - val_accuracy: 0.8500\n",
            "Epoch 35/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4080 - accuracy: 0.9179 - val_loss: 0.4386 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4136 - accuracy: 0.8993 - val_loss: 0.4663 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "134/134 [==============================] - 17s 124ms/step - loss: 0.4195 - accuracy: 0.8955 - val_loss: 0.4612 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4086 - accuracy: 0.9067 - val_loss: 0.4633 - val_accuracy: 0.8333\n",
            "Epoch 39/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4060 - accuracy: 0.9179 - val_loss: 0.4412 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4010 - accuracy: 0.9142 - val_loss: 0.4987 - val_accuracy: 0.8167\n",
            "Epoch 41/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4112 - accuracy: 0.9104 - val_loss: 0.4959 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3995 - accuracy: 0.9254 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3979 - accuracy: 0.9142 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4013 - accuracy: 0.9216 - val_loss: 0.4608 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3892 - accuracy: 0.9254 - val_loss: 0.4543 - val_accuracy: 0.8833\n",
            "Epoch 46/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3986 - accuracy: 0.9216 - val_loss: 0.4524 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3882 - accuracy: 0.9328 - val_loss: 0.4398 - val_accuracy: 0.8833\n",
            "Epoch 48/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3867 - accuracy: 0.9366 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 49/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3776 - accuracy: 0.9440 - val_loss: 0.4824 - val_accuracy: 0.8333\n",
            "Epoch 50/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3888 - accuracy: 0.9291 - val_loss: 0.4923 - val_accuracy: 0.8167\n",
            "Epoch 51/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3899 - accuracy: 0.9291 - val_loss: 0.4734 - val_accuracy: 0.8333\n",
            "Epoch 52/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4009 - accuracy: 0.9179 - val_loss: 0.4710 - val_accuracy: 0.8333\n",
            "Epoch 53/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3773 - accuracy: 0.9440 - val_loss: 0.5002 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3923 - accuracy: 0.9291 - val_loss: 0.4546 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3771 - accuracy: 0.9403 - val_loss: 0.4748 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3930 - accuracy: 0.9291 - val_loss: 0.4804 - val_accuracy: 0.8167\n",
            "Epoch 57/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3837 - accuracy: 0.9366 - val_loss: 0.4670 - val_accuracy: 0.8333\n",
            "Epoch 58/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3695 - accuracy: 0.9515 - val_loss: 0.4369 - val_accuracy: 0.8833\n",
            "Epoch 59/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3830 - accuracy: 0.9328 - val_loss: 0.4340 - val_accuracy: 0.8833\n",
            "Epoch 60/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3775 - accuracy: 0.9403 - val_loss: 0.4542 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3674 - accuracy: 0.9515 - val_loss: 0.4400 - val_accuracy: 0.8833\n",
            "Epoch 62/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3747 - accuracy: 0.9403 - val_loss: 0.4501 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3693 - accuracy: 0.9478 - val_loss: 0.4781 - val_accuracy: 0.8167\n",
            "Epoch 64/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3625 - accuracy: 0.9552 - val_loss: 0.4351 - val_accuracy: 0.8833\n",
            "Epoch 65/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3838 - accuracy: 0.9291 - val_loss: 0.4327 - val_accuracy: 0.8833\n",
            "Epoch 66/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9478 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
            "Epoch 67/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9515 - val_loss: 0.4767 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9440 - val_loss: 0.4532 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3763 - accuracy: 0.9366 - val_loss: 0.4743 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3589 - accuracy: 0.9552 - val_loss: 0.4877 - val_accuracy: 0.8167\n",
            "Epoch 71/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3560 - accuracy: 0.9664 - val_loss: 0.4616 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3646 - accuracy: 0.9552 - val_loss: 0.4886 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3543 - accuracy: 0.9627 - val_loss: 0.5030 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3562 - accuracy: 0.9627 - val_loss: 0.5077 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3523 - accuracy: 0.9664 - val_loss: 0.4707 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3576 - accuracy: 0.9590 - val_loss: 0.4801 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3603 - accuracy: 0.9552 - val_loss: 0.5191 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3669 - accuracy: 0.9440 - val_loss: 0.4686 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3591 - accuracy: 0.9515 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3708 - accuracy: 0.9403 - val_loss: 0.4465 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3495 - accuracy: 0.9701 - val_loss: 0.4657 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9552 - val_loss: 0.4355 - val_accuracy: 0.8833\n",
            "Epoch 83/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3573 - accuracy: 0.9515 - val_loss: 0.4195 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3440 - accuracy: 0.9739 - val_loss: 0.4263 - val_accuracy: 0.8833\n",
            "Epoch 85/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3515 - accuracy: 0.9664 - val_loss: 0.5191 - val_accuracy: 0.7833\n",
            "Epoch 86/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9627 - val_loss: 0.4628 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3695 - accuracy: 0.9440 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3635 - accuracy: 0.9515 - val_loss: 0.4759 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3463 - accuracy: 0.9664 - val_loss: 0.5083 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3585 - accuracy: 0.9515 - val_loss: 0.4583 - val_accuracy: 0.8333\n",
            "Epoch 91/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3517 - accuracy: 0.9590 - val_loss: 0.4490 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3496 - accuracy: 0.9701 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9590 - val_loss: 0.4643 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3498 - accuracy: 0.9590 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3492 - accuracy: 0.9627 - val_loss: 0.4788 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3438 - accuracy: 0.9664 - val_loss: 0.4847 - val_accuracy: 0.8167\n",
            "Epoch 97/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3446 - accuracy: 0.9739 - val_loss: 0.5205 - val_accuracy: 0.7833\n",
            "Epoch 98/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.3442 - accuracy: 0.9664 - val_loss: 0.4866 - val_accuracy: 0.8167\n",
            "Epoch 99/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3369 - accuracy: 0.9813 - val_loss: 0.4866 - val_accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3466 - accuracy: 0.9701 - val_loss: 0.4513 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb84798d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeGlE_Oz-RjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
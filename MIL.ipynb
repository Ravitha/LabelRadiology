{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravitha/LabelRadiology/blob/master/MIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arW_6_PrkIlZ",
        "colab_type": "text"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthF7sh_UZX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dicom # some machines not install pydicom\n",
        "import scipy.misc\n",
        "import numpy as np \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle as cPickle\n",
        "#import matplotlib\n",
        "#import matplotlib.pyplot as plt \n",
        "from skimage.filters import threshold_otsu\n",
        "import os\n",
        "from os.path import join as join\n",
        "import csv\n",
        "import scipy.ndimage\n",
        "#import pydicom as dicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82DTupaeUZYI",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evR_WImPaUN0",
        "colab_type": "code",
        "outputId": "98194496-fbc1-4be3-9c4f-42a23f8283f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcQkq4ShkC-6",
        "colab_type": "text"
      },
      "source": [
        "# Compose Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzRMqmRi0fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/drive/My Drive/CO-PO/INBreast'\n",
        "TrDataset = np.load(join(path,'Train.npy'))\n",
        "TrLabel = np.load(join(path,'TrainL.npy'))\n",
        "ValDataset= np.load(join(path,'valid.npy'))\n",
        "ValLabel = np.load(join(path,'validL.npy'))\n",
        "TDataset = np.load(join(path,'Test.npy'))\n",
        "TLabel = np.load(join(path,'TestL.npy'))\n",
        "\n",
        "Dataset = np.concatenate((TrDataset,ValDataset,TDataset),axis = 0)\n",
        "Labels = np.concatenate((TrLabel,ValLabel,TLabel),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRB7_brfjY3g",
        "colab_type": "code",
        "outputId": "6797f0e9-9199-417b-b7d0-78f6ac91001c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "np.save('Dataset.npy', Dataset)\n",
        "np.save('Label.npy', Labels)\n",
        "\n",
        "np.unique(Labels, return_counts=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256)\n",
            "(410,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crX86A3FUZZI",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXWpwOAkOVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "nb_classes = 2\n",
        "Dataset = Dataset.reshape((410,256,256,1))\n",
        "Dataset_extend = np.zeros((Dataset.shape[0],256, 256,3))\n",
        "for i in range(Dataset.shape[0]):\n",
        "    rex = np.resize(Dataset[i,:,:,:], (256, 256))\n",
        "    Dataset_extend[i,:,:,0] = rex\n",
        "    Dataset_extend[i,:,:,1] = rex\n",
        "    Dataset_extend[i,:,:,2] = rex\n",
        "Dataset = Dataset_extend\n",
        "Labels = np_utils.to_categorical(Labels, nb_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqsOUAXIlSL1",
        "colab_type": "text"
      },
      "source": [
        "# Print Data Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNitNFUBlQXT",
        "colab_type": "code",
        "outputId": "3ec24c9e-1975-4fb9-f8b1-a463c2e65f1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256, 3)\n",
            "(410, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_WtGDWTeiI",
        "colab_type": "code",
        "outputId": "92d5db44-cdf3-47f3-9537-121012739e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.unique(Labels[:,1],return_counts = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.], dtype=float32), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4qn6IkmSC1",
        "colab_type": "text"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKoXcmbmUCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model1():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_PMVBLi_l1q",
        "colab_type": "text"
      },
      "source": [
        "## Using K Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSm1LGMzKbeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class KPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(KPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],64))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output = tf.sort(x)\n",
        "        output=K.concatenate([1-output[:,:-2], output[:,62:64]])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 64)\n",
        "\n",
        "def model3():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = KPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXNmazY__4v4",
        "colab_type": "text"
      },
      "source": [
        "## Use Sparsity Constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN0ILfCIiRLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        max_val = K.max(x, axis=-1,keepdims=True)\n",
        "        self.add_loss(l1(1e-5)(x))\n",
        "        output=K.concatenate([1-max_val,max_val])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model4():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9SHG1mD_a08",
        "colab_type": "text"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLHg7YfSOZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "def model2():\n",
        "\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  \n",
        "  out1 = Lambda(lambda image: tf.image.resize(image, (4, 4)))(model.output)\n",
        "  out2 = Lambda(lambda image: tf.image.resize(image, (2, 2)))(model.output)\n",
        "  out3 = Lambda(lambda image: tf.image.resize(image, (6, 6)))(model.output)\n",
        "\n",
        "  SL1 = Conv2D(1, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "  SL2 = BatchNormalization()\n",
        "  SL3 = Conv2D(1, 1, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "\n",
        "  resize1 = Flatten()(SL3(SL2(SL1(out1))))\n",
        "  resize2 = Flatten()(SL3(SL2(SL1(out2))))\n",
        "  resize3 = Flatten()(SL3(SL2(SL1(out3))))\n",
        "\n",
        "  M1 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize1)\n",
        "  M2 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize2)\n",
        "  M3 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize3)\n",
        "\n",
        "  added10 = concatenate([M1,M2,M3], axis=1)\n",
        "  final = Dense(2, activation='softmax')(added10)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIfSI5K___Z6",
        "colab_type": "text"
      },
      "source": [
        "# Model 3 (ResNet Multiscale)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rym-sZ_dxvp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "def model5():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "\n",
        "  logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "  dense_2 = Flatten()(logistic1)\n",
        "\n",
        "  logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "  dense_3 = Flatten()(logistic2)\n",
        "\n",
        "  logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "  dense_4 = Flatten()(logistic3)\n",
        "\n",
        "  final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "  prediction = MaxPool(axis=1)(final)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTMerT0GAKAF",
        "colab_type": "text"
      },
      "source": [
        "## ResNet 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K922EQst61r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model6():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  dense = Flatten()(model.output)\n",
        "  final = Dense(2, activation='softmax')(dense)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7mepqnFBBQl",
        "colab_type": "text"
      },
      "source": [
        "# Model 4 (Attention with ResNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-O8n7VBBJor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten,UpSampling2D, Multiply\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "def model6():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  G = model.get_layer('conv5_block3_out').output\n",
        "  G = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(G)\n",
        "  G_flatten = Flatten()(G)\n",
        "  L1 = model.get_layer('conv4_block6_out').output\n",
        "  L1 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L1)\n",
        "  G_modified = UpSampling2D(size = (2,2), interpolation='bilinear')(G)\n",
        "  L1_modified = Multiply()([L1,G_modified])\n",
        "  L1_modified = Activation('softmax')(L1_modified)\n",
        "  L1_modified = Multiply()([L1_modified,G_modified])\n",
        "  L1_modified = Flatten()(L1_modified)\n",
        "\n",
        "  L2 = model.get_layer('conv3_block4_out').output\n",
        "  L2 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L2)\n",
        "  G_modified = UpSampling2D(size = (4,4), interpolation='bilinear')(G)\n",
        "  L2_modified = Multiply()([L2,G_modified])\n",
        "  L2_modified = Activation('softmax')(L2_modified)\n",
        "  L2_modified = Multiply()([L2_modified,G_modified])\n",
        "  L2_modified = Flatten()(L2_modified)\n",
        "\n",
        "  L3 = model.get_layer('conv2_block3_out').output\n",
        "  L3 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L3)\n",
        "  G_modified = UpSampling2D(size = (8,8), interpolation='bilinear')(G)\n",
        "  L3_modified = Multiply()([L3,G_modified])\n",
        "  L3_modified = Activation('softmax')(L3_modified)\n",
        "  L3_modified = Multiply()([L3_modified,G_modified])\n",
        "  L3_modified = Flatten()(L3_modified)\n",
        "\n",
        "  final = concatenate([L1_modified, L2_modified,L3_modified])\n",
        "  prediction = MaxPool(axis=1)(final)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssCxcR7lutw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFdleZCluLO",
        "colab_type": "code",
        "outputId": "38d4fc97-c922-4175-af28-94d3f32796a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Necessary Imports\n",
        "import numpy as np \n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "print('[INFO:Applying CV]')\n",
        "skf = KFold(n_splits=5)\n",
        "skf.get_n_splits(Dataset)\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(Dataset):\n",
        "    print(['FOLD:'], fold)\n",
        "    # Construct data from indexes\n",
        "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
        "    y_train, y_test = Labels[train_index], Labels[test_index]\n",
        "    \n",
        "    # Model Construction\n",
        "    model = model6()\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "    datagen.fit(X_train)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "    Stopping_Criterion = EarlyStopping(patience=50, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint('./SEfold'+str(fold)+'{epoch:08d}{val_accuracy:05f}.hdf5', monitor='val_accuracy',verbose=1,save_best_only=True)\n",
        "    model.fit_generator(datagen.flow(X_train, y_train,batch_size=10),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_test , y_test ),callbacks=[model_checkpoint, Stopping_Criterion])\n",
        "    datafilename = 'TestFold'+str(fold)+'.npy'\n",
        "    labelfilename = 'TestLabelFold'+str(fold)+'.npy'\n",
        "    np.save(datafilename,X_test)\n",
        "    np.save(labelfilename,y_test)\n",
        "    fold+=1\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO:Applying CV]\n",
            "['FOLD:'] 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 31s 940ms/step - loss: 1.0291 - accuracy: 0.2470 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold1000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.8052 - accuracy: 0.4421 - val_loss: 1.0692 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.5946 - accuracy: 0.7378 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.24390\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.5867 - accuracy: 0.7195 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.24390\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.5907 - accuracy: 0.7226 - val_loss: 0.7565 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.24390\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5230 - accuracy: 0.7927 - val_loss: 0.8368 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.24390\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.5017 - accuracy: 0.8049 - val_loss: 0.8654 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.24390\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.5208 - accuracy: 0.7866 - val_loss: 0.6961 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.24390 to 0.50000, saving model to ./SEfold1000000080.500000.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.5242 - accuracy: 0.7866 - val_loss: 0.7047 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.50000\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4734 - accuracy: 0.8354 - val_loss: 0.7134 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.50000\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4833 - accuracy: 0.8232 - val_loss: 0.6658 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.50000 to 0.75610, saving model to ./SEfold1000000110.756098.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.5136 - accuracy: 0.7896 - val_loss: 0.6173 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4543 - accuracy: 0.8598 - val_loss: 0.8041 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4812 - accuracy: 0.8262 - val_loss: 0.7678 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4824 - accuracy: 0.8262 - val_loss: 0.6835 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.5321 - accuracy: 0.7774 - val_loss: 0.6722 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4958 - accuracy: 0.8140 - val_loss: 0.6628 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4656 - accuracy: 0.8506 - val_loss: 0.6616 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4673 - accuracy: 0.8445 - val_loss: 0.8402 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4764 - accuracy: 0.8384 - val_loss: 0.6151 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4692 - accuracy: 0.8415 - val_loss: 0.6796 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4580 - accuracy: 0.8537 - val_loss: 0.8824 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4558 - accuracy: 0.8537 - val_loss: 1.0355 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4722 - accuracy: 0.8384 - val_loss: 1.0660 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4440 - accuracy: 0.8659 - val_loss: 0.8472 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4531 - accuracy: 0.8506 - val_loss: 1.0079 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4209 - accuracy: 0.8994 - val_loss: 0.9642 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4331 - accuracy: 0.8689 - val_loss: 1.0562 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4348 - accuracy: 0.8750 - val_loss: 0.9811 - val_accuracy: 0.2195\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4465 - accuracy: 0.8689 - val_loss: 1.0411 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4438 - accuracy: 0.8689 - val_loss: 0.8359 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4412 - accuracy: 0.8659 - val_loss: 1.0582 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4438 - accuracy: 0.8689 - val_loss: 0.6777 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4408 - accuracy: 0.8720 - val_loss: 0.6528 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.75610\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4406 - accuracy: 0.8659 - val_loss: 0.6605 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.75610\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4217 - accuracy: 0.8780 - val_loss: 0.6335 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.75610\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4440 - accuracy: 0.8659 - val_loss: 0.6662 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.75610\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4786 - accuracy: 0.8262 - val_loss: 0.6184 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.75610\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4222 - accuracy: 0.8933 - val_loss: 0.5950 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.75610\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4655 - accuracy: 0.8323 - val_loss: 0.8446 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.75610\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4121 - accuracy: 0.8994 - val_loss: 0.5296 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.75610\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4075 - accuracy: 0.9055 - val_loss: 0.5353 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00042: val_accuracy improved from 0.75610 to 0.78049, saving model to ./SEfold1000000420.780488.hdf5\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4236 - accuracy: 0.8750 - val_loss: 0.5640 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.78049\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4294 - accuracy: 0.8872 - val_loss: 0.5082 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.78049 to 0.80488, saving model to ./SEfold1000000440.804878.hdf5\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4283 - accuracy: 0.8780 - val_loss: 0.5371 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.80488\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4306 - accuracy: 0.8780 - val_loss: 0.5523 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.80488\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4181 - accuracy: 0.8963 - val_loss: 0.5207 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.80488\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4162 - accuracy: 0.8963 - val_loss: 0.5478 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.80488\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4274 - accuracy: 0.8811 - val_loss: 0.4979 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00049: val_accuracy improved from 0.80488 to 0.81707, saving model to ./SEfold1000000490.817073.hdf5\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4402 - accuracy: 0.8628 - val_loss: 0.5393 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.81707\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4371 - accuracy: 0.8811 - val_loss: 0.4849 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.81707 to 0.82927, saving model to ./SEfold1000000510.829268.hdf5\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4147 - accuracy: 0.8994 - val_loss: 0.5017 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.82927\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4189 - accuracy: 0.8902 - val_loss: 0.4602 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.82927 to 0.85366, saving model to ./SEfold1000000530.853659.hdf5\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4107 - accuracy: 0.8963 - val_loss: 0.5263 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.85366\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4151 - accuracy: 0.8963 - val_loss: 0.5076 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.85366\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4083 - accuracy: 0.9024 - val_loss: 0.5506 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.85366\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4222 - accuracy: 0.8841 - val_loss: 0.4823 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.85366\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4297 - accuracy: 0.8841 - val_loss: 0.5368 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.85366\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4179 - accuracy: 0.8933 - val_loss: 0.4867 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.85366\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4226 - accuracy: 0.8933 - val_loss: 0.4743 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.85366\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4101 - accuracy: 0.9024 - val_loss: 0.6085 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.85366\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3916 - accuracy: 0.9238 - val_loss: 0.5388 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.85366\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.3884 - accuracy: 0.9268 - val_loss: 0.5541 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.85366\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.3917 - accuracy: 0.9207 - val_loss: 0.6548 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.85366\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3861 - accuracy: 0.9299 - val_loss: 0.5927 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.85366\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4063 - accuracy: 0.9085 - val_loss: 0.4785 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.85366\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4174 - accuracy: 0.8933 - val_loss: 0.5183 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.85366\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.3991 - accuracy: 0.9116 - val_loss: 0.4990 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.85366\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4381 - accuracy: 0.8689 - val_loss: 0.5075 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.85366\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4085 - accuracy: 0.8994 - val_loss: 0.4961 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.85366\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.3934 - accuracy: 0.9146 - val_loss: 0.5065 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.85366\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.3954 - accuracy: 0.9207 - val_loss: 0.4825 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.85366\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4064 - accuracy: 0.9085 - val_loss: 0.5057 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.85366\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3929 - accuracy: 0.9146 - val_loss: 0.5067 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.85366\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3846 - accuracy: 0.9299 - val_loss: 0.4837 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.85366\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4029 - accuracy: 0.9085 - val_loss: 0.5218 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.85366\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4253 - accuracy: 0.8841 - val_loss: 0.5191 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.85366\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4199 - accuracy: 0.8902 - val_loss: 0.4752 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.85366\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4026 - accuracy: 0.9116 - val_loss: 0.4673 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.85366\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4443 - accuracy: 0.8659 - val_loss: 0.5061 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.85366\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4192 - accuracy: 0.8933 - val_loss: 0.4832 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.85366\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4253 - accuracy: 0.8811 - val_loss: 0.4808 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.85366\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4119 - accuracy: 0.9024 - val_loss: 0.5305 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.85366\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3889 - accuracy: 0.9207 - val_loss: 0.5572 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.85366\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4109 - accuracy: 0.9024 - val_loss: 0.5128 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.85366\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3980 - accuracy: 0.9177 - val_loss: 0.4977 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.85366\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.3974 - accuracy: 0.9177 - val_loss: 0.5375 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.85366\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4114 - accuracy: 0.8963 - val_loss: 0.4802 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.85366\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4362 - accuracy: 0.8720 - val_loss: 0.5345 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.85366\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.3830 - accuracy: 0.9299 - val_loss: 0.4698 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.85366\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.3879 - accuracy: 0.9238 - val_loss: 0.4663 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.85366\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4112 - accuracy: 0.8994 - val_loss: 0.4639 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.85366\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.3754 - accuracy: 0.9360 - val_loss: 0.5220 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.85366\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.3953 - accuracy: 0.9207 - val_loss: 0.4784 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.85366\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3897 - accuracy: 0.9207 - val_loss: 0.4833 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.85366\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3974 - accuracy: 0.9055 - val_loss: 0.5338 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.85366\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.3949 - accuracy: 0.9177 - val_loss: 0.7195 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.85366\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4023 - accuracy: 0.9085 - val_loss: 0.6930 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.85366\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4031 - accuracy: 0.9055 - val_loss: 0.5946 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.85366\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4829 - accuracy: 0.8323 - val_loss: 0.5028 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.85366\n",
            "['FOLD:'] 2\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 30s 918ms/step - loss: 1.0385 - accuracy: 0.2409 - val_loss: 1.0680 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold2000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.8475 - accuracy: 0.4207 - val_loss: 1.0693 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.6481 - accuracy: 0.6585 - val_loss: 0.6660 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.24390 to 0.75610, saving model to ./SEfold2000000030.756098.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5637 - accuracy: 0.7561 - val_loss: 0.5963 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5568 - accuracy: 0.7591 - val_loss: 0.5657 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5442 - accuracy: 0.7713 - val_loss: 0.5711 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.5469 - accuracy: 0.7652 - val_loss: 0.5860 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5057 - accuracy: 0.8049 - val_loss: 0.5943 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5209 - accuracy: 0.7896 - val_loss: 0.6512 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5098 - accuracy: 0.8171 - val_loss: 0.6259 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4740 - accuracy: 0.8445 - val_loss: 0.6018 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4733 - accuracy: 0.8354 - val_loss: 0.7361 - val_accuracy: 0.4024\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4879 - accuracy: 0.8232 - val_loss: 0.6100 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4559 - accuracy: 0.8567 - val_loss: 0.5868 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4583 - accuracy: 0.8567 - val_loss: 0.5763 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4210 - accuracy: 0.8872 - val_loss: 0.5843 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4495 - accuracy: 0.8628 - val_loss: 0.5785 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4533 - accuracy: 0.8567 - val_loss: 0.5664 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4747 - accuracy: 0.8384 - val_loss: 0.5592 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4563 - accuracy: 0.8567 - val_loss: 0.5775 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4841 - accuracy: 0.8262 - val_loss: 0.5696 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4482 - accuracy: 0.8659 - val_loss: 0.7939 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4446 - accuracy: 0.8689 - val_loss: 0.8348 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4320 - accuracy: 0.8811 - val_loss: 0.6347 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4768 - accuracy: 0.8323 - val_loss: 1.0635 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4349 - accuracy: 0.8689 - val_loss: 1.0678 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 15s 469ms/step - loss: 0.4333 - accuracy: 0.8780 - val_loss: 0.6168 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4355 - accuracy: 0.8780 - val_loss: 0.5870 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4399 - accuracy: 0.8720 - val_loss: 0.5687 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4518 - accuracy: 0.8598 - val_loss: 0.5557 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4204 - accuracy: 0.8902 - val_loss: 0.5561 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4181 - accuracy: 0.8994 - val_loss: 0.5562 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4437 - accuracy: 0.8598 - val_loss: 0.5435 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.75610 to 0.76829, saving model to ./SEfold2000000330.768293.hdf5\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4167 - accuracy: 0.8963 - val_loss: 0.5469 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.76829\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4595 - accuracy: 0.8537 - val_loss: 0.5479 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.76829\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4378 - accuracy: 0.8689 - val_loss: 0.5435 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.76829\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4455 - accuracy: 0.8628 - val_loss: 0.5470 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.76829\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4378 - accuracy: 0.8689 - val_loss: 0.5439 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.76829\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4473 - accuracy: 0.8628 - val_loss: 0.5419 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.76829\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4212 - accuracy: 0.8933 - val_loss: 0.5007 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00040: val_accuracy improved from 0.76829 to 0.80488, saving model to ./SEfold2000000400.804878.hdf5\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4513 - accuracy: 0.8598 - val_loss: 0.5253 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.80488\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4326 - accuracy: 0.8841 - val_loss: 0.5360 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.80488\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4377 - accuracy: 0.8750 - val_loss: 0.5476 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.80488\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4408 - accuracy: 0.8689 - val_loss: 0.5636 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.80488\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4296 - accuracy: 0.8780 - val_loss: 0.5604 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.80488\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4269 - accuracy: 0.8872 - val_loss: 0.5285 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.80488\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4025 - accuracy: 0.9146 - val_loss: 0.5288 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.80488\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4185 - accuracy: 0.8902 - val_loss: 0.5520 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.80488\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4072 - accuracy: 0.9024 - val_loss: 0.5508 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.80488\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4361 - accuracy: 0.8780 - val_loss: 0.5418 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.80488\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4605 - accuracy: 0.8537 - val_loss: 0.5447 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.80488\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4362 - accuracy: 0.8811 - val_loss: 0.5565 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.80488\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4133 - accuracy: 0.9024 - val_loss: 0.5536 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.80488\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4187 - accuracy: 0.8933 - val_loss: 0.5357 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.80488\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4067 - accuracy: 0.9055 - val_loss: 0.5491 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.80488\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4310 - accuracy: 0.8841 - val_loss: 0.5490 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.80488\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4226 - accuracy: 0.8872 - val_loss: 0.5072 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.80488\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4195 - accuracy: 0.8933 - val_loss: 0.5351 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.80488\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4250 - accuracy: 0.8872 - val_loss: 0.5212 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.80488\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4146 - accuracy: 0.8994 - val_loss: 0.5241 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.80488\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4119 - accuracy: 0.9024 - val_loss: 0.5985 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.80488\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4826 - accuracy: 0.8293 - val_loss: 0.5573 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.80488\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4306 - accuracy: 0.8811 - val_loss: 0.5578 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.80488\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4292 - accuracy: 0.8780 - val_loss: 0.5211 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.80488\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4670 - accuracy: 0.8445 - val_loss: 0.5325 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.80488\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5409 - accuracy: 0.7622 - val_loss: 0.5776 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.80488\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4743 - accuracy: 0.8354 - val_loss: 0.6286 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.80488\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4878 - accuracy: 0.8232 - val_loss: 0.5260 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.80488\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4817 - accuracy: 0.8262 - val_loss: 0.5392 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.80488\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4576 - accuracy: 0.8537 - val_loss: 0.5583 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.80488\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4889 - accuracy: 0.8201 - val_loss: 0.5577 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.80488\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4464 - accuracy: 0.8689 - val_loss: 0.5578 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.80488\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4918 - accuracy: 0.8201 - val_loss: 0.5491 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.80488\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4803 - accuracy: 0.8293 - val_loss: 0.5423 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.80488\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4617 - accuracy: 0.8537 - val_loss: 0.5614 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.80488\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4866 - accuracy: 0.8262 - val_loss: 0.5584 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.80488\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4960 - accuracy: 0.8171 - val_loss: 0.5338 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.80488\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4731 - accuracy: 0.8384 - val_loss: 0.5233 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.80488\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4871 - accuracy: 0.8232 - val_loss: 0.5131 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.80488\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4967 - accuracy: 0.8140 - val_loss: 0.5072 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.80488\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4423 - accuracy: 0.8689 - val_loss: 0.5615 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.80488\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4388 - accuracy: 0.8720 - val_loss: 0.5654 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.80488\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4360 - accuracy: 0.8780 - val_loss: 0.5337 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.80488\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4202 - accuracy: 0.8872 - val_loss: 0.5087 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.80488\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4538 - accuracy: 0.8567 - val_loss: 0.5572 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.80488\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4307 - accuracy: 0.8841 - val_loss: 0.5312 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.80488\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4315 - accuracy: 0.8811 - val_loss: 0.5632 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.80488\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.4424 - accuracy: 0.8720 - val_loss: 0.4994 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00088: val_accuracy improved from 0.80488 to 0.81707, saving model to ./SEfold2000000880.817073.hdf5\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4080 - accuracy: 0.9055 - val_loss: 0.5337 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.81707\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 15s 466ms/step - loss: 0.4208 - accuracy: 0.8902 - val_loss: 0.5571 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.81707\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4280 - accuracy: 0.8872 - val_loss: 0.4934 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.81707\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.5427 - accuracy: 0.7652 - val_loss: 0.5106 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.81707\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4603 - accuracy: 0.8476 - val_loss: 0.5160 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.81707\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4648 - accuracy: 0.8476 - val_loss: 0.4613 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00094: val_accuracy improved from 0.81707 to 0.85366, saving model to ./SEfold2000000940.853659.hdf5\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4626 - accuracy: 0.8506 - val_loss: 0.4947 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.85366\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4587 - accuracy: 0.8537 - val_loss: 0.5531 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.85366\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4746 - accuracy: 0.8323 - val_loss: 0.5532 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.85366\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4387 - accuracy: 0.8628 - val_loss: 0.5355 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.85366\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 15s 467ms/step - loss: 0.4643 - accuracy: 0.8415 - val_loss: 0.5070 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.85366\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 15s 468ms/step - loss: 0.4277 - accuracy: 0.8811 - val_loss: 0.5563 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.85366\n",
            "['FOLD:'] 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c6d5264717f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     model.fit_generator(datagen.flow(X_train, y_train,batch_size=10),\n\u001b[1;32m     38\u001b[0m                         \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                         validation_data=(X_test , y_test ),callbacks=[model_checkpoint, Stopping_Criterion])\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mdatafilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'TestFold'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mlabelfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'TestLabelFold'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    315\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvhat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvhats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mm_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m             \u001b[0mv_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(a, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    995\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6090\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6091\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 6092\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   6093\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6094\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0;31m# Default name if not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_type_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0J3YG2DUZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def classifyEval(testX, testy,model):\n",
        "  # predict probabilities for test set  \n",
        "  yhat_probs = model.predict(testX, verbose=0)\n",
        "  yhat = np.max(yhat_probs,axis=1)\n",
        "  yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "  x=1\n",
        "  print(testy[:,x])\n",
        "  print(yhat_classes)\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(testy[:,x], yhat_classes)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(testy[:,x], yhat_classes)\n",
        "  print('Precision: %f' % precision)\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(testy[:,x], yhat_classes)\n",
        "  print('Recall: %f' % recall)\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(testy[:,x], yhat_classes)\n",
        "  print('F1 score: %f' % f1)\n",
        " \n",
        "  # kappa\n",
        "  kappa = cohen_kappa_score(testy[:,x], yhat_classes)\n",
        "  print('Cohens kappa: %f' % kappa)\n",
        "  # ROC AUC\n",
        "  auc = roc_auc_score(testy[:,x], yhat_probs[:,x])\n",
        "  print('ROC AUC: %f' % auc)\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(testy[:,x], yhat_classes)\n",
        "  print(matrix)\n",
        "\n",
        "  fpr_keras, tpr_keras, thresholds_keras = sklearn.metrics.roc_curve(testy[:,x], yhat_probs[:,x])\n",
        "  return fpr_keras, tpr_keras, thresholds_keras\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.plot(fpr_keras, tpr_keras, label='CV1 (area = {:.3f})'.format(auc))\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acURLMezCxA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpr_list = []\n",
        "fpr_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxXwIdDaTncb",
        "colab_type": "code",
        "outputId": "7776d07c-5526-4c1b-ecc7-65c6f2498799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold1.npy')\n",
        "Y_test = np.load('/content/TestLabelFold1.npy')\n",
        "model = model6()\n",
        "model.load_weights('/content/SEfold1000000530.853659.hdf5')\n",
        "fpr, tpr, _ = classifyEval(X_test_extend, Y_test, model)\n",
        "fpr_list.append(fpr)\n",
        "tpr_list.append(tpr)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.\n",
            " 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0]\n",
            "Accuracy: 0.853659\n",
            "Precision: 0.900000\n",
            "Recall: 0.450000\n",
            "F1 score: 0.600000\n",
            "Cohens kappa: 0.522330\n",
            "ROC AUC: 0.761290\n",
            "[[61  1]\n",
            " [11  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q99ruMRwFMuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auc = [.95,.80,.73,.84,.69]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2ejE2XEHlN",
        "colab_type": "code",
        "outputId": "8fad6c41-dc6f-4747-da18-152a13bd537c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "for i in range(5):\n",
        "  plt.plot(fpr_list[i], tpr_list[i],label='CV{} (area = {:.2f})'.format(i,auc[i]))\n",
        "  plt.legend(loc='CV{}'.format(i))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV0'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV1'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV2'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV3'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV4'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuERoQ6CUZZ0",
        "colab_type": "text"
      },
      "source": [
        "## Attention Model for Multi Instance Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtK_lHXrUZZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featureextractor(data):\n",
        "    conv = Conv2D(20, 5, activation = None, padding = 'same')(data)\n",
        "    conv_a = Activation('relu')(BatchNormalization()(conv))\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv_a)\n",
        "    conv1 = Conv2D(50, 5, activation = None, padding = 'same')(pool1)\n",
        "    conv1_a = Activation('relu')(BatchNormalization()(conv1))\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
        "    return pool2\n",
        "\n",
        "def featureextractorx2(data):\n",
        "    dense = Dense(500,activation='relu')(data)\n",
        "    return dense\n",
        "\n",
        "def Attention(data):\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    return dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3miMxRUZZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "l1factor=1e-2\n",
        "l2factor=0\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "prediction = MaxPool(axis=1)(dense_1)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbRDRdVcYkP",
        "colab_type": "code",
        "outputId": "fab69d65-a6a7-48f0-f900-18355494f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIHWT6X6fPv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUaTaZWirf0",
        "colab_type": "code",
        "outputId": "c155dade-a1b1-43bb-cb6f-7c13d55aadec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}\n",
        "Features = []\n",
        "\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "\n",
        "logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "dense_2 = Flatten()(logistic1)\n",
        "\n",
        "logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "dense_3 = Flatten()(logistic2)\n",
        "\n",
        "logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "dense_4 = Flatten()(logistic3)\n",
        "\n",
        "final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "prediction = MaxPool(axis=1)(final)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FwRbn0eiwib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "noises=50\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "datagen.fit(X_train_extend)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bg_PaepkBhP",
        "colab_type": "code",
        "outputId": "9567289f-268b-42bd-9eda-a9d2e29f67ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.00001),\n",
        "              metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(X_train_extend, Y_train,batch_size=2),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_valid_extend, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., epochs=100)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134/134 [==============================] - 35s 264ms/step - loss: 0.6514 - accuracy: 0.7127 - val_loss: 0.8838 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.6267 - accuracy: 0.7425 - val_loss: 0.7636 - val_accuracy: 0.4333\n",
            "Epoch 3/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.6024 - accuracy: 0.7836 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.5672 - accuracy: 0.8507 - val_loss: 0.5682 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5584 - accuracy: 0.8433 - val_loss: 0.5442 - val_accuracy: 0.8167\n",
            "Epoch 6/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.5589 - accuracy: 0.8097 - val_loss: 0.5946 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5288 - accuracy: 0.8545 - val_loss: 0.5800 - val_accuracy: 0.7667\n",
            "Epoch 8/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5151 - accuracy: 0.8582 - val_loss: 0.5377 - val_accuracy: 0.8500\n",
            "Epoch 9/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5261 - accuracy: 0.8358 - val_loss: 0.5270 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5130 - accuracy: 0.8470 - val_loss: 0.5216 - val_accuracy: 0.8500\n",
            "Epoch 11/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4840 - accuracy: 0.8731 - val_loss: 0.5267 - val_accuracy: 0.8167\n",
            "Epoch 12/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4953 - accuracy: 0.8619 - val_loss: 0.5123 - val_accuracy: 0.8167\n",
            "Epoch 13/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5000 - accuracy: 0.8433 - val_loss: 0.4984 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4907 - accuracy: 0.8507 - val_loss: 0.4877 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4814 - accuracy: 0.8694 - val_loss: 0.4697 - val_accuracy: 0.8833\n",
            "Epoch 16/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4705 - accuracy: 0.8806 - val_loss: 0.4655 - val_accuracy: 0.8833\n",
            "Epoch 17/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4634 - accuracy: 0.8843 - val_loss: 0.4711 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4425 - accuracy: 0.9030 - val_loss: 0.4611 - val_accuracy: 0.8833\n",
            "Epoch 19/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4562 - accuracy: 0.8843 - val_loss: 0.4742 - val_accuracy: 0.8500\n",
            "Epoch 20/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4466 - accuracy: 0.8881 - val_loss: 0.4702 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4490 - accuracy: 0.8843 - val_loss: 0.4764 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4406 - accuracy: 0.8993 - val_loss: 0.4772 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9067 - val_loss: 0.4578 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4504 - accuracy: 0.8881 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4518 - accuracy: 0.8806 - val_loss: 0.5093 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4471 - accuracy: 0.8769 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9104 - val_loss: 0.4399 - val_accuracy: 0.9000\n",
            "Epoch 28/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4114 - accuracy: 0.9254 - val_loss: 0.4405 - val_accuracy: 0.8833\n",
            "Epoch 29/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4215 - accuracy: 0.9067 - val_loss: 0.4320 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4280 - accuracy: 0.8918 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4124 - accuracy: 0.9179 - val_loss: 0.4489 - val_accuracy: 0.8833\n",
            "Epoch 32/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4201 - accuracy: 0.9067 - val_loss: 0.4370 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4097 - accuracy: 0.9104 - val_loss: 0.4642 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4228 - accuracy: 0.8993 - val_loss: 0.4591 - val_accuracy: 0.8500\n",
            "Epoch 35/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4080 - accuracy: 0.9179 - val_loss: 0.4386 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4136 - accuracy: 0.8993 - val_loss: 0.4663 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "134/134 [==============================] - 17s 124ms/step - loss: 0.4195 - accuracy: 0.8955 - val_loss: 0.4612 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4086 - accuracy: 0.9067 - val_loss: 0.4633 - val_accuracy: 0.8333\n",
            "Epoch 39/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4060 - accuracy: 0.9179 - val_loss: 0.4412 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4010 - accuracy: 0.9142 - val_loss: 0.4987 - val_accuracy: 0.8167\n",
            "Epoch 41/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4112 - accuracy: 0.9104 - val_loss: 0.4959 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3995 - accuracy: 0.9254 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3979 - accuracy: 0.9142 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4013 - accuracy: 0.9216 - val_loss: 0.4608 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3892 - accuracy: 0.9254 - val_loss: 0.4543 - val_accuracy: 0.8833\n",
            "Epoch 46/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3986 - accuracy: 0.9216 - val_loss: 0.4524 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3882 - accuracy: 0.9328 - val_loss: 0.4398 - val_accuracy: 0.8833\n",
            "Epoch 48/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3867 - accuracy: 0.9366 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 49/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3776 - accuracy: 0.9440 - val_loss: 0.4824 - val_accuracy: 0.8333\n",
            "Epoch 50/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3888 - accuracy: 0.9291 - val_loss: 0.4923 - val_accuracy: 0.8167\n",
            "Epoch 51/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3899 - accuracy: 0.9291 - val_loss: 0.4734 - val_accuracy: 0.8333\n",
            "Epoch 52/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4009 - accuracy: 0.9179 - val_loss: 0.4710 - val_accuracy: 0.8333\n",
            "Epoch 53/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3773 - accuracy: 0.9440 - val_loss: 0.5002 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3923 - accuracy: 0.9291 - val_loss: 0.4546 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3771 - accuracy: 0.9403 - val_loss: 0.4748 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3930 - accuracy: 0.9291 - val_loss: 0.4804 - val_accuracy: 0.8167\n",
            "Epoch 57/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3837 - accuracy: 0.9366 - val_loss: 0.4670 - val_accuracy: 0.8333\n",
            "Epoch 58/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3695 - accuracy: 0.9515 - val_loss: 0.4369 - val_accuracy: 0.8833\n",
            "Epoch 59/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3830 - accuracy: 0.9328 - val_loss: 0.4340 - val_accuracy: 0.8833\n",
            "Epoch 60/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3775 - accuracy: 0.9403 - val_loss: 0.4542 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3674 - accuracy: 0.9515 - val_loss: 0.4400 - val_accuracy: 0.8833\n",
            "Epoch 62/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3747 - accuracy: 0.9403 - val_loss: 0.4501 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3693 - accuracy: 0.9478 - val_loss: 0.4781 - val_accuracy: 0.8167\n",
            "Epoch 64/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3625 - accuracy: 0.9552 - val_loss: 0.4351 - val_accuracy: 0.8833\n",
            "Epoch 65/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3838 - accuracy: 0.9291 - val_loss: 0.4327 - val_accuracy: 0.8833\n",
            "Epoch 66/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9478 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
            "Epoch 67/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9515 - val_loss: 0.4767 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9440 - val_loss: 0.4532 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3763 - accuracy: 0.9366 - val_loss: 0.4743 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3589 - accuracy: 0.9552 - val_loss: 0.4877 - val_accuracy: 0.8167\n",
            "Epoch 71/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3560 - accuracy: 0.9664 - val_loss: 0.4616 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3646 - accuracy: 0.9552 - val_loss: 0.4886 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3543 - accuracy: 0.9627 - val_loss: 0.5030 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3562 - accuracy: 0.9627 - val_loss: 0.5077 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3523 - accuracy: 0.9664 - val_loss: 0.4707 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3576 - accuracy: 0.9590 - val_loss: 0.4801 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3603 - accuracy: 0.9552 - val_loss: 0.5191 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3669 - accuracy: 0.9440 - val_loss: 0.4686 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3591 - accuracy: 0.9515 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3708 - accuracy: 0.9403 - val_loss: 0.4465 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3495 - accuracy: 0.9701 - val_loss: 0.4657 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9552 - val_loss: 0.4355 - val_accuracy: 0.8833\n",
            "Epoch 83/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3573 - accuracy: 0.9515 - val_loss: 0.4195 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3440 - accuracy: 0.9739 - val_loss: 0.4263 - val_accuracy: 0.8833\n",
            "Epoch 85/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3515 - accuracy: 0.9664 - val_loss: 0.5191 - val_accuracy: 0.7833\n",
            "Epoch 86/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9627 - val_loss: 0.4628 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3695 - accuracy: 0.9440 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3635 - accuracy: 0.9515 - val_loss: 0.4759 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3463 - accuracy: 0.9664 - val_loss: 0.5083 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3585 - accuracy: 0.9515 - val_loss: 0.4583 - val_accuracy: 0.8333\n",
            "Epoch 91/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3517 - accuracy: 0.9590 - val_loss: 0.4490 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3496 - accuracy: 0.9701 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9590 - val_loss: 0.4643 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3498 - accuracy: 0.9590 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3492 - accuracy: 0.9627 - val_loss: 0.4788 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3438 - accuracy: 0.9664 - val_loss: 0.4847 - val_accuracy: 0.8167\n",
            "Epoch 97/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3446 - accuracy: 0.9739 - val_loss: 0.5205 - val_accuracy: 0.7833\n",
            "Epoch 98/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.3442 - accuracy: 0.9664 - val_loss: 0.4866 - val_accuracy: 0.8167\n",
            "Epoch 99/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3369 - accuracy: 0.9813 - val_loss: 0.4866 - val_accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3466 - accuracy: 0.9701 - val_loss: 0.4513 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb84798d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeGlE_Oz-RjS",
        "colab_type": "code",
        "outputId": "f36f1173-8550-4982-e5fe-20d4af123438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = model5()\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 64, 64, 1)    257         conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 32, 32, 1)    513         conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 1)    1025        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 1)      2049        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_53 (Flatten)            (None, 4096)         0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_54 (Flatten)            (None, 1024)         0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_55 (Flatten)            (None, 256)          0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_56 (Flatten)            (None, 64)           0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 5440)         0           flatten_53[0][0]                 \n",
            "                                                                 flatten_54[0][0]                 \n",
            "                                                                 flatten_55[0][0]                 \n",
            "                                                                 flatten_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_14 (MaxPool)           (None, 2)            0           concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 2)            0           max_pool_14[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 23,591,556\n",
            "Trainable params: 23,538,436\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfcXjhymHnt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
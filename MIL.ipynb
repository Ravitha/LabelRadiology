{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravitha/LabelRadiology/blob/master/MIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arW_6_PrkIlZ",
        "colab_type": "text"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthF7sh_UZX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dicom # some machines not install pydicom\n",
        "import scipy.misc\n",
        "import numpy as np \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle as cPickle\n",
        "#import matplotlib\n",
        "#import matplotlib.pyplot as plt \n",
        "from skimage.filters import threshold_otsu\n",
        "import os\n",
        "from os.path import join as join\n",
        "import csv\n",
        "import scipy.ndimage\n",
        "#import pydicom as dicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82DTupaeUZYI",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evR_WImPaUN0",
        "colab_type": "code",
        "outputId": "769cb75e-e345-4275-8b48-9eb686842283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcQkq4ShkC-6",
        "colab_type": "text"
      },
      "source": [
        "# Compose Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzRMqmRi0fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/drive/My Drive/CO-PO/INBreast'\n",
        "TrDataset = np.load(join(path,'Train.npy'))\n",
        "TrLabel = np.load(join(path,'TrainL.npy'))\n",
        "ValDataset= np.load(join(path,'valid.npy'))\n",
        "ValLabel = np.load(join(path,'validL.npy'))\n",
        "TDataset = np.load(join(path,'Test.npy'))\n",
        "TLabel = np.load(join(path,'TestL.npy'))\n",
        "\n",
        "Dataset = np.concatenate((TrDataset,ValDataset,TDataset),axis = 0)\n",
        "Labels = np.concatenate((TrLabel,ValLabel,TLabel),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRB7_brfjY3g",
        "colab_type": "code",
        "outputId": "c1ca910a-fbf6-47d4-f91a-be4daed55f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "np.save('Dataset.npy', Dataset)\n",
        "np.save('Label.npy', Labels)\n",
        "\n",
        "np.unique(Labels, return_counts=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256)\n",
            "(410,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crX86A3FUZZI",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXWpwOAkOVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "nb_classes = 2\n",
        "Dataset = Dataset.reshape((410,256,256,1))\n",
        "Dataset_extend = np.zeros((Dataset.shape[0],256, 256,3))\n",
        "for i in range(Dataset.shape[0]):\n",
        "    rex = np.resize(Dataset[i,:,:,:], (256, 256))\n",
        "    Dataset_extend[i,:,:,0] = rex\n",
        "    Dataset_extend[i,:,:,1] = rex\n",
        "    Dataset_extend[i,:,:,2] = rex\n",
        "Dataset = Dataset_extend\n",
        "Labels = np_utils.to_categorical(Labels, nb_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqsOUAXIlSL1",
        "colab_type": "text"
      },
      "source": [
        "# Print Data Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNitNFUBlQXT",
        "colab_type": "code",
        "outputId": "959a57ba-1d16-49e5-b462-a5f90e3747bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256, 3)\n",
            "(410, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4qn6IkmSC1",
        "colab_type": "text"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKoXcmbmUCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model1():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLHg7YfSOZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "def model2():\n",
        "\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  \n",
        "  out1 = Lambda(lambda image: tf.image.resize(image, (4, 4)))(model.output)\n",
        "  out2 = Lambda(lambda image: tf.image.resize(image, (2, 2)))(model.output)\n",
        "  out3 = Lambda(lambda image: tf.image.resize(image, (6, 6)))(model.output)\n",
        "\n",
        "  SL1 = Conv2D(1, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "  SL2 = BatchNormalization()\n",
        "  SL3 = Conv2D(1, 1, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "\n",
        "  resize1 = Flatten()(SL3(SL2(SL1(out1))))\n",
        "  resize2 = Flatten()(SL3(SL2(SL1(out2))))\n",
        "  resize3 = Flatten()(SL3(SL2(SL1(out3))))\n",
        "\n",
        "  M1 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize1)\n",
        "  M2 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize2)\n",
        "  M3 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize3)\n",
        "\n",
        "  added10 = concatenate([M1,M2,M3], axis=1)\n",
        "  final = Dense(2, activation='softmax')(added10)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssCxcR7lutw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFdleZCluLO",
        "colab_type": "code",
        "outputId": "3ccb5f54-e3ff-428f-df20-56adc859929b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Necessary Imports\n",
        "import numpy as np \n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "print('[INFO:Applying CV]')\n",
        "skf = KFold(n_splits=5)\n",
        "skf.get_n_splits(Dataset)\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(Dataset):\n",
        "    print(['FOLD:'], fold)\n",
        "    # Construct data from indexes\n",
        "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
        "    y_train, y_test = Labels[train_index], Labels[test_index]\n",
        "    \n",
        "    # Model Construction\n",
        "    model = model2()\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "    datagen.fit(X_train)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "    Stopping_Criterion = EarlyStopping(patience=50, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint('./SEfold'+str(fold)+'{epoch:08d}{val_accuracy:05f}.hdf5', monitor='loss',verbose=1,save_best_only=True)\n",
        "    model.fit_generator(datagen.flow(X_train, y_train,batch_size=10),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_test , y_test ),callbacks=[model_checkpoint, Stopping_Criterion])\n",
        "    datafilename = 'TestFold'+str(fold)+'.npy'\n",
        "    labelfilename = 'TestLabelFold'+str(fold)+'.npy'\n",
        "    np.save(datafilename,X_test)\n",
        "    np.save(labelfilename,y_test)\n",
        "    fold+=1\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO:Applying CV]\n",
            "['FOLD:'] 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 26s 801ms/step - loss: 0.5920 - accuracy: 0.7287 - val_loss: 0.6434 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.59309, saving model to ./SEfold1000000010.756098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.5833 - accuracy: 0.7409 - val_loss: 0.5569 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00002: loss improved from 0.59309 to 0.58342, saving model to ./SEfold1000000020.756098.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.5582 - accuracy: 0.7591 - val_loss: 0.6590 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: loss improved from 0.58342 to 0.55901, saving model to ./SEfold1000000030.756098.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.5352 - accuracy: 0.7561 - val_loss: 0.6082 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: loss improved from 0.55901 to 0.53655, saving model to ./SEfold1000000040.756098.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4870 - accuracy: 0.7774 - val_loss: 0.6135 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: loss improved from 0.53655 to 0.48758, saving model to ./SEfold1000000050.756098.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4961 - accuracy: 0.7591 - val_loss: 0.6487 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.48758\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.4402 - accuracy: 0.7805 - val_loss: 0.6088 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: loss improved from 0.48758 to 0.44104, saving model to ./SEfold1000000070.756098.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4229 - accuracy: 0.8171 - val_loss: 0.6446 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: loss improved from 0.44104 to 0.42378, saving model to ./SEfold1000000080.756098.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4278 - accuracy: 0.8049 - val_loss: 0.6458 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.42378\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3720 - accuracy: 0.8476 - val_loss: 0.6107 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: loss improved from 0.42378 to 0.37251, saving model to ./SEfold1000000100.756098.hdf5\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.3571 - accuracy: 0.8689 - val_loss: 0.5956 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: loss improved from 0.37251 to 0.35590, saving model to ./SEfold1000000110.756098.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3736 - accuracy: 0.8689 - val_loss: 0.6307 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.35590\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3583 - accuracy: 0.8659 - val_loss: 0.6494 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.35590\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2917 - accuracy: 0.9024 - val_loss: 0.6786 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: loss improved from 0.35590 to 0.29164, saving model to ./SEfold1000000140.756098.hdf5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.2982 - accuracy: 0.8994 - val_loss: 0.6935 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.29164\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.2349 - accuracy: 0.9329 - val_loss: 0.7377 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00016: loss improved from 0.29164 to 0.23368, saving model to ./SEfold1000000160.292683.hdf5\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3279 - accuracy: 0.8659 - val_loss: 0.7737 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.23368\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3102 - accuracy: 0.8537 - val_loss: 0.6741 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.23368\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2505 - accuracy: 0.9024 - val_loss: 0.7240 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.23368\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2350 - accuracy: 0.9238 - val_loss: 0.7539 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.23368\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2894 - accuracy: 0.9055 - val_loss: 0.6330 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.23368\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2604 - accuracy: 0.9024 - val_loss: 0.6751 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.23368\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2563 - accuracy: 0.9116 - val_loss: 0.6860 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.23368\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2244 - accuracy: 0.9055 - val_loss: 0.7017 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00024: loss improved from 0.23368 to 0.22519, saving model to ./SEfold1000000240.451219.hdf5\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1837 - accuracy: 0.9482 - val_loss: 0.6894 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00025: loss improved from 0.22519 to 0.18363, saving model to ./SEfold1000000250.573171.hdf5\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1605 - accuracy: 0.9421 - val_loss: 0.6524 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00026: loss improved from 0.18363 to 0.16103, saving model to ./SEfold1000000260.743902.hdf5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1933 - accuracy: 0.9329 - val_loss: 0.6774 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.16103\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1896 - accuracy: 0.9360 - val_loss: 0.6285 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.16103\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1909 - accuracy: 0.9329 - val_loss: 0.6233 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.16103\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1816 - accuracy: 0.9360 - val_loss: 0.7380 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.16103\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1790 - accuracy: 0.9299 - val_loss: 0.7322 - val_accuracy: 0.4634\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.16103\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1688 - accuracy: 0.9390 - val_loss: 0.8188 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.16103\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1565 - accuracy: 0.9390 - val_loss: 1.2411 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00033: loss improved from 0.16103 to 0.15628, saving model to ./SEfold1000000330.756098.hdf5\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1358 - accuracy: 0.9512 - val_loss: 1.1794 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00034: loss improved from 0.15628 to 0.13589, saving model to ./SEfold1000000340.756098.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.2172 - accuracy: 0.9268 - val_loss: 1.0966 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.13589\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1294 - accuracy: 0.9695 - val_loss: 1.1729 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00036: loss improved from 0.13589 to 0.12913, saving model to ./SEfold1000000360.768293.hdf5\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1083 - accuracy: 0.9726 - val_loss: 1.0203 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00037: loss improved from 0.12913 to 0.10855, saving model to ./SEfold1000000370.792683.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1084 - accuracy: 0.9604 - val_loss: 1.2060 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00038: loss improved from 0.10855 to 0.10826, saving model to ./SEfold1000000380.780488.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1076 - accuracy: 0.9665 - val_loss: 1.1801 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00039: loss improved from 0.10826 to 0.10791, saving model to ./SEfold1000000390.804878.hdf5\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1295 - accuracy: 0.9573 - val_loss: 0.8828 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.10791\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0714 - accuracy: 0.9787 - val_loss: 0.9122 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00041: loss improved from 0.10791 to 0.07154, saving model to ./SEfold1000000410.804878.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1064 - accuracy: 0.9634 - val_loss: 1.3660 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.07154\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0806 - accuracy: 0.9848 - val_loss: 1.4071 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.07154\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0901 - accuracy: 0.9695 - val_loss: 0.9616 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.07154\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1697 - accuracy: 0.9512 - val_loss: 1.0937 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.07154\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1152 - accuracy: 0.9695 - val_loss: 1.3027 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.07154\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1338 - accuracy: 0.9512 - val_loss: 0.7015 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.07154\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1048 - accuracy: 0.9726 - val_loss: 0.5634 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.07154\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1031 - accuracy: 0.9665 - val_loss: 0.6972 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.07154\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1868 - accuracy: 0.9421 - val_loss: 0.8380 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.07154\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0914 - accuracy: 0.9787 - val_loss: 0.5776 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.07154\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1047 - accuracy: 0.9756 - val_loss: 0.7027 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.07154\n",
            "Epoch 00052: early stopping\n",
            "['FOLD:'] 2\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 26s 797ms/step - loss: 0.7966 - accuracy: 0.3262 - val_loss: 0.6979 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.79704, saving model to ./SEfold2000000010.256098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6533 - accuracy: 0.6646 - val_loss: 0.7063 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: loss improved from 0.79704 to 0.65307, saving model to ./SEfold2000000020.243902.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.6424 - accuracy: 0.7104 - val_loss: 0.7105 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: loss improved from 0.65307 to 0.64279, saving model to ./SEfold2000000030.243902.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6286 - accuracy: 0.7683 - val_loss: 0.6996 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: loss improved from 0.64279 to 0.62800, saving model to ./SEfold2000000040.243902.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.6134 - accuracy: 0.7409 - val_loss: 0.6998 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: loss improved from 0.62800 to 0.61255, saving model to ./SEfold2000000050.243902.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 10s 310ms/step - loss: 0.5905 - accuracy: 0.7988 - val_loss: 0.6933 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00006: loss improved from 0.61255 to 0.59016, saving model to ./SEfold2000000060.475610.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.5876 - accuracy: 0.8079 - val_loss: 0.6947 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00007: loss improved from 0.59016 to 0.58717, saving model to ./SEfold2000000070.341463.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 10s 310ms/step - loss: 0.5625 - accuracy: 0.8293 - val_loss: 0.6990 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00008: loss improved from 0.58717 to 0.56231, saving model to ./SEfold2000000080.256098.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5551 - accuracy: 0.8018 - val_loss: 0.6878 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: loss improved from 0.56231 to 0.55579, saving model to ./SEfold2000000090.756098.hdf5\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5553 - accuracy: 0.8232 - val_loss: 0.6877 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: loss improved from 0.55579 to 0.55502, saving model to ./SEfold2000000100.756098.hdf5\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5327 - accuracy: 0.8323 - val_loss: 0.6874 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00011: loss improved from 0.55502 to 0.53270, saving model to ./SEfold2000000110.743902.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4922 - accuracy: 0.8720 - val_loss: 0.6871 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: loss improved from 0.53270 to 0.49264, saving model to ./SEfold2000000120.756098.hdf5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.5216 - accuracy: 0.8262 - val_loss: 0.6876 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.49264\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5350 - accuracy: 0.8232 - val_loss: 0.6801 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.49264\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4934 - accuracy: 0.8659 - val_loss: 0.6736 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.49264\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.5032 - accuracy: 0.8323 - val_loss: 0.6794 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.49264\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4899 - accuracy: 0.8506 - val_loss: 0.6759 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00017: loss improved from 0.49264 to 0.49091, saving model to ./SEfold2000000170.756098.hdf5\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4498 - accuracy: 0.8780 - val_loss: 0.6879 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00018: loss improved from 0.49091 to 0.44964, saving model to ./SEfold2000000180.585366.hdf5\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4466 - accuracy: 0.8811 - val_loss: 0.7054 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00019: loss improved from 0.44964 to 0.44601, saving model to ./SEfold2000000190.524390.hdf5\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4542 - accuracy: 0.8567 - val_loss: 1.6912 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.44601\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4468 - accuracy: 0.8750 - val_loss: 0.7835 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.44601\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4227 - accuracy: 0.8720 - val_loss: 0.6587 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00022: loss improved from 0.44601 to 0.42329, saving model to ./SEfold2000000220.756098.hdf5\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4158 - accuracy: 0.8963 - val_loss: 0.6552 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00023: loss improved from 0.42329 to 0.41590, saving model to ./SEfold2000000230.756098.hdf5\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4259 - accuracy: 0.8841 - val_loss: 0.6582 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.41590\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3964 - accuracy: 0.8994 - val_loss: 0.6626 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00025: loss improved from 0.41590 to 0.39571, saving model to ./SEfold2000000250.756098.hdf5\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3942 - accuracy: 0.8872 - val_loss: 1.0360 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00026: loss improved from 0.39571 to 0.39346, saving model to ./SEfold2000000260.439024.hdf5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.4048 - accuracy: 0.8780 - val_loss: 0.6910 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.39346\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3671 - accuracy: 0.9055 - val_loss: 0.6649 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00028: loss improved from 0.39346 to 0.36786, saving model to ./SEfold2000000280.707317.hdf5\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.3651 - accuracy: 0.9146 - val_loss: 0.9528 - val_accuracy: 0.4146\n",
            "\n",
            "Epoch 00029: loss improved from 0.36786 to 0.36508, saving model to ./SEfold2000000290.414634.hdf5\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4157 - accuracy: 0.8720 - val_loss: 0.6872 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.36508\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3933 - accuracy: 0.8689 - val_loss: 0.6121 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.36508\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3599 - accuracy: 0.8902 - val_loss: 2.3088 - val_accuracy: 0.3537\n",
            "\n",
            "Epoch 00032: loss improved from 0.36508 to 0.35983, saving model to ./SEfold2000000320.353659.hdf5\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3635 - accuracy: 0.9116 - val_loss: 0.6556 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.35983\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.3387 - accuracy: 0.9116 - val_loss: 0.8883 - val_accuracy: 0.4146\n",
            "\n",
            "Epoch 00034: loss improved from 0.35983 to 0.33768, saving model to ./SEfold2000000340.414634.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.3108 - accuracy: 0.9299 - val_loss: 0.7382 - val_accuracy: 0.5122\n",
            "\n",
            "Epoch 00035: loss improved from 0.33768 to 0.30989, saving model to ./SEfold2000000350.512195.hdf5\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3134 - accuracy: 0.9207 - val_loss: 0.6518 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.30989\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.3165 - accuracy: 0.9268 - val_loss: 0.6011 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.30989\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3704 - accuracy: 0.8750 - val_loss: 0.6171 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.30989\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3366 - accuracy: 0.9116 - val_loss: 0.5462 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.30989\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3143 - accuracy: 0.9146 - val_loss: 0.5955 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.30989\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2633 - accuracy: 0.9482 - val_loss: 0.5231 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00041: loss improved from 0.30989 to 0.26325, saving model to ./SEfold2000000410.768293.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2942 - accuracy: 0.9238 - val_loss: 0.5226 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.26325\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2644 - accuracy: 0.9512 - val_loss: 0.5270 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.26325\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3077 - accuracy: 0.9116 - val_loss: 0.5133 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.26325\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2899 - accuracy: 0.9268 - val_loss: 0.5326 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.26325\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2723 - accuracy: 0.9360 - val_loss: 0.4767 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.26325\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2374 - accuracy: 0.9482 - val_loss: 0.5451 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00047: loss improved from 0.26325 to 0.23733, saving model to ./SEfold2000000470.756098.hdf5\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.2328 - accuracy: 0.9573 - val_loss: 0.5069 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00048: loss improved from 0.23733 to 0.23314, saving model to ./SEfold2000000480.817073.hdf5\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2294 - accuracy: 0.9482 - val_loss: 0.5577 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00049: loss improved from 0.23314 to 0.22938, saving model to ./SEfold2000000490.780488.hdf5\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2466 - accuracy: 0.9451 - val_loss: 0.5837 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.22938\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3359 - accuracy: 0.8902 - val_loss: 0.9300 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.22938\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2585 - accuracy: 0.9360 - val_loss: 0.5202 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.22938\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2646 - accuracy: 0.9451 - val_loss: 0.5005 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.22938\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2516 - accuracy: 0.9482 - val_loss: 0.5171 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.22938\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2000 - accuracy: 0.9543 - val_loss: 0.5261 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00055: loss improved from 0.22938 to 0.19955, saving model to ./SEfold2000000550.780488.hdf5\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1924 - accuracy: 0.9726 - val_loss: 0.4816 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00056: loss improved from 0.19955 to 0.19085, saving model to ./SEfold2000000560.817073.hdf5\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1773 - accuracy: 0.9695 - val_loss: 0.5224 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00057: loss improved from 0.19085 to 0.17766, saving model to ./SEfold2000000570.780488.hdf5\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1869 - accuracy: 0.9543 - val_loss: 0.4969 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.17766\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1643 - accuracy: 0.9756 - val_loss: 1.6773 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00059: loss improved from 0.17766 to 0.16459, saving model to ./SEfold2000000590.451219.hdf5\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1502 - accuracy: 0.9787 - val_loss: 0.6854 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00060: loss improved from 0.16459 to 0.15046, saving model to ./SEfold2000000600.743902.hdf5\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1757 - accuracy: 0.9665 - val_loss: 0.7037 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.15046\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1876 - accuracy: 0.9634 - val_loss: 0.5003 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.15046\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1548 - accuracy: 0.9787 - val_loss: 0.5345 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.15046\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1612 - accuracy: 0.9756 - val_loss: 0.5909 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.15046\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1552 - accuracy: 0.9756 - val_loss: 0.5549 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.15046\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1474 - accuracy: 0.9726 - val_loss: 0.5209 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00066: loss improved from 0.15046 to 0.14751, saving model to ./SEfold2000000660.792683.hdf5\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1818 - accuracy: 0.9512 - val_loss: 0.5714 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.14751\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1662 - accuracy: 0.9726 - val_loss: 0.5590 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.14751\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1364 - accuracy: 0.9787 - val_loss: 0.5277 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00069: loss improved from 0.14751 to 0.13656, saving model to ./SEfold2000000690.768293.hdf5\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1469 - accuracy: 0.9726 - val_loss: 0.5296 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.13656\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1578 - accuracy: 0.9604 - val_loss: 0.5534 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.13656\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1477 - accuracy: 0.9756 - val_loss: 0.5413 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.13656\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1253 - accuracy: 0.9848 - val_loss: 0.5435 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00073: loss improved from 0.13656 to 0.12564, saving model to ./SEfold2000000730.768293.hdf5\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1188 - accuracy: 0.9817 - val_loss: 0.5665 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00074: loss improved from 0.12564 to 0.11849, saving model to ./SEfold2000000740.804878.hdf5\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1285 - accuracy: 0.9787 - val_loss: 0.6568 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.11849\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1391 - accuracy: 0.9726 - val_loss: 0.6030 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.11849\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2126 - accuracy: 0.9390 - val_loss: 0.8015 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.11849\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1781 - accuracy: 0.9665 - val_loss: 0.5648 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.11849\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1291 - accuracy: 0.9787 - val_loss: 0.6380 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.11849\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.0943 - accuracy: 0.9939 - val_loss: 0.5405 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00080: loss improved from 0.11849 to 0.09412, saving model to ./SEfold2000000800.780488.hdf5\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0944 - accuracy: 0.9909 - val_loss: 0.5072 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.09412\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1154 - accuracy: 0.9787 - val_loss: 0.5691 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.09412\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0969 - accuracy: 0.9939 - val_loss: 0.5157 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.09412\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.1393 - accuracy: 0.9695 - val_loss: 0.5334 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.09412\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1184 - accuracy: 0.9726 - val_loss: 0.5331 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.09412\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.1297 - accuracy: 0.9726 - val_loss: 0.5632 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.09412\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2222 - accuracy: 0.9268 - val_loss: 0.6615 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.09412\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1862 - accuracy: 0.9543 - val_loss: 0.8155 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.09412\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1644 - accuracy: 0.9543 - val_loss: 0.4874 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.09412\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1540 - accuracy: 0.9665 - val_loss: 0.4933 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.09412\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1371 - accuracy: 0.9665 - val_loss: 0.5119 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.09412\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1254 - accuracy: 0.9756 - val_loss: 0.5674 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.09412\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1662 - accuracy: 0.9512 - val_loss: 0.6135 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.09412\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1134 - accuracy: 0.9848 - val_loss: 0.5992 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.09412\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.1044 - accuracy: 0.9787 - val_loss: 0.6324 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.09412\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.1109 - accuracy: 0.9787 - val_loss: 0.6247 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.09412\n",
            "Epoch 00096: early stopping\n",
            "['FOLD:'] 3\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 27s 819ms/step - loss: 0.6559 - accuracy: 0.6220 - val_loss: 0.6987 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.65670, saving model to ./SEfold3000000010.292683.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6091 - accuracy: 0.7195 - val_loss: 0.7037 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: loss improved from 0.65670 to 0.60944, saving model to ./SEfold3000000020.243902.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 10s 316ms/step - loss: 0.5778 - accuracy: 0.7256 - val_loss: 0.6941 - val_accuracy: 0.5122\n",
            "\n",
            "Epoch 00003: loss improved from 0.60944 to 0.57811, saving model to ./SEfold3000000030.512195.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.5214 - accuracy: 0.7591 - val_loss: 0.6921 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00004: loss improved from 0.57811 to 0.52316, saving model to ./SEfold3000000040.621951.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 10s 316ms/step - loss: 0.5012 - accuracy: 0.7805 - val_loss: 0.7045 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00005: loss improved from 0.52316 to 0.50180, saving model to ./SEfold3000000050.256098.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4588 - accuracy: 0.8232 - val_loss: 0.6950 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00006: loss improved from 0.50180 to 0.45863, saving model to ./SEfold3000000060.475610.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4451 - accuracy: 0.8323 - val_loss: 0.6883 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00007: loss improved from 0.45863 to 0.44520, saving model to ./SEfold3000000070.682927.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4658 - accuracy: 0.7988 - val_loss: 0.6948 - val_accuracy: 0.3902\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.44520\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4451 - accuracy: 0.8018 - val_loss: 0.6856 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.44520\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.4779 - accuracy: 0.8110 - val_loss: 0.6753 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.44520\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.3752 - accuracy: 0.8476 - val_loss: 0.6805 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00011: loss improved from 0.44520 to 0.37557, saving model to ./SEfold3000000110.707317.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4243 - accuracy: 0.8079 - val_loss: 0.6828 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.37557\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.4240 - accuracy: 0.8445 - val_loss: 0.6754 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.37557\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.3782 - accuracy: 0.8445 - val_loss: 0.6840 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.37557\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3461 - accuracy: 0.8720 - val_loss: 0.6579 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00015: loss improved from 0.37557 to 0.34708, saving model to ./SEfold3000000150.731707.hdf5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3217 - accuracy: 0.8689 - val_loss: 0.6477 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: loss improved from 0.34708 to 0.32310, saving model to ./SEfold3000000160.756098.hdf5\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.3007 - accuracy: 0.8780 - val_loss: 0.6454 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00017: loss improved from 0.32310 to 0.30118, saving model to ./SEfold3000000170.743902.hdf5\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2960 - accuracy: 0.8780 - val_loss: 0.5917 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00018: loss improved from 0.30118 to 0.29619, saving model to ./SEfold3000000180.756098.hdf5\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2495 - accuracy: 0.9085 - val_loss: 0.5895 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00019: loss improved from 0.29619 to 0.25049, saving model to ./SEfold3000000190.756098.hdf5\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2537 - accuracy: 0.9055 - val_loss: 0.6319 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.25049\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2996 - accuracy: 0.8872 - val_loss: 0.6646 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.25049\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 10s 316ms/step - loss: 0.2892 - accuracy: 0.9085 - val_loss: 0.6755 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.25049\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2874 - accuracy: 0.8994 - val_loss: 0.7832 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.25049\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2483 - accuracy: 0.9177 - val_loss: 0.6400 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00024: loss improved from 0.25049 to 0.24875, saving model to ./SEfold3000000240.719512.hdf5\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2654 - accuracy: 0.9085 - val_loss: 0.7220 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.24875\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2560 - accuracy: 0.9024 - val_loss: 0.7252 - val_accuracy: 0.3902\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.24875\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2335 - accuracy: 0.9146 - val_loss: 0.7246 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00027: loss improved from 0.24875 to 0.23281, saving model to ./SEfold3000000270.378049.hdf5\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2486 - accuracy: 0.9207 - val_loss: 0.6638 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.23281\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2387 - accuracy: 0.9116 - val_loss: 0.5820 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.23281\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.1992 - accuracy: 0.9207 - val_loss: 0.6668 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00030: loss improved from 0.23281 to 0.19839, saving model to ./SEfold3000000300.658537.hdf5\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1824 - accuracy: 0.9360 - val_loss: 0.7065 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00031: loss improved from 0.19839 to 0.18315, saving model to ./SEfold3000000310.573171.hdf5\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1882 - accuracy: 0.9451 - val_loss: 0.7156 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.18315\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2507 - accuracy: 0.9024 - val_loss: 0.5424 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.18315\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1967 - accuracy: 0.9360 - val_loss: 0.5755 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.18315\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1871 - accuracy: 0.9482 - val_loss: 0.8436 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.18315\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2231 - accuracy: 0.9207 - val_loss: 0.6446 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.18315\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1629 - accuracy: 0.9543 - val_loss: 0.9412 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00037: loss improved from 0.18315 to 0.16354, saving model to ./SEfold3000000370.768293.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1315 - accuracy: 0.9634 - val_loss: 1.0221 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00038: loss improved from 0.16354 to 0.13064, saving model to ./SEfold3000000380.780488.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1362 - accuracy: 0.9604 - val_loss: 0.6513 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.13064\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1434 - accuracy: 0.9482 - val_loss: 0.9535 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.13064\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1318 - accuracy: 0.9543 - val_loss: 0.9228 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.13064\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1144 - accuracy: 0.9756 - val_loss: 0.6329 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00042: loss improved from 0.13064 to 0.11451, saving model to ./SEfold3000000420.829268.hdf5\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1486 - accuracy: 0.9573 - val_loss: 0.7462 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.11451\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1351 - accuracy: 0.9512 - val_loss: 0.8371 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.11451\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0725 - accuracy: 0.9848 - val_loss: 0.8544 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00045: loss improved from 0.11451 to 0.07277, saving model to ./SEfold3000000450.792683.hdf5\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1659 - accuracy: 0.9451 - val_loss: 0.9553 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.07277\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1349 - accuracy: 0.9604 - val_loss: 0.8943 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.07277\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1446 - accuracy: 0.9573 - val_loss: 1.1396 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.07277\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1789 - accuracy: 0.9390 - val_loss: 0.7414 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.07277\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1133 - accuracy: 0.9665 - val_loss: 0.7139 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.07277\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1110 - accuracy: 0.9604 - val_loss: 0.5912 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.07277\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0950 - accuracy: 0.9604 - val_loss: 0.5890 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.07277\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1117 - accuracy: 0.9634 - val_loss: 0.8009 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.07277\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1273 - accuracy: 0.9665 - val_loss: 0.6216 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.07277\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1186 - accuracy: 0.9665 - val_loss: 0.7910 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.07277\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.1312 - accuracy: 0.9634 - val_loss: 0.7206 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.07277\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1356 - accuracy: 0.9634 - val_loss: 0.8040 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.07277\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1180 - accuracy: 0.9634 - val_loss: 1.0949 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.07277\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1418 - accuracy: 0.9512 - val_loss: 0.8972 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.07277\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1570 - accuracy: 0.9482 - val_loss: 0.8867 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.07277\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1397 - accuracy: 0.9604 - val_loss: 0.8614 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.07277\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1100 - accuracy: 0.9604 - val_loss: 0.9130 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.07277\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1305 - accuracy: 0.9634 - val_loss: 0.8438 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.07277\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1072 - accuracy: 0.9634 - val_loss: 0.9839 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.07277\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1341 - accuracy: 0.9421 - val_loss: 1.0815 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.07277\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1155 - accuracy: 0.9573 - val_loss: 1.1671 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.07277\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0936 - accuracy: 0.9787 - val_loss: 1.0471 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.07277\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1145 - accuracy: 0.9665 - val_loss: 0.8122 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.07277\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0983 - accuracy: 0.9665 - val_loss: 0.8721 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.07277\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0562 - accuracy: 0.9878 - val_loss: 0.9817 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00070: loss improved from 0.07277 to 0.05650, saving model to ./SEfold3000000700.804878.hdf5\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 10s 310ms/step - loss: 0.0804 - accuracy: 0.9848 - val_loss: 0.9227 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.05650\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0647 - accuracy: 0.9848 - val_loss: 0.9575 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.05650\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0625 - accuracy: 0.9878 - val_loss: 1.0655 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.05650\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0780 - accuracy: 0.9756 - val_loss: 1.0871 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.05650\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0963 - accuracy: 0.9756 - val_loss: 0.8765 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.05650\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.1360 - accuracy: 0.9573 - val_loss: 0.6245 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.05650\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0839 - accuracy: 0.9726 - val_loss: 0.7998 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.05650\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0811 - accuracy: 0.9726 - val_loss: 0.7290 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.05650\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0609 - accuracy: 0.9726 - val_loss: 0.9465 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.05650\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1868 - accuracy: 0.9451 - val_loss: 0.9072 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.05650\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1523 - accuracy: 0.9451 - val_loss: 0.6514 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.05650\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1127 - accuracy: 0.9726 - val_loss: 0.6434 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.05650\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1009 - accuracy: 0.9695 - val_loss: 0.7954 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.05650\n",
            "Epoch 00083: early stopping\n",
            "['FOLD:'] 4\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 27s 813ms/step - loss: 0.6165 - accuracy: 0.7561 - val_loss: 0.5625 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.61459, saving model to ./SEfold4000000010.756098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5642 - accuracy: 0.7591 - val_loss: 0.5716 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00002: loss improved from 0.61459 to 0.56338, saving model to ./SEfold4000000020.756098.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 10s 316ms/step - loss: 0.4681 - accuracy: 0.7530 - val_loss: 0.5770 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: loss improved from 0.56338 to 0.46932, saving model to ./SEfold4000000030.756098.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 10s 316ms/step - loss: 0.4258 - accuracy: 0.8262 - val_loss: 0.5986 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: loss improved from 0.46932 to 0.42680, saving model to ./SEfold4000000040.756098.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4027 - accuracy: 0.8262 - val_loss: 0.5930 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: loss improved from 0.42680 to 0.39944, saving model to ./SEfold4000000050.756098.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3794 - accuracy: 0.8476 - val_loss: 0.7072 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00006: loss improved from 0.39944 to 0.37962, saving model to ./SEfold4000000060.243902.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 10s 310ms/step - loss: 0.4099 - accuracy: 0.8171 - val_loss: 0.6614 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.37962\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3416 - accuracy: 0.8841 - val_loss: 0.7330 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00008: loss improved from 0.37962 to 0.33816, saving model to ./SEfold4000000080.243902.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2627 - accuracy: 0.8750 - val_loss: 0.7414 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00009: loss improved from 0.33816 to 0.26316, saving model to ./SEfold4000000090.243902.hdf5\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.2768 - accuracy: 0.8872 - val_loss: 0.7703 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.26316\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2710 - accuracy: 0.9207 - val_loss: 0.8477 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.26316\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2501 - accuracy: 0.9299 - val_loss: 0.8440 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00012: loss improved from 0.26316 to 0.25019, saving model to ./SEfold4000000120.243902.hdf5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.2344 - accuracy: 0.9238 - val_loss: 0.8453 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00013: loss improved from 0.25019 to 0.23493, saving model to ./SEfold4000000130.243902.hdf5\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.2212 - accuracy: 0.9329 - val_loss: 0.9184 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00014: loss improved from 0.23493 to 0.22179, saving model to ./SEfold4000000140.243902.hdf5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2127 - accuracy: 0.9329 - val_loss: 0.9219 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00015: loss improved from 0.22179 to 0.21192, saving model to ./SEfold4000000150.243902.hdf5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.2365 - accuracy: 0.9238 - val_loss: 0.8798 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.21192\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2284 - accuracy: 0.9390 - val_loss: 0.9613 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.21192\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2095 - accuracy: 0.9329 - val_loss: 1.0513 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00018: loss improved from 0.21192 to 0.20948, saving model to ./SEfold4000000180.243902.hdf5\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 10s 310ms/step - loss: 0.2323 - accuracy: 0.9238 - val_loss: 1.0512 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.20948\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1489 - accuracy: 0.9695 - val_loss: 1.0260 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00020: loss improved from 0.20948 to 0.14895, saving model to ./SEfold4000000200.243902.hdf5\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1198 - accuracy: 0.9817 - val_loss: 1.0916 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00021: loss improved from 0.14895 to 0.12006, saving model to ./SEfold4000000210.243902.hdf5\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1379 - accuracy: 0.9726 - val_loss: 1.0857 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.12006\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1723 - accuracy: 0.9573 - val_loss: 1.0666 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.12006\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1276 - accuracy: 0.9787 - val_loss: 1.0010 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.12006\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1292 - accuracy: 0.9665 - val_loss: 1.0309 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.12006\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1172 - accuracy: 0.9817 - val_loss: 0.9952 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00026: loss improved from 0.12006 to 0.11725, saving model to ./SEfold4000000260.292683.hdf5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1712 - accuracy: 0.9543 - val_loss: 0.7875 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.11725\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1415 - accuracy: 0.9543 - val_loss: 0.9587 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.11725\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1681 - accuracy: 0.9573 - val_loss: 0.7736 - val_accuracy: 0.4878\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.11725\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1406 - accuracy: 0.9634 - val_loss: 1.2047 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.11725\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1548 - accuracy: 0.9482 - val_loss: 0.7763 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.11725\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1257 - accuracy: 0.9695 - val_loss: 0.6658 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.11725\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1518 - accuracy: 0.9512 - val_loss: 0.6007 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.11725\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2164 - accuracy: 0.9207 - val_loss: 0.5379 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.11725\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1230 - accuracy: 0.9726 - val_loss: 0.7404 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.11725\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.0955 - accuracy: 0.9817 - val_loss: 0.6570 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00036: loss improved from 0.11725 to 0.09558, saving model to ./SEfold4000000360.768293.hdf5\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0847 - accuracy: 0.9817 - val_loss: 0.5709 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00037: loss improved from 0.09558 to 0.08484, saving model to ./SEfold4000000370.768293.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.0768 - accuracy: 0.9878 - val_loss: 0.5439 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00038: loss improved from 0.08484 to 0.07687, saving model to ./SEfold4000000380.780488.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.0957 - accuracy: 0.9878 - val_loss: 0.9118 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.07687\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1052 - accuracy: 0.9756 - val_loss: 0.6536 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.07687\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1050 - accuracy: 0.9634 - val_loss: 0.4035 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.07687\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1417 - accuracy: 0.9573 - val_loss: 0.5865 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.07687\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0910 - accuracy: 0.9817 - val_loss: 0.5222 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.07687\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1079 - accuracy: 0.9726 - val_loss: 0.3436 - val_accuracy: 0.9024\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.07687\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0918 - accuracy: 0.9756 - val_loss: 0.5385 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.07687\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1032 - accuracy: 0.9695 - val_loss: 0.6909 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.07687\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1063 - accuracy: 0.9756 - val_loss: 0.6999 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.07687\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1184 - accuracy: 0.9695 - val_loss: 0.4424 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.07687\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.0839 - accuracy: 0.9787 - val_loss: 0.5573 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.07687\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0656 - accuracy: 0.9909 - val_loss: 0.3915 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00050: loss improved from 0.07687 to 0.06530, saving model to ./SEfold4000000500.829268.hdf5\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.1652 - accuracy: 0.9451 - val_loss: 0.5058 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.06530\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1252 - accuracy: 0.9573 - val_loss: 0.6688 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.06530\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1049 - accuracy: 0.9726 - val_loss: 0.7296 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.06530\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1149 - accuracy: 0.9665 - val_loss: 0.6351 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.06530\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0847 - accuracy: 0.9817 - val_loss: 0.7425 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.06530\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0634 - accuracy: 0.9909 - val_loss: 0.7950 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00056: loss improved from 0.06530 to 0.06366, saving model to ./SEfold4000000560.829268.hdf5\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.0690 - accuracy: 0.9787 - val_loss: 0.5924 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.06366\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.1188 - accuracy: 0.9604 - val_loss: 0.7620 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.06366\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.1130 - accuracy: 0.9695 - val_loss: 0.6174 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.06366\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1037 - accuracy: 0.9665 - val_loss: 0.4963 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.06366\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1305 - accuracy: 0.9543 - val_loss: 0.4617 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.06366\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0842 - accuracy: 0.9756 - val_loss: 0.4454 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.06366\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0710 - accuracy: 0.9848 - val_loss: 0.4624 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.06366\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0692 - accuracy: 0.9787 - val_loss: 0.5976 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.06366\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0567 - accuracy: 0.9939 - val_loss: 0.5287 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00065: loss improved from 0.06366 to 0.05621, saving model to ./SEfold4000000650.817073.hdf5\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0410 - accuracy: 0.9970 - val_loss: 0.5277 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00066: loss improved from 0.05621 to 0.04060, saving model to ./SEfold4000000660.817073.hdf5\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0899 - accuracy: 0.9726 - val_loss: 0.6750 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.04060\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0740 - accuracy: 0.9909 - val_loss: 0.5876 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00068: loss did not improve from 0.04060\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0805 - accuracy: 0.9817 - val_loss: 0.8057 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.04060\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0586 - accuracy: 0.9848 - val_loss: 0.7741 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.04060\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0617 - accuracy: 0.9817 - val_loss: 0.5634 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.04060\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0705 - accuracy: 0.9848 - val_loss: 0.5131 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.04060\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0762 - accuracy: 0.9787 - val_loss: 1.1261 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.04060\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0608 - accuracy: 0.9848 - val_loss: 1.1970 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.04060\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0431 - accuracy: 0.9939 - val_loss: 0.8037 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.04060\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0548 - accuracy: 0.9848 - val_loss: 0.5248 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.04060\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0659 - accuracy: 0.9787 - val_loss: 0.9325 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.04060\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0636 - accuracy: 0.9817 - val_loss: 1.4236 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.04060\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.0593 - accuracy: 0.9909 - val_loss: 0.6725 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.04060\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0736 - accuracy: 0.9726 - val_loss: 0.6363 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.04060\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0596 - accuracy: 0.9848 - val_loss: 0.6196 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.04060\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0937 - accuracy: 0.9695 - val_loss: 0.8798 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00082: loss did not improve from 0.04060\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0443 - accuracy: 0.9909 - val_loss: 0.6112 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.04060\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.5290 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00084: loss improved from 0.04060 to 0.02573, saving model to ./SEfold4000000840.841463.hdf5\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.0321 - accuracy: 0.9939 - val_loss: 0.4977 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.02573\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.0284 - accuracy: 0.9970 - val_loss: 0.4913 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.02573\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0250 - accuracy: 0.9970 - val_loss: 0.5687 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00087: loss improved from 0.02573 to 0.02514, saving model to ./SEfold4000000870.817073.hdf5\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.0514 - accuracy: 0.9848 - val_loss: 1.2937 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.02514\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0795 - accuracy: 0.9787 - val_loss: 0.8440 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.02514\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0471 - accuracy: 0.9878 - val_loss: 0.7793 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.02514\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0500 - accuracy: 0.9909 - val_loss: 1.2055 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.02514\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.0943 - accuracy: 0.9695 - val_loss: 0.6580 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.02514\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.0838 - accuracy: 0.9726 - val_loss: 0.5010 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.02514\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.1309 - accuracy: 0.9634 - val_loss: 1.9730 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.02514\n",
            "Epoch 00094: early stopping\n",
            "['FOLD:'] 5\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 27s 831ms/step - loss: 0.6984 - accuracy: 0.3171 - val_loss: 0.6919 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.69833, saving model to ./SEfold5000000010.756098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6918 - accuracy: 0.5640 - val_loss: 0.6909 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00002: loss improved from 0.69833 to 0.69179, saving model to ./SEfold5000000020.756098.hdf5\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 10s 316ms/step - loss: 0.6852 - accuracy: 0.8018 - val_loss: 0.6902 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: loss improved from 0.69179 to 0.68517, saving model to ./SEfold5000000030.756098.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.6787 - accuracy: 0.8049 - val_loss: 0.6875 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: loss improved from 0.68517 to 0.67876, saving model to ./SEfold5000000040.756098.hdf5\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6756 - accuracy: 0.8262 - val_loss: 0.6847 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: loss improved from 0.67876 to 0.67542, saving model to ./SEfold5000000050.756098.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.6764 - accuracy: 0.7805 - val_loss: 0.6835 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.67542\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6704 - accuracy: 0.8140 - val_loss: 0.6819 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: loss improved from 0.67542 to 0.67041, saving model to ./SEfold5000000070.756098.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.6673 - accuracy: 0.8201 - val_loss: 0.6798 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: loss improved from 0.67041 to 0.66725, saving model to ./SEfold5000000080.756098.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6621 - accuracy: 0.8323 - val_loss: 0.6777 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: loss improved from 0.66725 to 0.66200, saving model to ./SEfold5000000090.756098.hdf5\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6647 - accuracy: 0.7927 - val_loss: 0.6757 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.66200\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6579 - accuracy: 0.8262 - val_loss: 0.6752 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: loss improved from 0.66200 to 0.65808, saving model to ./SEfold5000000110.756098.hdf5\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6552 - accuracy: 0.8323 - val_loss: 0.6735 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: loss improved from 0.65808 to 0.65513, saving model to ./SEfold5000000120.756098.hdf5\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6555 - accuracy: 0.8079 - val_loss: 0.6720 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.65513\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.6457 - accuracy: 0.8476 - val_loss: 0.6708 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: loss improved from 0.65513 to 0.64586, saving model to ./SEfold5000000140.756098.hdf5\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.6448 - accuracy: 0.8354 - val_loss: 0.6689 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00015: loss improved from 0.64586 to 0.64491, saving model to ./SEfold5000000150.756098.hdf5\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6515 - accuracy: 0.7927 - val_loss: 0.6678 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.64491\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6415 - accuracy: 0.8201 - val_loss: 0.6659 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00017: loss improved from 0.64491 to 0.64155, saving model to ./SEfold5000000170.756098.hdf5\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.6427 - accuracy: 0.8201 - val_loss: 0.6641 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.64155\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6325 - accuracy: 0.8445 - val_loss: 0.6625 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00019: loss improved from 0.64155 to 0.63235, saving model to ./SEfold5000000190.756098.hdf5\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6368 - accuracy: 0.8079 - val_loss: 0.6608 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.63235\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6220 - accuracy: 0.8476 - val_loss: 0.6566 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00021: loss improved from 0.63235 to 0.62194, saving model to ./SEfold5000000210.756098.hdf5\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6249 - accuracy: 0.8323 - val_loss: 0.6556 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.62194\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6239 - accuracy: 0.8110 - val_loss: 0.6538 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.62194\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6069 - accuracy: 0.8537 - val_loss: 0.6522 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00024: loss improved from 0.62194 to 0.60710, saving model to ./SEfold5000000240.756098.hdf5\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6056 - accuracy: 0.8537 - val_loss: 0.6497 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00025: loss improved from 0.60710 to 0.60528, saving model to ./SEfold5000000250.756098.hdf5\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.5979 - accuracy: 0.8537 - val_loss: 0.6502 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00026: loss improved from 0.60528 to 0.59816, saving model to ./SEfold5000000260.756098.hdf5\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5930 - accuracy: 0.8506 - val_loss: 0.6445 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00027: loss improved from 0.59816 to 0.59308, saving model to ./SEfold5000000270.768293.hdf5\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6090 - accuracy: 0.8262 - val_loss: 0.6441 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.59308\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.6041 - accuracy: 0.8110 - val_loss: 0.6456 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.59308\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6211 - accuracy: 0.7744 - val_loss: 0.6431 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.59308\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.6032 - accuracy: 0.8201 - val_loss: 0.6319 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.59308\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.5940 - accuracy: 0.8384 - val_loss: 0.6316 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.59308\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.5768 - accuracy: 0.8598 - val_loss: 0.6280 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00033: loss improved from 0.59308 to 0.57673, saving model to ./SEfold5000000330.756098.hdf5\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.5649 - accuracy: 0.8780 - val_loss: 0.6341 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00034: loss improved from 0.57673 to 0.56466, saving model to ./SEfold5000000340.756098.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.5575 - accuracy: 0.8689 - val_loss: 0.6313 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00035: loss improved from 0.56466 to 0.55780, saving model to ./SEfold5000000350.756098.hdf5\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5613 - accuracy: 0.8659 - val_loss: 0.6106 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.55780\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.5664 - accuracy: 0.8445 - val_loss: 0.6036 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.55780\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.5602 - accuracy: 0.8415 - val_loss: 0.5946 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.55780\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5475 - accuracy: 0.8811 - val_loss: 0.5580 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00039: loss improved from 0.55780 to 0.54788, saving model to ./SEfold5000000390.817073.hdf5\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.5546 - accuracy: 0.8445 - val_loss: 0.5848 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.54788\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5481 - accuracy: 0.8384 - val_loss: 0.6123 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00041: loss improved from 0.54788 to 0.54771, saving model to ./SEfold5000000410.756098.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.5484 - accuracy: 0.8323 - val_loss: 0.6064 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.54771\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.5422 - accuracy: 0.8537 - val_loss: 0.5718 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00043: loss improved from 0.54771 to 0.54200, saving model to ./SEfold5000000430.780488.hdf5\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.5377 - accuracy: 0.8567 - val_loss: 0.5734 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00044: loss improved from 0.54200 to 0.53735, saving model to ./SEfold5000000440.792683.hdf5\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.5239 - accuracy: 0.8750 - val_loss: 0.5777 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00045: loss improved from 0.53735 to 0.52359, saving model to ./SEfold5000000450.792683.hdf5\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.5262 - accuracy: 0.8384 - val_loss: 0.5543 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.52359\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.5138 - accuracy: 0.8537 - val_loss: 0.5626 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00047: loss improved from 0.52359 to 0.51404, saving model to ./SEfold5000000470.792683.hdf5\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4863 - accuracy: 0.8902 - val_loss: 0.5603 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00048: loss improved from 0.51404 to 0.48633, saving model to ./SEfold5000000480.804878.hdf5\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.4901 - accuracy: 0.8811 - val_loss: 0.5895 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.48633\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4686 - accuracy: 0.9116 - val_loss: 0.5867 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00050: loss improved from 0.48633 to 0.46819, saving model to ./SEfold5000000500.768293.hdf5\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4934 - accuracy: 0.8689 - val_loss: 0.5632 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00051: loss did not improve from 0.46819\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.4803 - accuracy: 0.8933 - val_loss: 0.5453 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.46819\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4930 - accuracy: 0.8659 - val_loss: 0.5781 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.46819\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4636 - accuracy: 0.8720 - val_loss: 0.5399 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00054: loss improved from 0.46819 to 0.46318, saving model to ./SEfold5000000540.792683.hdf5\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4794 - accuracy: 0.8750 - val_loss: 0.5518 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.46318\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4576 - accuracy: 0.8811 - val_loss: 0.5538 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00056: loss improved from 0.46318 to 0.45796, saving model to ./SEfold5000000560.804878.hdf5\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.4309 - accuracy: 0.9207 - val_loss: 0.5735 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00057: loss improved from 0.45796 to 0.43120, saving model to ./SEfold5000000570.804878.hdf5\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4495 - accuracy: 0.8902 - val_loss: 0.5410 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.43120\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4540 - accuracy: 0.8841 - val_loss: 0.5435 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.43120\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.4511 - accuracy: 0.8933 - val_loss: 0.5201 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.43120\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4261 - accuracy: 0.9055 - val_loss: 0.5313 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00061: loss improved from 0.43120 to 0.42645, saving model to ./SEfold5000000610.768293.hdf5\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4427 - accuracy: 0.8780 - val_loss: 0.5032 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.42645\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4075 - accuracy: 0.9085 - val_loss: 0.5246 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00063: loss improved from 0.42645 to 0.40522, saving model to ./SEfold5000000630.780488.hdf5\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3915 - accuracy: 0.9268 - val_loss: 0.5383 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00064: loss improved from 0.40522 to 0.39137, saving model to ./SEfold5000000640.768293.hdf5\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3909 - accuracy: 0.9268 - val_loss: 0.5450 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00065: loss improved from 0.39137 to 0.39123, saving model to ./SEfold5000000650.768293.hdf5\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4079 - accuracy: 0.8872 - val_loss: 0.5085 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.39123\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.4008 - accuracy: 0.9055 - val_loss: 0.5207 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.39123\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.3672 - accuracy: 0.9299 - val_loss: 0.4990 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00068: loss improved from 0.39123 to 0.36687, saving model to ./SEfold5000000680.804878.hdf5\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3694 - accuracy: 0.9207 - val_loss: 0.5228 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.36687\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.3704 - accuracy: 0.9146 - val_loss: 0.5020 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.36687\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3432 - accuracy: 0.9390 - val_loss: 0.5131 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00071: loss improved from 0.36687 to 0.34329, saving model to ./SEfold5000000710.780488.hdf5\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.3706 - accuracy: 0.9085 - val_loss: 0.5376 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.34329\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.4097 - accuracy: 0.8841 - val_loss: 0.4964 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.34329\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3845 - accuracy: 0.9055 - val_loss: 0.5283 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.34329\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.3368 - accuracy: 0.9238 - val_loss: 0.4924 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00075: loss improved from 0.34329 to 0.33715, saving model to ./SEfold5000000750.780488.hdf5\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.3512 - accuracy: 0.9238 - val_loss: 0.5637 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.33715\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.3237 - accuracy: 0.9421 - val_loss: 0.4873 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00077: loss improved from 0.33715 to 0.32391, saving model to ./SEfold5000000770.804878.hdf5\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 10s 311ms/step - loss: 0.3116 - accuracy: 0.9329 - val_loss: 0.5719 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00078: loss improved from 0.32391 to 0.31186, saving model to ./SEfold5000000780.719512.hdf5\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3362 - accuracy: 0.9085 - val_loss: 0.4838 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.31186\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.3348 - accuracy: 0.9116 - val_loss: 0.4635 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.31186\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.3316 - accuracy: 0.9177 - val_loss: 0.4579 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00081: loss did not improve from 0.31186\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2892 - accuracy: 0.9512 - val_loss: 0.4871 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00082: loss improved from 0.31186 to 0.28937, saving model to ./SEfold5000000820.817073.hdf5\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2822 - accuracy: 0.9421 - val_loss: 0.4465 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00083: loss improved from 0.28937 to 0.28258, saving model to ./SEfold5000000830.829268.hdf5\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2772 - accuracy: 0.9451 - val_loss: 0.4925 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00084: loss improved from 0.28258 to 0.27752, saving model to ./SEfold5000000840.792683.hdf5\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2752 - accuracy: 0.9543 - val_loss: 0.5012 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00085: loss improved from 0.27752 to 0.27556, saving model to ./SEfold5000000850.792683.hdf5\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.3066 - accuracy: 0.9268 - val_loss: 0.5034 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00086: loss did not improve from 0.27556\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2719 - accuracy: 0.9512 - val_loss: 0.5435 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00087: loss improved from 0.27556 to 0.27215, saving model to ./SEfold5000000870.743902.hdf5\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2565 - accuracy: 0.9634 - val_loss: 0.5388 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00088: loss improved from 0.27215 to 0.25657, saving model to ./SEfold5000000880.731707.hdf5\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2641 - accuracy: 0.9390 - val_loss: 0.5979 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.25657\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2713 - accuracy: 0.9390 - val_loss: 0.5447 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.25657\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2543 - accuracy: 0.9482 - val_loss: 0.5234 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00091: loss improved from 0.25657 to 0.25450, saving model to ./SEfold5000000910.780488.hdf5\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 10s 313ms/step - loss: 0.2559 - accuracy: 0.9512 - val_loss: 0.5235 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.25450\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2703 - accuracy: 0.9390 - val_loss: 0.5051 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.25450\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 10s 316ms/step - loss: 0.2616 - accuracy: 0.9390 - val_loss: 0.5592 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.25450\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2138 - accuracy: 0.9726 - val_loss: 0.8822 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00095: loss improved from 0.25450 to 0.21398, saving model to ./SEfold5000000950.256098.hdf5\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2314 - accuracy: 0.9543 - val_loss: 0.5745 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.21398\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 10s 314ms/step - loss: 0.2142 - accuracy: 0.9695 - val_loss: 0.5028 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.21398\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2113 - accuracy: 0.9634 - val_loss: 0.5737 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00098: loss improved from 0.21398 to 0.21116, saving model to ./SEfold5000000980.768293.hdf5\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 10s 312ms/step - loss: 0.2144 - accuracy: 0.9604 - val_loss: 0.6167 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.21116\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 10s 315ms/step - loss: 0.2087 - accuracy: 0.9573 - val_loss: 0.5292 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00100: loss improved from 0.21116 to 0.20877, saving model to ./SEfold5000001000.804878.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0J3YG2DUZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def classifyEval(testX, testy,model):\n",
        "  # predict probabilities for test set  \n",
        "  yhat_probs = model.predict(testX, verbose=0)\n",
        "  yhat = np.max(yhat_probs,axis=1)\n",
        "  yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "  x=1\n",
        "  print(testy[:,x])\n",
        "  print(yhat_classes)\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(testy[:,x], yhat_classes)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(testy[:,x], yhat_classes)\n",
        "  print('Precision: %f' % precision)\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(testy[:,x], yhat_classes)\n",
        "  print('Recall: %f' % recall)\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(testy[:,x], yhat_classes)\n",
        "  print('F1 score: %f' % f1)\n",
        " \n",
        "  # kappa\n",
        "  kappa = cohen_kappa_score(testy[:,x], yhat_classes)\n",
        "  print('Cohens kappa: %f' % kappa)\n",
        "  # ROC AUC\n",
        "  auc = roc_auc_score(testy[:,x], yhat_probs[:,x])\n",
        "  print('ROC AUC: %f' % auc)\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(testy[:,x], yhat_classes)\n",
        "  print(matrix)\n",
        "\n",
        "  fpr_keras, tpr_keras, thresholds_keras = sklearn.metrics.roc_curve(testy[:,x], yhat_probs[:,x])\n",
        "  return fpr_keras, tpr_keras, thresholds_keras\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.plot(fpr_keras, tpr_keras, label='CV1 (area = {:.3f})'.format(auc))\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acURLMezCxA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpr_list = []\n",
        "fpr_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxXwIdDaTncb",
        "colab_type": "code",
        "outputId": "7ec68215-9354-4f4c-aba1-b770af4adf54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold5.npy')\n",
        "Y_test = np.load('/content/TestLabelFold5.npy')\n",
        "model = model2()\n",
        "model.load_weights('/content/SEfold5000000830.829268.hdf5')\n",
        "fpr, tpr, _ = classifyEval(X_test_extend, Y_test, model)\n",
        "fpr_list.append(fpr)\n",
        "tpr_list.append(tpr)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
            "[0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0\n",
            " 1 0 0 1 0 0 1 1]\n",
            "Accuracy: 0.829268\n",
            "Precision: 0.750000\n",
            "Recall: 0.450000\n",
            "F1 score: 0.562500\n",
            "Cohens kappa: 0.464552\n",
            "ROC AUC: 0.684677\n",
            "[[59  3]\n",
            " [11  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q99ruMRwFMuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auc = [.79,.62,.78,.83,.68]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2ejE2XEHlN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30c6903f-ff7d-4d09-e749-6cd00cbb8462"
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "for i in range(5):\n",
        "  plt.plot(fpr_list[i], tpr_list[i],label='CV{} (area = {:.2f})'.format(i,auc[i]))\n",
        "  plt.legend(loc='CV{}'.format(i))\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV0'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV1'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV2'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV3'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV4'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACD81-NjF5Sg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b691dd51-f7e5-4c50-a60f-598d5264b30f"
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold5.npy')\n",
        "Y_test = np.load('/content/TestLabelFold5.npy')\n",
        "model = model2()\n",
        "model.load_weights('/content/SEfold5000000830.829268.hdf5')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CavROEcGAv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Model(input=)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuERoQ6CUZZ0",
        "colab_type": "text"
      },
      "source": [
        "## Attention Model for Multi Instance Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtK_lHXrUZZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featureextractor(data):\n",
        "    conv = Conv2D(20, 5, activation = None, padding = 'same')(data)\n",
        "    conv_a = Activation('relu')(BatchNormalization()(conv))\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv_a)\n",
        "    conv1 = Conv2D(50, 5, activation = None, padding = 'same')(pool1)\n",
        "    conv1_a = Activation('relu')(BatchNormalization()(conv1))\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
        "    return pool2\n",
        "\n",
        "def featureextractorx2(data):\n",
        "    dense = Dense(500,activation='relu')(data)\n",
        "    return dense\n",
        "\n",
        "def Attention(data):\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    return dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3miMxRUZZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "l1factor=1e-2\n",
        "l2factor=0\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "prediction = MaxPool(axis=1)(dense_1)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbRDRdVcYkP",
        "colab_type": "code",
        "outputId": "fab69d65-a6a7-48f0-f900-18355494f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIHWT6X6fPv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUaTaZWirf0",
        "colab_type": "code",
        "outputId": "c155dade-a1b1-43bb-cb6f-7c13d55aadec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}\n",
        "Features = []\n",
        "\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "\n",
        "logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "dense_2 = Flatten()(logistic1)\n",
        "\n",
        "logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "dense_3 = Flatten()(logistic2)\n",
        "\n",
        "logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "dense_4 = Flatten()(logistic3)\n",
        "\n",
        "final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "prediction = MaxPool(axis=1)(final)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FwRbn0eiwib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "noises=50\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "datagen.fit(X_train_extend)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bg_PaepkBhP",
        "colab_type": "code",
        "outputId": "9567289f-268b-42bd-9eda-a9d2e29f67ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.00001),\n",
        "              metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(X_train_extend, Y_train,batch_size=2),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_valid_extend, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., epochs=100)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134/134 [==============================] - 35s 264ms/step - loss: 0.6514 - accuracy: 0.7127 - val_loss: 0.8838 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.6267 - accuracy: 0.7425 - val_loss: 0.7636 - val_accuracy: 0.4333\n",
            "Epoch 3/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.6024 - accuracy: 0.7836 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.5672 - accuracy: 0.8507 - val_loss: 0.5682 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5584 - accuracy: 0.8433 - val_loss: 0.5442 - val_accuracy: 0.8167\n",
            "Epoch 6/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.5589 - accuracy: 0.8097 - val_loss: 0.5946 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5288 - accuracy: 0.8545 - val_loss: 0.5800 - val_accuracy: 0.7667\n",
            "Epoch 8/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5151 - accuracy: 0.8582 - val_loss: 0.5377 - val_accuracy: 0.8500\n",
            "Epoch 9/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5261 - accuracy: 0.8358 - val_loss: 0.5270 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5130 - accuracy: 0.8470 - val_loss: 0.5216 - val_accuracy: 0.8500\n",
            "Epoch 11/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4840 - accuracy: 0.8731 - val_loss: 0.5267 - val_accuracy: 0.8167\n",
            "Epoch 12/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4953 - accuracy: 0.8619 - val_loss: 0.5123 - val_accuracy: 0.8167\n",
            "Epoch 13/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5000 - accuracy: 0.8433 - val_loss: 0.4984 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4907 - accuracy: 0.8507 - val_loss: 0.4877 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4814 - accuracy: 0.8694 - val_loss: 0.4697 - val_accuracy: 0.8833\n",
            "Epoch 16/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4705 - accuracy: 0.8806 - val_loss: 0.4655 - val_accuracy: 0.8833\n",
            "Epoch 17/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4634 - accuracy: 0.8843 - val_loss: 0.4711 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4425 - accuracy: 0.9030 - val_loss: 0.4611 - val_accuracy: 0.8833\n",
            "Epoch 19/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4562 - accuracy: 0.8843 - val_loss: 0.4742 - val_accuracy: 0.8500\n",
            "Epoch 20/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4466 - accuracy: 0.8881 - val_loss: 0.4702 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4490 - accuracy: 0.8843 - val_loss: 0.4764 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4406 - accuracy: 0.8993 - val_loss: 0.4772 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9067 - val_loss: 0.4578 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4504 - accuracy: 0.8881 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4518 - accuracy: 0.8806 - val_loss: 0.5093 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4471 - accuracy: 0.8769 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9104 - val_loss: 0.4399 - val_accuracy: 0.9000\n",
            "Epoch 28/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4114 - accuracy: 0.9254 - val_loss: 0.4405 - val_accuracy: 0.8833\n",
            "Epoch 29/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4215 - accuracy: 0.9067 - val_loss: 0.4320 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4280 - accuracy: 0.8918 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4124 - accuracy: 0.9179 - val_loss: 0.4489 - val_accuracy: 0.8833\n",
            "Epoch 32/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4201 - accuracy: 0.9067 - val_loss: 0.4370 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4097 - accuracy: 0.9104 - val_loss: 0.4642 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4228 - accuracy: 0.8993 - val_loss: 0.4591 - val_accuracy: 0.8500\n",
            "Epoch 35/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4080 - accuracy: 0.9179 - val_loss: 0.4386 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4136 - accuracy: 0.8993 - val_loss: 0.4663 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "134/134 [==============================] - 17s 124ms/step - loss: 0.4195 - accuracy: 0.8955 - val_loss: 0.4612 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4086 - accuracy: 0.9067 - val_loss: 0.4633 - val_accuracy: 0.8333\n",
            "Epoch 39/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4060 - accuracy: 0.9179 - val_loss: 0.4412 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4010 - accuracy: 0.9142 - val_loss: 0.4987 - val_accuracy: 0.8167\n",
            "Epoch 41/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4112 - accuracy: 0.9104 - val_loss: 0.4959 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3995 - accuracy: 0.9254 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3979 - accuracy: 0.9142 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4013 - accuracy: 0.9216 - val_loss: 0.4608 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3892 - accuracy: 0.9254 - val_loss: 0.4543 - val_accuracy: 0.8833\n",
            "Epoch 46/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3986 - accuracy: 0.9216 - val_loss: 0.4524 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3882 - accuracy: 0.9328 - val_loss: 0.4398 - val_accuracy: 0.8833\n",
            "Epoch 48/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3867 - accuracy: 0.9366 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 49/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3776 - accuracy: 0.9440 - val_loss: 0.4824 - val_accuracy: 0.8333\n",
            "Epoch 50/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3888 - accuracy: 0.9291 - val_loss: 0.4923 - val_accuracy: 0.8167\n",
            "Epoch 51/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3899 - accuracy: 0.9291 - val_loss: 0.4734 - val_accuracy: 0.8333\n",
            "Epoch 52/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4009 - accuracy: 0.9179 - val_loss: 0.4710 - val_accuracy: 0.8333\n",
            "Epoch 53/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3773 - accuracy: 0.9440 - val_loss: 0.5002 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3923 - accuracy: 0.9291 - val_loss: 0.4546 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3771 - accuracy: 0.9403 - val_loss: 0.4748 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3930 - accuracy: 0.9291 - val_loss: 0.4804 - val_accuracy: 0.8167\n",
            "Epoch 57/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3837 - accuracy: 0.9366 - val_loss: 0.4670 - val_accuracy: 0.8333\n",
            "Epoch 58/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3695 - accuracy: 0.9515 - val_loss: 0.4369 - val_accuracy: 0.8833\n",
            "Epoch 59/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3830 - accuracy: 0.9328 - val_loss: 0.4340 - val_accuracy: 0.8833\n",
            "Epoch 60/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3775 - accuracy: 0.9403 - val_loss: 0.4542 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3674 - accuracy: 0.9515 - val_loss: 0.4400 - val_accuracy: 0.8833\n",
            "Epoch 62/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3747 - accuracy: 0.9403 - val_loss: 0.4501 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3693 - accuracy: 0.9478 - val_loss: 0.4781 - val_accuracy: 0.8167\n",
            "Epoch 64/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3625 - accuracy: 0.9552 - val_loss: 0.4351 - val_accuracy: 0.8833\n",
            "Epoch 65/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3838 - accuracy: 0.9291 - val_loss: 0.4327 - val_accuracy: 0.8833\n",
            "Epoch 66/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9478 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
            "Epoch 67/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9515 - val_loss: 0.4767 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9440 - val_loss: 0.4532 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3763 - accuracy: 0.9366 - val_loss: 0.4743 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3589 - accuracy: 0.9552 - val_loss: 0.4877 - val_accuracy: 0.8167\n",
            "Epoch 71/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3560 - accuracy: 0.9664 - val_loss: 0.4616 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3646 - accuracy: 0.9552 - val_loss: 0.4886 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3543 - accuracy: 0.9627 - val_loss: 0.5030 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3562 - accuracy: 0.9627 - val_loss: 0.5077 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3523 - accuracy: 0.9664 - val_loss: 0.4707 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3576 - accuracy: 0.9590 - val_loss: 0.4801 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3603 - accuracy: 0.9552 - val_loss: 0.5191 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3669 - accuracy: 0.9440 - val_loss: 0.4686 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3591 - accuracy: 0.9515 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3708 - accuracy: 0.9403 - val_loss: 0.4465 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3495 - accuracy: 0.9701 - val_loss: 0.4657 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9552 - val_loss: 0.4355 - val_accuracy: 0.8833\n",
            "Epoch 83/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3573 - accuracy: 0.9515 - val_loss: 0.4195 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3440 - accuracy: 0.9739 - val_loss: 0.4263 - val_accuracy: 0.8833\n",
            "Epoch 85/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3515 - accuracy: 0.9664 - val_loss: 0.5191 - val_accuracy: 0.7833\n",
            "Epoch 86/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9627 - val_loss: 0.4628 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3695 - accuracy: 0.9440 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3635 - accuracy: 0.9515 - val_loss: 0.4759 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3463 - accuracy: 0.9664 - val_loss: 0.5083 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3585 - accuracy: 0.9515 - val_loss: 0.4583 - val_accuracy: 0.8333\n",
            "Epoch 91/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3517 - accuracy: 0.9590 - val_loss: 0.4490 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3496 - accuracy: 0.9701 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9590 - val_loss: 0.4643 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3498 - accuracy: 0.9590 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3492 - accuracy: 0.9627 - val_loss: 0.4788 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3438 - accuracy: 0.9664 - val_loss: 0.4847 - val_accuracy: 0.8167\n",
            "Epoch 97/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3446 - accuracy: 0.9739 - val_loss: 0.5205 - val_accuracy: 0.7833\n",
            "Epoch 98/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.3442 - accuracy: 0.9664 - val_loss: 0.4866 - val_accuracy: 0.8167\n",
            "Epoch 99/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3369 - accuracy: 0.9813 - val_loss: 0.4866 - val_accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3466 - accuracy: 0.9701 - val_loss: 0.4513 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb84798d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeGlE_Oz-RjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
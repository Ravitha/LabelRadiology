{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravitha/LabelRadiology/blob/master/MIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arW_6_PrkIlZ",
        "colab_type": "text"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthF7sh_UZX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dicom # some machines not install pydicom\n",
        "import scipy.misc\n",
        "import numpy as np \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle as cPickle\n",
        "#import matplotlib\n",
        "#import matplotlib.pyplot as plt \n",
        "from skimage.filters import threshold_otsu\n",
        "import os\n",
        "from os.path import join as join\n",
        "import csv\n",
        "import scipy.ndimage\n",
        "#import pydicom as dicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82DTupaeUZYI",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evR_WImPaUN0",
        "colab_type": "code",
        "outputId": "8e756606-385f-4097-8423-ac5ce2196cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcQkq4ShkC-6",
        "colab_type": "text"
      },
      "source": [
        "# Compose Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzRMqmRi0fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/drive/My Drive/CO-PO/INBreast'\n",
        "TrDataset = np.load(join(path,'Train.npy'))\n",
        "TrLabel = np.load(join(path,'TrainL.npy'))\n",
        "ValDataset= np.load(join(path,'valid.npy'))\n",
        "ValLabel = np.load(join(path,'validL.npy'))\n",
        "TDataset = np.load(join(path,'Test.npy'))\n",
        "TLabel = np.load(join(path,'TestL.npy'))\n",
        "\n",
        "Dataset = np.concatenate((TrDataset,ValDataset,TDataset),axis = 0)\n",
        "Labels = np.concatenate((TrLabel,ValLabel,TLabel),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRB7_brfjY3g",
        "colab_type": "code",
        "outputId": "f68a9c2f-c969-4564-ea6e-62104888e61b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "np.save('Dataset.npy', Dataset)\n",
        "np.save('Label.npy', Labels)\n",
        "\n",
        "np.unique(Labels, return_counts=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256)\n",
            "(410,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crX86A3FUZZI",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXWpwOAkOVK",
        "colab_type": "code",
        "outputId": "5b8a2332-e7d7-4718-d56d-b24f0476905d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "nb_classes = 2\n",
        "Dataset = Dataset.reshape((410,256,256,1))\n",
        "Dataset_extend = np.zeros((Dataset.shape[0],256, 256,3))\n",
        "for i in range(Dataset.shape[0]):\n",
        "    rex = np.resize(Dataset[i,:,:,:], (256, 256))\n",
        "    Dataset_extend[i,:,:,0] = rex\n",
        "    Dataset_extend[i,:,:,1] = rex\n",
        "    Dataset_extend[i,:,:,2] = rex\n",
        "Dataset = Dataset_extend\n",
        "Labels = np_utils.to_categorical(Labels, nb_classes)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqsOUAXIlSL1",
        "colab_type": "text"
      },
      "source": [
        "# Print Data Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNitNFUBlQXT",
        "colab_type": "code",
        "outputId": "dfb40ec1-6737-409c-be9f-b9ac697246fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256, 3)\n",
            "(410, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_WtGDWTeiI",
        "colab_type": "code",
        "outputId": "92d5db44-cdf3-47f3-9537-121012739e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.unique(Labels[:,1],return_counts = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.], dtype=float32), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_SNbuEjS9SP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label Assignment\n",
        "Labels_Assign = np.zeros((410,64))\n",
        "for i in range(0,410):\n",
        "  if(Labels[i,0] == 1):\n",
        "    Labels_Assign[i,:-2] = 1\n",
        "  else:\n",
        "    Labels_Assign[i,62:64] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaqLtUTiUXpJ",
        "colab_type": "code",
        "outputId": "9030b0a8-2517-4c82-a9a8-6e9f2b3c65c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(Labels_Assign[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4qn6IkmSC1",
        "colab_type": "text"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKoXcmbmUCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model1():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLHg7YfSOZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "def model2():\n",
        "\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  \n",
        "  out1 = Lambda(lambda image: tf.image.resize(image, (4, 4)))(model.output)\n",
        "  out2 = Lambda(lambda image: tf.image.resize(image, (2, 2)))(model.output)\n",
        "  out3 = Lambda(lambda image: tf.image.resize(image, (6, 6)))(model.output)\n",
        "\n",
        "  SL1 = Conv2D(1, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "  SL2 = BatchNormalization()\n",
        "  SL3 = Conv2D(1, 1, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "\n",
        "  resize1 = Flatten()(SL3(SL2(SL1(out1))))\n",
        "  resize2 = Flatten()(SL3(SL2(SL1(out2))))\n",
        "  resize3 = Flatten()(SL3(SL2(SL1(out3))))\n",
        "\n",
        "  M1 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize1)\n",
        "  M2 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize2)\n",
        "  M3 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize3)\n",
        "\n",
        "  added10 = concatenate([M1,M2,M3], axis=1)\n",
        "  final = Dense(2, activation='softmax')(added10)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSm1LGMzKbeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class KPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(KPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],64))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output = tf.sort(x)\n",
        "        output=K.concatenate([1-output[:,:-2], output[:,62:64]])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 64)\n",
        "\n",
        "def model3():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = KPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN0ILfCIiRLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        max_val = K.max(x, axis=-1,keepdims=True)\n",
        "        self.add_loss(l1(1e-5)(x))\n",
        "        output=K.concatenate([1-max_val,max_val])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model4():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rym-sZ_dxvp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "def model5():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "\n",
        "  logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "  dense_2 = Flatten()(logistic1)\n",
        "\n",
        "  logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "  dense_3 = Flatten()(logistic2)\n",
        "\n",
        "  logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "  dense_4 = Flatten()(logistic3)\n",
        "\n",
        "  final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "  prediction = MaxPool(axis=1)(final)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K922EQst61r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model6():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  dense = Flatten()(model.output)\n",
        "  final = Dense(2, activation='softmax')(dense)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOIFdxJVvqWS",
        "colab_type": "code",
        "outputId": "bcc7eca8-0ed3-40d7-8d6b-3334d2e4e50d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = model6()\n",
        "print(model.summary())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 7s 0us/step\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 131072)       0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            262146      flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 23,849,858\n",
            "Trainable params: 23,796,738\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssCxcR7lutw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFdleZCluLO",
        "colab_type": "code",
        "outputId": "3a317747-97ab-4dab-eac1-df2b6fc423af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Necessary Imports\n",
        "import numpy as np \n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "print('[INFO:Applying CV]')\n",
        "skf = KFold(n_splits=5)\n",
        "skf.get_n_splits(Dataset)\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(Dataset):\n",
        "    print(['FOLD:'], fold)\n",
        "    # Construct data from indexes\n",
        "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
        "    y_train, y_test = Labels[train_index], Labels[test_index]\n",
        "    \n",
        "    # Model Construction\n",
        "    model = model6()\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "    datagen.fit(X_train)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "    Stopping_Criterion = EarlyStopping(patience=50, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint('./SEfold'+str(fold)+'{epoch:08d}{val_accuracy:05f}.hdf5', monitor='val_accuracy',verbose=1,save_best_only=True)\n",
        "    model.fit_generator(datagen.flow(X_train, y_train,batch_size=10),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_test , y_test ),callbacks=[model_checkpoint, Stopping_Criterion])\n",
        "    datafilename = 'TestFold'+str(fold)+'.npy'\n",
        "    labelfilename = 'TestLabelFold'+str(fold)+'.npy'\n",
        "    np.save(datafilename,X_test)\n",
        "    np.save(labelfilename,y_test)\n",
        "    fold+=1\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO:Applying CV]\n",
            "['FOLD:'] 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 51s 2s/step - loss: 1.7247 - accuracy: 0.6463 - val_loss: 1.6190 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75610, saving model to ./SEfold1000000010.756098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 1.6859 - accuracy: 0.6829 - val_loss: 1.6186 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.75610\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 1.4264 - accuracy: 0.7500 - val_loss: 1.8654 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.75610\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 1.5841 - accuracy: 0.7134 - val_loss: 1.1339 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 1.3201 - accuracy: 0.7896 - val_loss: 1.4891 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 1.3865 - accuracy: 0.7561 - val_loss: 0.7578 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 24s 715ms/step - loss: 1.3452 - accuracy: 0.7713 - val_loss: 2.4347 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 1.0518 - accuracy: 0.7805 - val_loss: 2.3920 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 1.0791 - accuracy: 0.7866 - val_loss: 1.9101 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.8737 - accuracy: 0.8018 - val_loss: 0.6368 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.6774 - accuracy: 0.8354 - val_loss: 1.1433 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.4924 - accuracy: 0.8659 - val_loss: 1.3562 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.4532 - accuracy: 0.8445 - val_loss: 1.2946 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.7533 - accuracy: 0.8537 - val_loss: 0.8359 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 24s 716ms/step - loss: 0.3939 - accuracy: 0.8902 - val_loss: 0.6868 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.6102 - accuracy: 0.8720 - val_loss: 0.7177 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.3737 - accuracy: 0.8872 - val_loss: 0.7431 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.4699 - accuracy: 0.8994 - val_loss: 0.6531 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.3323 - accuracy: 0.9207 - val_loss: 0.6339 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.5061 - accuracy: 0.8628 - val_loss: 0.6748 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.7444 - accuracy: 0.8628 - val_loss: 0.7783 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.5118 - accuracy: 0.8750 - val_loss: 2.3456 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 24s 716ms/step - loss: 0.3061 - accuracy: 0.8994 - val_loss: 6.7668 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.3125 - accuracy: 0.9024 - val_loss: 3.7852 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.1756 - accuracy: 0.9360 - val_loss: 2.1096 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.2368 - accuracy: 0.9238 - val_loss: 2.5640 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.2679 - accuracy: 0.9238 - val_loss: 2.1033 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.1289 - accuracy: 0.9482 - val_loss: 3.4366 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.2177 - accuracy: 0.9207 - val_loss: 4.1425 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1688 - accuracy: 0.9482 - val_loss: 5.2143 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.2573 - accuracy: 0.9390 - val_loss: 3.5002 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.1394 - accuracy: 0.9512 - val_loss: 3.2483 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.1975 - accuracy: 0.9390 - val_loss: 2.2076 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.2388 - accuracy: 0.9268 - val_loss: 2.5181 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.75610\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.2309 - accuracy: 0.9146 - val_loss: 2.6270 - val_accuracy: 0.4024\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.75610\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1087 - accuracy: 0.9665 - val_loss: 1.4044 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.75610\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.1479 - accuracy: 0.9604 - val_loss: 1.4323 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.75610\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.2175 - accuracy: 0.9390 - val_loss: 0.9542 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.75610\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.1206 - accuracy: 0.9543 - val_loss: 1.5799 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00039: val_accuracy improved from 0.75610 to 0.82927, saving model to ./SEfold1000000390.829268.hdf5\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.2118 - accuracy: 0.9512 - val_loss: 1.0451 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00040: val_accuracy improved from 0.82927 to 0.84146, saving model to ./SEfold1000000400.841463.hdf5\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.1030 - accuracy: 0.9665 - val_loss: 1.0856 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.84146 to 0.86585, saving model to ./SEfold1000000410.865854.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1175 - accuracy: 0.9665 - val_loss: 1.5541 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.86585\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.2272 - accuracy: 0.9360 - val_loss: 0.5960 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.86585\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1099 - accuracy: 0.9543 - val_loss: 0.7173 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.86585 to 0.87805, saving model to ./SEfold1000000440.878049.hdf5\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.2052 - accuracy: 0.9390 - val_loss: 1.0583 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.87805 to 0.89024, saving model to ./SEfold1000000450.890244.hdf5\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.0850 - accuracy: 0.9695 - val_loss: 0.9136 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89024\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.2204 - accuracy: 0.9299 - val_loss: 0.7680 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89024\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.1781 - accuracy: 0.9451 - val_loss: 0.8147 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89024\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1029 - accuracy: 0.9726 - val_loss: 0.6558 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89024\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1920 - accuracy: 0.9573 - val_loss: 0.7519 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89024\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.1155 - accuracy: 0.9482 - val_loss: 0.6762 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89024\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.0675 - accuracy: 0.9756 - val_loss: 0.7907 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89024\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.0902 - accuracy: 0.9756 - val_loss: 0.7537 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89024\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.0426 - accuracy: 0.9848 - val_loss: 1.0931 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89024\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.0715 - accuracy: 0.9756 - val_loss: 0.8354 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89024\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1324 - accuracy: 0.9634 - val_loss: 1.1702 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89024\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.1070 - accuracy: 0.9573 - val_loss: 2.0839 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89024\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.2026 - accuracy: 0.9665 - val_loss: 1.4948 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89024\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.2270 - accuracy: 0.9451 - val_loss: 0.8118 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89024\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.1248 - accuracy: 0.9634 - val_loss: 1.2580 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89024\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.0860 - accuracy: 0.9665 - val_loss: 1.4228 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89024\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.0427 - accuracy: 0.9848 - val_loss: 1.4943 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89024\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.1149 - accuracy: 0.9726 - val_loss: 1.0760 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89024\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1060 - accuracy: 0.9726 - val_loss: 0.6998 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89024\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.1163 - accuracy: 0.9756 - val_loss: 0.8207 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.89024\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.0811 - accuracy: 0.9695 - val_loss: 1.7405 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.89024\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.1736 - accuracy: 0.9543 - val_loss: 2.9918 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.89024\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1670 - accuracy: 0.9604 - val_loss: 3.0005 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.89024\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.1069 - accuracy: 0.9543 - val_loss: 4.1374 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.89024\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1558 - accuracy: 0.9695 - val_loss: 2.6078 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.89024\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.1802 - accuracy: 0.9634 - val_loss: 0.7597 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.89024\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.0851 - accuracy: 0.9756 - val_loss: 0.6543 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.89024\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.0525 - accuracy: 0.9787 - val_loss: 0.9037 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.89024\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 24s 715ms/step - loss: 0.0852 - accuracy: 0.9817 - val_loss: 1.0250 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.89024\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.1924 - accuracy: 0.9543 - val_loss: 1.0865 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.89024\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 24s 715ms/step - loss: 0.0345 - accuracy: 0.9848 - val_loss: 1.0113 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.89024\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.1092 - accuracy: 0.9726 - val_loss: 1.8572 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.89024\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.0432 - accuracy: 0.9817 - val_loss: 1.0783 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.89024\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.1771 - accuracy: 0.9451 - val_loss: 1.1148 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.89024\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.0949 - accuracy: 0.9695 - val_loss: 1.4612 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.89024\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.1090 - accuracy: 0.9787 - val_loss: 1.8083 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.89024\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 24s 715ms/step - loss: 0.0612 - accuracy: 0.9787 - val_loss: 1.5525 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.89024\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.1561 - accuracy: 0.9634 - val_loss: 1.0736 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.89024\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 24s 716ms/step - loss: 0.1217 - accuracy: 0.9634 - val_loss: 1.2045 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.89024\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.1447 - accuracy: 0.9543 - val_loss: 2.9987 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.89024\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 24s 715ms/step - loss: 0.2787 - accuracy: 0.9329 - val_loss: 3.2458 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.89024\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 24s 715ms/step - loss: 0.3672 - accuracy: 0.9299 - val_loss: 2.5209 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.89024\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.3685 - accuracy: 0.9146 - val_loss: 1.0042 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.89024\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.2158 - accuracy: 0.9665 - val_loss: 1.1956 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.89024\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.2070 - accuracy: 0.9634 - val_loss: 1.4950 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.89024\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 24s 716ms/step - loss: 0.1877 - accuracy: 0.9390 - val_loss: 1.0441 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.89024\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.1284 - accuracy: 0.9543 - val_loss: 1.9065 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.89024\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.0886 - accuracy: 0.9665 - val_loss: 1.3228 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.89024\n",
            "Epoch 00093: early stopping\n",
            "['FOLD:'] 2\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 42s 1s/step - loss: 2.3302 - accuracy: 0.6707 - val_loss: 1.3192 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75610, saving model to ./SEfold2000000010.756098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 1.7064 - accuracy: 0.6982 - val_loss: 0.5917 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.75610\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 1.3094 - accuracy: 0.7622 - val_loss: 0.7640 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.75610\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 1.9225 - accuracy: 0.7652 - val_loss: 1.0534 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 1.2269 - accuracy: 0.7713 - val_loss: 0.9605 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 1.1350 - accuracy: 0.8110 - val_loss: 0.9130 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.9893 - accuracy: 0.8079 - val_loss: 1.1897 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.8546 - accuracy: 0.8232 - val_loss: 0.6274 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.6484 - accuracy: 0.8476 - val_loss: 1.7714 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.8043 - accuracy: 0.8262 - val_loss: 3.2392 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.4279 - accuracy: 0.8689 - val_loss: 0.8071 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.4713 - accuracy: 0.8689 - val_loss: 1.8800 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.6114 - accuracy: 0.8476 - val_loss: 1.4650 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.5889 - accuracy: 0.8537 - val_loss: 0.7098 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.3699 - accuracy: 0.8872 - val_loss: 1.7340 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.3936 - accuracy: 0.8933 - val_loss: 1.5545 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.6910 - accuracy: 0.8476 - val_loss: 0.8103 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.4605 - accuracy: 0.8506 - val_loss: 1.4145 - val_accuracy: 0.2195\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.3695 - accuracy: 0.8963 - val_loss: 1.0878 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.4241 - accuracy: 0.8750 - val_loss: 1.4164 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.4227 - accuracy: 0.8963 - val_loss: 1.7760 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.4173 - accuracy: 0.8872 - val_loss: 2.5126 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.4013 - accuracy: 0.8963 - val_loss: 2.1917 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.3029 - accuracy: 0.9055 - val_loss: 4.1321 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.4129 - accuracy: 0.9055 - val_loss: 2.3082 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.5416 - accuracy: 0.8872 - val_loss: 1.1091 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.3172 - accuracy: 0.8811 - val_loss: 0.7352 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.1692 - accuracy: 0.9421 - val_loss: 0.6163 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.1459 - accuracy: 0.9451 - val_loss: 0.7461 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.2251 - accuracy: 0.9207 - val_loss: 0.8678 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.4453 - accuracy: 0.9024 - val_loss: 1.2171 - val_accuracy: 0.4634\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.3776 - accuracy: 0.8994 - val_loss: 0.8754 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.2497 - accuracy: 0.9421 - val_loss: 0.7249 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.2384 - accuracy: 0.9299 - val_loss: 0.6201 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.75610\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.3473 - accuracy: 0.9329 - val_loss: 1.1725 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.75610 to 0.76829, saving model to ./SEfold2000000350.768293.hdf5\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 24s 714ms/step - loss: 0.3209 - accuracy: 0.9116 - val_loss: 1.0938 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.76829 to 0.78049, saving model to ./SEfold2000000360.780488.hdf5\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.1434 - accuracy: 0.9482 - val_loss: 0.6841 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.78049 to 0.81707, saving model to ./SEfold2000000370.817073.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.1913 - accuracy: 0.9512 - val_loss: 0.7669 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.81707\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.0930 - accuracy: 0.9604 - val_loss: 0.6421 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.81707\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1820 - accuracy: 0.9421 - val_loss: 1.3743 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.81707\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.1848 - accuracy: 0.9573 - val_loss: 1.2423 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.81707\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 0.2265 - accuracy: 0.9451 - val_loss: 1.3816 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.81707\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.1596 - accuracy: 0.9421 - val_loss: 1.2635 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.81707\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.1113 - accuracy: 0.9695 - val_loss: 1.3520 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.81707\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 23s 712ms/step - loss: 0.1533 - accuracy: 0.9512 - val_loss: 1.3039 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.81707\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.1424 - accuracy: 0.9512 - val_loss: 1.6575 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.81707\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.2213 - accuracy: 0.9207 - val_loss: 2.0067 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.81707\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1244 - accuracy: 0.9543 - val_loss: 1.5457 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.81707\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.4058 - accuracy: 0.9055 - val_loss: 1.5831 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.81707\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 24s 712ms/step - loss: 0.2431 - accuracy: 0.9390 - val_loss: 1.8248 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.81707\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1589 - accuracy: 0.9390 - val_loss: 0.9382 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.81707\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.1231 - accuracy: 0.9665 - val_loss: 1.3835 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.81707\n",
            "Epoch 00052: early stopping\n",
            "['FOLD:'] 3\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 42s 1s/step - loss: 2.4001 - accuracy: 0.6463 - val_loss: 1.8151 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75610, saving model to ./SEfold3000000010.756098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 1.5098 - accuracy: 0.6890 - val_loss: 1.2698 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.75610\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 1.8024 - accuracy: 0.7073 - val_loss: 0.5879 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.75610\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 1.0690 - accuracy: 0.7927 - val_loss: 1.9403 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 1.0656 - accuracy: 0.7896 - val_loss: 1.0898 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 24s 713ms/step - loss: 1.2780 - accuracy: 0.7927 - val_loss: 1.4371 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 1.0588 - accuracy: 0.8018 - val_loss: 1.3891 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.6800 - accuracy: 0.8354 - val_loss: 1.3053 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.5352 - accuracy: 0.8780 - val_loss: 0.7349 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.8336 - accuracy: 0.8537 - val_loss: 0.9619 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.6992 - accuracy: 0.8476 - val_loss: 0.5885 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.5594 - accuracy: 0.8384 - val_loss: 1.8397 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.5401 - accuracy: 0.8415 - val_loss: 1.5642 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.7830 - accuracy: 0.8415 - val_loss: 2.3347 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.7292 - accuracy: 0.8384 - val_loss: 1.0305 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.5550 - accuracy: 0.8415 - val_loss: 1.0000 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.7018 - accuracy: 0.8537 - val_loss: 1.2408 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.5052 - accuracy: 0.8902 - val_loss: 1.6302 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3654 - accuracy: 0.8994 - val_loss: 1.6501 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3022 - accuracy: 0.9085 - val_loss: 1.3949 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.3718 - accuracy: 0.8902 - val_loss: 0.7537 - val_accuracy: 0.5610\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.1979 - accuracy: 0.9268 - val_loss: 0.8455 - val_accuracy: 0.4878\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.2009 - accuracy: 0.9177 - val_loss: 1.3520 - val_accuracy: 0.3537\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.1577 - accuracy: 0.9421 - val_loss: 1.7954 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.1612 - accuracy: 0.9451 - val_loss: 1.4172 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.3198 - accuracy: 0.9116 - val_loss: 1.2796 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.1533 - accuracy: 0.9390 - val_loss: 1.1496 - val_accuracy: 0.4878\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.2231 - accuracy: 0.9543 - val_loss: 1.9524 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.3123 - accuracy: 0.9116 - val_loss: 2.1936 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.2683 - accuracy: 0.9390 - val_loss: 1.0078 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.2379 - accuracy: 0.9238 - val_loss: 3.3682 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.2586 - accuracy: 0.9451 - val_loss: 1.2782 - val_accuracy: 0.5610\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.2518 - accuracy: 0.9299 - val_loss: 1.7264 - val_accuracy: 0.5122\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.1852 - accuracy: 0.9604 - val_loss: 1.2331 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.75610\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1223 - accuracy: 0.9726 - val_loss: 1.5091 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.75610\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.1855 - accuracy: 0.9634 - val_loss: 2.1083 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.75610 to 0.76829, saving model to ./SEfold3000000360.768293.hdf5\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.0975 - accuracy: 0.9634 - val_loss: 2.1701 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.76829\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.1330 - accuracy: 0.9695 - val_loss: 2.0456 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.76829\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.0878 - accuracy: 0.9665 - val_loss: 2.3647 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.76829\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.1237 - accuracy: 0.9634 - val_loss: 2.4698 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.76829\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.1232 - accuracy: 0.9573 - val_loss: 2.1669 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.76829 to 0.78049, saving model to ./SEfold3000000410.780488.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.2306 - accuracy: 0.9573 - val_loss: 1.4657 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.78049\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.1879 - accuracy: 0.9482 - val_loss: 2.0751 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.78049 to 0.82927, saving model to ./SEfold3000000430.829268.hdf5\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.1492 - accuracy: 0.9512 - val_loss: 2.7618 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.82927\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.1431 - accuracy: 0.9573 - val_loss: 3.4318 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.82927\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.1381 - accuracy: 0.9604 - val_loss: 2.6729 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.82927\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.0705 - accuracy: 0.9695 - val_loss: 2.9103 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.82927\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 0.0536 - accuracy: 0.9909 - val_loss: 2.3139 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.82927\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 0.0843 - accuracy: 0.9634 - val_loss: 3.7848 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.82927\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.1283 - accuracy: 0.9482 - val_loss: 2.5671 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.82927\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.1346 - accuracy: 0.9695 - val_loss: 3.0056 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.82927\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.0734 - accuracy: 0.9665 - val_loss: 1.8837 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.82927\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.2333 - accuracy: 0.9482 - val_loss: 1.4578 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.82927\n",
            "Epoch 00053: early stopping\n",
            "['FOLD:'] 4\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 43s 1s/step - loss: 2.2516 - accuracy: 0.6311 - val_loss: 2.2138 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold4000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 1.4573 - accuracy: 0.7104 - val_loss: 0.7287 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 1.5075 - accuracy: 0.7165 - val_loss: 0.6549 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.24390 to 0.75610, saving model to ./SEfold4000000030.756098.hdf5\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 23s 710ms/step - loss: 2.1089 - accuracy: 0.7043 - val_loss: 0.7833 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 1.2251 - accuracy: 0.7805 - val_loss: 0.9114 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 1.1726 - accuracy: 0.7591 - val_loss: 0.9120 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.8329 - accuracy: 0.8293 - val_loss: 1.0015 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 1.0822 - accuracy: 0.8049 - val_loss: 0.9240 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.7380 - accuracy: 0.8323 - val_loss: 1.2709 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.6552 - accuracy: 0.8476 - val_loss: 4.0558 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.7415 - accuracy: 0.8232 - val_loss: 3.4829 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.6407 - accuracy: 0.8476 - val_loss: 8.0457 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.6457 - accuracy: 0.8323 - val_loss: 13.1235 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.4142 - accuracy: 0.8720 - val_loss: 4.5144 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.6589 - accuracy: 0.8567 - val_loss: 3.9115 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.5670 - accuracy: 0.8384 - val_loss: 1.0369 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.5057 - accuracy: 0.8689 - val_loss: 1.1438 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.4105 - accuracy: 0.8841 - val_loss: 1.5247 - val_accuracy: 0.4878\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.5691 - accuracy: 0.8780 - val_loss: 0.8870 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.3505 - accuracy: 0.9055 - val_loss: 0.7091 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.5789 - accuracy: 0.8567 - val_loss: 0.7302 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.5309 - accuracy: 0.8750 - val_loss: 0.6391 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.4101 - accuracy: 0.8933 - val_loss: 0.6558 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.3579 - accuracy: 0.8872 - val_loss: 1.0260 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3172 - accuracy: 0.9085 - val_loss: 1.1336 - val_accuracy: 0.4878\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 23s 702ms/step - loss: 0.3563 - accuracy: 0.9146 - val_loss: 0.7955 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.2701 - accuracy: 0.9177 - val_loss: 1.0258 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.2276 - accuracy: 0.9360 - val_loss: 1.4326 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1967 - accuracy: 0.9360 - val_loss: 1.0924 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1690 - accuracy: 0.9390 - val_loss: 1.4858 - val_accuracy: 0.4634\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.1323 - accuracy: 0.9543 - val_loss: 1.4711 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1952 - accuracy: 0.9329 - val_loss: 1.9536 - val_accuracy: 0.4146\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1955 - accuracy: 0.9360 - val_loss: 1.3041 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1760 - accuracy: 0.9299 - val_loss: 1.0427 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.75610\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.2521 - accuracy: 0.9207 - val_loss: 3.7518 - val_accuracy: 0.3293\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.75610\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.3076 - accuracy: 0.9146 - val_loss: 8.2855 - val_accuracy: 0.3049\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.75610\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.2537 - accuracy: 0.9268 - val_loss: 8.5813 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.75610\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.2970 - accuracy: 0.9360 - val_loss: 1.5726 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.75610\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1392 - accuracy: 0.9390 - val_loss: 1.5765 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.75610\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1382 - accuracy: 0.9390 - val_loss: 1.0402 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00040: val_accuracy improved from 0.75610 to 0.78049, saving model to ./SEfold4000000400.780488.hdf5\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.1968 - accuracy: 0.9299 - val_loss: 1.0772 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.78049\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.2042 - accuracy: 0.9634 - val_loss: 1.2812 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.78049\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.1260 - accuracy: 0.9665 - val_loss: 0.8767 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.78049\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.1527 - accuracy: 0.9512 - val_loss: 0.7869 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.78049 to 0.81707, saving model to ./SEfold4000000440.817073.hdf5\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.1146 - accuracy: 0.9695 - val_loss: 0.7892 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.81707\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.1503 - accuracy: 0.9604 - val_loss: 1.1709 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.81707\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.1459 - accuracy: 0.9634 - val_loss: 0.7345 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.81707 to 0.82927, saving model to ./SEfold4000000470.829268.hdf5\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 23s 702ms/step - loss: 0.1287 - accuracy: 0.9390 - val_loss: 1.0502 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.82927\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.1307 - accuracy: 0.9543 - val_loss: 0.8166 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00049: val_accuracy improved from 0.82927 to 0.86585, saving model to ./SEfold4000000490.865854.hdf5\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.2421 - accuracy: 0.9482 - val_loss: 0.6897 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.86585\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1672 - accuracy: 0.9390 - val_loss: 1.2053 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.86585\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.4269 - accuracy: 0.9024 - val_loss: 2.6977 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.86585\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3340 - accuracy: 0.9329 - val_loss: 1.2295 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.86585\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.4957 - accuracy: 0.8872 - val_loss: 3.3637 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.86585\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.2467 - accuracy: 0.9390 - val_loss: 2.7041 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.86585\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.0990 - accuracy: 0.9482 - val_loss: 3.4460 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.86585\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.2252 - accuracy: 0.9482 - val_loss: 2.1016 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.86585\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.2103 - accuracy: 0.9360 - val_loss: 1.5809 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.86585\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1737 - accuracy: 0.9604 - val_loss: 1.5004 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.86585\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.2966 - accuracy: 0.9055 - val_loss: 1.4788 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.86585\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.2320 - accuracy: 0.9451 - val_loss: 1.3857 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.86585\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.1647 - accuracy: 0.9512 - val_loss: 1.3808 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.86585\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1601 - accuracy: 0.9512 - val_loss: 0.9122 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.86585\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.2757 - accuracy: 0.9482 - val_loss: 1.4911 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.86585\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1798 - accuracy: 0.9451 - val_loss: 1.3106 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.86585\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1377 - accuracy: 0.9604 - val_loss: 1.1149 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.86585\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.1466 - accuracy: 0.9421 - val_loss: 1.6631 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.86585\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.1958 - accuracy: 0.9451 - val_loss: 1.3564 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.86585\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.1597 - accuracy: 0.9543 - val_loss: 1.3151 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.86585\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.1424 - accuracy: 0.9665 - val_loss: 1.6703 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.86585\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.1978 - accuracy: 0.9451 - val_loss: 1.7175 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.86585\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1547 - accuracy: 0.9543 - val_loss: 2.7658 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.86585\n",
            "Epoch 00072: early stopping\n",
            "['FOLD:'] 5\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 43s 1s/step - loss: 1.8266 - accuracy: 0.6494 - val_loss: 0.8582 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75610, saving model to ./SEfold5000000010.756098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 23s 711ms/step - loss: 1.6098 - accuracy: 0.7073 - val_loss: 1.4117 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.75610\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 1.8576 - accuracy: 0.7470 - val_loss: 1.0003 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.75610\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 1.1281 - accuracy: 0.8293 - val_loss: 1.2639 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 1.2378 - accuracy: 0.7835 - val_loss: 3.4510 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.9670 - accuracy: 0.8201 - val_loss: 3.7508 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 1.1458 - accuracy: 0.8079 - val_loss: 3.0102 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 1.1121 - accuracy: 0.8079 - val_loss: 2.0834 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.9227 - accuracy: 0.8445 - val_loss: 2.1906 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.9563 - accuracy: 0.8262 - val_loss: 2.5502 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.7798 - accuracy: 0.8262 - val_loss: 1.6437 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.4304 - accuracy: 0.8750 - val_loss: 1.4801 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.5645 - accuracy: 0.8750 - val_loss: 1.6299 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 23s 708ms/step - loss: 0.5399 - accuracy: 0.8811 - val_loss: 1.4445 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3424 - accuracy: 0.9116 - val_loss: 1.9098 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3481 - accuracy: 0.8933 - val_loss: 1.9203 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3357 - accuracy: 0.9085 - val_loss: 2.0801 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.5943 - accuracy: 0.8628 - val_loss: 3.3569 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.4117 - accuracy: 0.8933 - val_loss: 1.1125 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 23s 702ms/step - loss: 0.3508 - accuracy: 0.9116 - val_loss: 2.6107 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.4013 - accuracy: 0.9116 - val_loss: 1.9863 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.2251 - accuracy: 0.9146 - val_loss: 1.3803 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.3226 - accuracy: 0.9116 - val_loss: 1.6983 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3933 - accuracy: 0.9055 - val_loss: 1.1686 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.3380 - accuracy: 0.8963 - val_loss: 1.0856 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.1891 - accuracy: 0.9299 - val_loss: 0.8540 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.4192 - accuracy: 0.8933 - val_loss: 0.8296 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.4705 - accuracy: 0.8994 - val_loss: 0.7434 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 23s 707ms/step - loss: 0.2828 - accuracy: 0.9238 - val_loss: 0.8391 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1699 - accuracy: 0.9329 - val_loss: 1.1480 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.3012 - accuracy: 0.9238 - val_loss: 0.8100 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 23s 709ms/step - loss: 0.1540 - accuracy: 0.9573 - val_loss: 0.8521 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1517 - accuracy: 0.9634 - val_loss: 0.8641 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.1964 - accuracy: 0.9604 - val_loss: 0.9626 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.75610\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 23s 701ms/step - loss: 0.2005 - accuracy: 0.9451 - val_loss: 0.9048 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.75610\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 23s 702ms/step - loss: 0.1030 - accuracy: 0.9512 - val_loss: 0.9248 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.75610\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 23s 701ms/step - loss: 0.1983 - accuracy: 0.9390 - val_loss: 1.0252 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.75610\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.2046 - accuracy: 0.9543 - val_loss: 1.6600 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.75610 to 0.78049, saving model to ./SEfold5000000380.780488.hdf5\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.1668 - accuracy: 0.9482 - val_loss: 1.8694 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.78049\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 23s 705ms/step - loss: 0.1359 - accuracy: 0.9329 - val_loss: 1.6012 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.78049\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 23s 702ms/step - loss: 0.1880 - accuracy: 0.9360 - val_loss: 0.8972 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.78049 to 0.80488, saving model to ./SEfold5000000410.804878.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 23s 702ms/step - loss: 0.1248 - accuracy: 0.9665 - val_loss: 1.0055 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.80488\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.0496 - accuracy: 0.9817 - val_loss: 1.3517 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.80488\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.1601 - accuracy: 0.9726 - val_loss: 1.2958 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.80488\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.3051 - accuracy: 0.9329 - val_loss: 1.4991 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.80488\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.3792 - accuracy: 0.9238 - val_loss: 2.0055 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.80488\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 23s 701ms/step - loss: 0.1291 - accuracy: 0.9665 - val_loss: 2.4554 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.80488\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 23s 701ms/step - loss: 0.2180 - accuracy: 0.9512 - val_loss: 1.7998 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.80488\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.0628 - accuracy: 0.9665 - val_loss: 1.3322 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.80488\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 23s 701ms/step - loss: 0.1028 - accuracy: 0.9726 - val_loss: 2.0231 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.80488\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.1034 - accuracy: 0.9726 - val_loss: 1.6109 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.80488\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.1536 - accuracy: 0.9573 - val_loss: 1.3818 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.80488\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.1579 - accuracy: 0.9390 - val_loss: 2.0036 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.80488\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 23s 703ms/step - loss: 0.1131 - accuracy: 0.9726 - val_loss: 2.5144 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.80488\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 23s 706ms/step - loss: 0.0965 - accuracy: 0.9726 - val_loss: 3.7902 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.80488\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 23s 704ms/step - loss: 0.1233 - accuracy: 0.9604 - val_loss: 2.8321 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.80488\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 23s 702ms/step - loss: 0.0941 - accuracy: 0.9695 - val_loss: 1.1971 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.80488\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.1355 - accuracy: 0.9665 - val_loss: 1.5421 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.80488\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.1684 - accuracy: 0.9573 - val_loss: 2.5190 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.80488\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.1444 - accuracy: 0.9512 - val_loss: 1.9562 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00060: val_accuracy improved from 0.80488 to 0.81707, saving model to ./SEfold5000000600.817073.hdf5\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 23s 701ms/step - loss: 0.0846 - accuracy: 0.9634 - val_loss: 3.9450 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.81707\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.1549 - accuracy: 0.9756 - val_loss: 3.8939 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.81707\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.2347 - accuracy: 0.9726 - val_loss: 2.1634 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.81707\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 23s 699ms/step - loss: 0.2722 - accuracy: 0.9268 - val_loss: 2.1052 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.81707\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.2024 - accuracy: 0.9634 - val_loss: 2.3684 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.81707\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 23s 699ms/step - loss: 0.2532 - accuracy: 0.9482 - val_loss: 3.6327 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.81707\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.2129 - accuracy: 0.9451 - val_loss: 3.2837 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.81707\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.0890 - accuracy: 0.9665 - val_loss: 2.1626 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.81707\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.0706 - accuracy: 0.9726 - val_loss: 2.5439 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.81707\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.0551 - accuracy: 0.9726 - val_loss: 2.5430 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.81707\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.0357 - accuracy: 0.9848 - val_loss: 2.8229 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.81707\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.0343 - accuracy: 0.9909 - val_loss: 3.0645 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.81707\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.1164 - accuracy: 0.9665 - val_loss: 7.0275 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.81707\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 23s 700ms/step - loss: 0.5796 - accuracy: 0.9299 - val_loss: 1.6078 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.81707\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 23s 701ms/step - loss: 0.3322 - accuracy: 0.9360 - val_loss: 1.8728 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.81707\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 23s 702ms/step - loss: 0.4126 - accuracy: 0.9268 - val_loss: 1.7003 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.81707\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 23s 699ms/step - loss: 0.3546 - accuracy: 0.9360 - val_loss: 0.9874 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.81707\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 23s 698ms/step - loss: 0.1816 - accuracy: 0.9604 - val_loss: 1.3019 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.81707\n",
            "Epoch 00078: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0J3YG2DUZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def classifyEval(testX, testy,model):\n",
        "  # predict probabilities for test set  \n",
        "  yhat_probs = model.predict(testX, verbose=0)\n",
        "  yhat = np.max(yhat_probs,axis=1)\n",
        "  yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "  x=1\n",
        "  print(testy[:,x])\n",
        "  print(yhat_classes)\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(testy[:,x], yhat_classes)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(testy[:,x], yhat_classes)\n",
        "  print('Precision: %f' % precision)\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(testy[:,x], yhat_classes)\n",
        "  print('Recall: %f' % recall)\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(testy[:,x], yhat_classes)\n",
        "  print('F1 score: %f' % f1)\n",
        " \n",
        "  # kappa\n",
        "  kappa = cohen_kappa_score(testy[:,x], yhat_classes)\n",
        "  print('Cohens kappa: %f' % kappa)\n",
        "  # ROC AUC\n",
        "  auc = roc_auc_score(testy[:,x], yhat_probs[:,x])\n",
        "  print('ROC AUC: %f' % auc)\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(testy[:,x], yhat_classes)\n",
        "  print(matrix)\n",
        "\n",
        "  fpr_keras, tpr_keras, thresholds_keras = sklearn.metrics.roc_curve(testy[:,x], yhat_probs[:,x])\n",
        "  return fpr_keras, tpr_keras, thresholds_keras\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.plot(fpr_keras, tpr_keras, label='CV1 (area = {:.3f})'.format(auc))\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acURLMezCxA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpr_list = []\n",
        "fpr_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxXwIdDaTncb",
        "colab_type": "code",
        "outputId": "b797d95b-9c9e-454e-f6c5-84bc9c7d903b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold5.npy')\n",
        "Y_test = np.load('/content/TestLabelFold5.npy')\n",
        "model = model6()\n",
        "model.load_weights('/content/SEfold5000000600.817073.hdf5')\n",
        "fpr, tpr, _ = classifyEval(X_test_extend, Y_test, model)\n",
        "fpr_list.append(fpr)\n",
        "tpr_list.append(tpr)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
            "[0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 0 1 0 0 1 0]\n",
            "Accuracy: 0.817073\n",
            "Precision: 0.777778\n",
            "Recall: 0.350000\n",
            "F1 score: 0.482759\n",
            "Cohens kappa: 0.390486\n",
            "ROC AUC: 0.699597\n",
            "[[60  2]\n",
            " [13  7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q99ruMRwFMuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auc = [.82,.75,.71,.87,.70]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2ejE2XEHlN",
        "colab_type": "code",
        "outputId": "dbbc4a62-777a-4768-e978-7634890329d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "for i in range(5):\n",
        "  plt.plot(fpr_list[i], tpr_list[i],label='CV{} (area = {:.2f})'.format(i,auc[i]))\n",
        "  plt.legend(loc='CV{}'.format(i))\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV0'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV1'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV2'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV3'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV4'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACD81-NjF5Sg",
        "colab_type": "code",
        "outputId": "b691dd51-f7e5-4c50-a60f-598d5264b30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold5.npy')\n",
        "Y_test = np.load('/content/TestLabelFold5.npy')\n",
        "model = model2()\n",
        "model.load_weights('/content/SEfold5000000830.829268.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CavROEcGAv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Model(input=)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuERoQ6CUZZ0",
        "colab_type": "text"
      },
      "source": [
        "## Attention Model for Multi Instance Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtK_lHXrUZZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featureextractor(data):\n",
        "    conv = Conv2D(20, 5, activation = None, padding = 'same')(data)\n",
        "    conv_a = Activation('relu')(BatchNormalization()(conv))\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv_a)\n",
        "    conv1 = Conv2D(50, 5, activation = None, padding = 'same')(pool1)\n",
        "    conv1_a = Activation('relu')(BatchNormalization()(conv1))\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
        "    return pool2\n",
        "\n",
        "def featureextractorx2(data):\n",
        "    dense = Dense(500,activation='relu')(data)\n",
        "    return dense\n",
        "\n",
        "def Attention(data):\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    return dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3miMxRUZZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "l1factor=1e-2\n",
        "l2factor=0\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "prediction = MaxPool(axis=1)(dense_1)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbRDRdVcYkP",
        "colab_type": "code",
        "outputId": "fab69d65-a6a7-48f0-f900-18355494f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIHWT6X6fPv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUaTaZWirf0",
        "colab_type": "code",
        "outputId": "c155dade-a1b1-43bb-cb6f-7c13d55aadec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}\n",
        "Features = []\n",
        "\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "\n",
        "logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "dense_2 = Flatten()(logistic1)\n",
        "\n",
        "logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "dense_3 = Flatten()(logistic2)\n",
        "\n",
        "logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "dense_4 = Flatten()(logistic3)\n",
        "\n",
        "final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "prediction = MaxPool(axis=1)(final)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FwRbn0eiwib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "noises=50\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "datagen.fit(X_train_extend)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bg_PaepkBhP",
        "colab_type": "code",
        "outputId": "9567289f-268b-42bd-9eda-a9d2e29f67ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.00001),\n",
        "              metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(X_train_extend, Y_train,batch_size=2),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_valid_extend, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., epochs=100)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134/134 [==============================] - 35s 264ms/step - loss: 0.6514 - accuracy: 0.7127 - val_loss: 0.8838 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.6267 - accuracy: 0.7425 - val_loss: 0.7636 - val_accuracy: 0.4333\n",
            "Epoch 3/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.6024 - accuracy: 0.7836 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.5672 - accuracy: 0.8507 - val_loss: 0.5682 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5584 - accuracy: 0.8433 - val_loss: 0.5442 - val_accuracy: 0.8167\n",
            "Epoch 6/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.5589 - accuracy: 0.8097 - val_loss: 0.5946 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5288 - accuracy: 0.8545 - val_loss: 0.5800 - val_accuracy: 0.7667\n",
            "Epoch 8/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5151 - accuracy: 0.8582 - val_loss: 0.5377 - val_accuracy: 0.8500\n",
            "Epoch 9/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5261 - accuracy: 0.8358 - val_loss: 0.5270 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5130 - accuracy: 0.8470 - val_loss: 0.5216 - val_accuracy: 0.8500\n",
            "Epoch 11/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4840 - accuracy: 0.8731 - val_loss: 0.5267 - val_accuracy: 0.8167\n",
            "Epoch 12/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4953 - accuracy: 0.8619 - val_loss: 0.5123 - val_accuracy: 0.8167\n",
            "Epoch 13/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5000 - accuracy: 0.8433 - val_loss: 0.4984 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4907 - accuracy: 0.8507 - val_loss: 0.4877 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4814 - accuracy: 0.8694 - val_loss: 0.4697 - val_accuracy: 0.8833\n",
            "Epoch 16/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4705 - accuracy: 0.8806 - val_loss: 0.4655 - val_accuracy: 0.8833\n",
            "Epoch 17/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4634 - accuracy: 0.8843 - val_loss: 0.4711 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4425 - accuracy: 0.9030 - val_loss: 0.4611 - val_accuracy: 0.8833\n",
            "Epoch 19/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4562 - accuracy: 0.8843 - val_loss: 0.4742 - val_accuracy: 0.8500\n",
            "Epoch 20/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4466 - accuracy: 0.8881 - val_loss: 0.4702 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4490 - accuracy: 0.8843 - val_loss: 0.4764 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4406 - accuracy: 0.8993 - val_loss: 0.4772 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9067 - val_loss: 0.4578 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4504 - accuracy: 0.8881 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4518 - accuracy: 0.8806 - val_loss: 0.5093 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4471 - accuracy: 0.8769 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9104 - val_loss: 0.4399 - val_accuracy: 0.9000\n",
            "Epoch 28/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4114 - accuracy: 0.9254 - val_loss: 0.4405 - val_accuracy: 0.8833\n",
            "Epoch 29/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4215 - accuracy: 0.9067 - val_loss: 0.4320 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4280 - accuracy: 0.8918 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4124 - accuracy: 0.9179 - val_loss: 0.4489 - val_accuracy: 0.8833\n",
            "Epoch 32/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4201 - accuracy: 0.9067 - val_loss: 0.4370 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4097 - accuracy: 0.9104 - val_loss: 0.4642 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4228 - accuracy: 0.8993 - val_loss: 0.4591 - val_accuracy: 0.8500\n",
            "Epoch 35/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4080 - accuracy: 0.9179 - val_loss: 0.4386 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4136 - accuracy: 0.8993 - val_loss: 0.4663 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "134/134 [==============================] - 17s 124ms/step - loss: 0.4195 - accuracy: 0.8955 - val_loss: 0.4612 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4086 - accuracy: 0.9067 - val_loss: 0.4633 - val_accuracy: 0.8333\n",
            "Epoch 39/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4060 - accuracy: 0.9179 - val_loss: 0.4412 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4010 - accuracy: 0.9142 - val_loss: 0.4987 - val_accuracy: 0.8167\n",
            "Epoch 41/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4112 - accuracy: 0.9104 - val_loss: 0.4959 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3995 - accuracy: 0.9254 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3979 - accuracy: 0.9142 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4013 - accuracy: 0.9216 - val_loss: 0.4608 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3892 - accuracy: 0.9254 - val_loss: 0.4543 - val_accuracy: 0.8833\n",
            "Epoch 46/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3986 - accuracy: 0.9216 - val_loss: 0.4524 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3882 - accuracy: 0.9328 - val_loss: 0.4398 - val_accuracy: 0.8833\n",
            "Epoch 48/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3867 - accuracy: 0.9366 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 49/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3776 - accuracy: 0.9440 - val_loss: 0.4824 - val_accuracy: 0.8333\n",
            "Epoch 50/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3888 - accuracy: 0.9291 - val_loss: 0.4923 - val_accuracy: 0.8167\n",
            "Epoch 51/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3899 - accuracy: 0.9291 - val_loss: 0.4734 - val_accuracy: 0.8333\n",
            "Epoch 52/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4009 - accuracy: 0.9179 - val_loss: 0.4710 - val_accuracy: 0.8333\n",
            "Epoch 53/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3773 - accuracy: 0.9440 - val_loss: 0.5002 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3923 - accuracy: 0.9291 - val_loss: 0.4546 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3771 - accuracy: 0.9403 - val_loss: 0.4748 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3930 - accuracy: 0.9291 - val_loss: 0.4804 - val_accuracy: 0.8167\n",
            "Epoch 57/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3837 - accuracy: 0.9366 - val_loss: 0.4670 - val_accuracy: 0.8333\n",
            "Epoch 58/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3695 - accuracy: 0.9515 - val_loss: 0.4369 - val_accuracy: 0.8833\n",
            "Epoch 59/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3830 - accuracy: 0.9328 - val_loss: 0.4340 - val_accuracy: 0.8833\n",
            "Epoch 60/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3775 - accuracy: 0.9403 - val_loss: 0.4542 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3674 - accuracy: 0.9515 - val_loss: 0.4400 - val_accuracy: 0.8833\n",
            "Epoch 62/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3747 - accuracy: 0.9403 - val_loss: 0.4501 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3693 - accuracy: 0.9478 - val_loss: 0.4781 - val_accuracy: 0.8167\n",
            "Epoch 64/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3625 - accuracy: 0.9552 - val_loss: 0.4351 - val_accuracy: 0.8833\n",
            "Epoch 65/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3838 - accuracy: 0.9291 - val_loss: 0.4327 - val_accuracy: 0.8833\n",
            "Epoch 66/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9478 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
            "Epoch 67/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9515 - val_loss: 0.4767 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9440 - val_loss: 0.4532 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3763 - accuracy: 0.9366 - val_loss: 0.4743 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3589 - accuracy: 0.9552 - val_loss: 0.4877 - val_accuracy: 0.8167\n",
            "Epoch 71/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3560 - accuracy: 0.9664 - val_loss: 0.4616 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3646 - accuracy: 0.9552 - val_loss: 0.4886 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3543 - accuracy: 0.9627 - val_loss: 0.5030 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3562 - accuracy: 0.9627 - val_loss: 0.5077 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3523 - accuracy: 0.9664 - val_loss: 0.4707 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3576 - accuracy: 0.9590 - val_loss: 0.4801 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3603 - accuracy: 0.9552 - val_loss: 0.5191 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3669 - accuracy: 0.9440 - val_loss: 0.4686 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3591 - accuracy: 0.9515 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3708 - accuracy: 0.9403 - val_loss: 0.4465 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3495 - accuracy: 0.9701 - val_loss: 0.4657 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9552 - val_loss: 0.4355 - val_accuracy: 0.8833\n",
            "Epoch 83/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3573 - accuracy: 0.9515 - val_loss: 0.4195 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3440 - accuracy: 0.9739 - val_loss: 0.4263 - val_accuracy: 0.8833\n",
            "Epoch 85/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3515 - accuracy: 0.9664 - val_loss: 0.5191 - val_accuracy: 0.7833\n",
            "Epoch 86/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9627 - val_loss: 0.4628 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3695 - accuracy: 0.9440 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3635 - accuracy: 0.9515 - val_loss: 0.4759 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3463 - accuracy: 0.9664 - val_loss: 0.5083 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3585 - accuracy: 0.9515 - val_loss: 0.4583 - val_accuracy: 0.8333\n",
            "Epoch 91/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3517 - accuracy: 0.9590 - val_loss: 0.4490 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3496 - accuracy: 0.9701 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9590 - val_loss: 0.4643 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3498 - accuracy: 0.9590 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3492 - accuracy: 0.9627 - val_loss: 0.4788 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3438 - accuracy: 0.9664 - val_loss: 0.4847 - val_accuracy: 0.8167\n",
            "Epoch 97/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3446 - accuracy: 0.9739 - val_loss: 0.5205 - val_accuracy: 0.7833\n",
            "Epoch 98/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.3442 - accuracy: 0.9664 - val_loss: 0.4866 - val_accuracy: 0.8167\n",
            "Epoch 99/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3369 - accuracy: 0.9813 - val_loss: 0.4866 - val_accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3466 - accuracy: 0.9701 - val_loss: 0.4513 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb84798d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeGlE_Oz-RjS",
        "colab_type": "code",
        "outputId": "f36f1173-8550-4982-e5fe-20d4af123438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = model5()\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 64, 64, 1)    257         conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 32, 32, 1)    513         conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 1)    1025        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 1)      2049        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_53 (Flatten)            (None, 4096)         0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_54 (Flatten)            (None, 1024)         0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_55 (Flatten)            (None, 256)          0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_56 (Flatten)            (None, 64)           0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 5440)         0           flatten_53[0][0]                 \n",
            "                                                                 flatten_54[0][0]                 \n",
            "                                                                 flatten_55[0][0]                 \n",
            "                                                                 flatten_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_14 (MaxPool)           (None, 2)            0           concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 2)            0           max_pool_14[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 23,591,556\n",
            "Trainable params: 23,538,436\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfcXjhymHnt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
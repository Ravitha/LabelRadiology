{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravitha/LabelRadiology/blob/master/MIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arW_6_PrkIlZ",
        "colab_type": "text"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthF7sh_UZX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dicom # some machines not install pydicom\n",
        "import scipy.misc\n",
        "import numpy as np \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle as cPickle\n",
        "#import matplotlib\n",
        "#import matplotlib.pyplot as plt \n",
        "from skimage.filters import threshold_otsu\n",
        "import os\n",
        "from os.path import join as join\n",
        "import csv\n",
        "import scipy.ndimage\n",
        "#import pydicom as dicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82DTupaeUZYI",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evR_WImPaUN0",
        "colab_type": "code",
        "outputId": "b918d054-1e5e-4745-8ec1-64d79aad0c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcQkq4ShkC-6",
        "colab_type": "text"
      },
      "source": [
        "# Compose Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzRMqmRi0fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/drive/My Drive/CO-PO/INBreast'\n",
        "TrDataset = np.load(join(path,'Train.npy'))\n",
        "TrLabel = np.load(join(path,'TrainL.npy'))\n",
        "ValDataset= np.load(join(path,'valid.npy'))\n",
        "ValLabel = np.load(join(path,'validL.npy'))\n",
        "TDataset = np.load(join(path,'Test.npy'))\n",
        "TLabel = np.load(join(path,'TestL.npy'))\n",
        "\n",
        "Dataset = np.concatenate((TrDataset,ValDataset,TDataset),axis = 0)\n",
        "Labels = np.concatenate((TrLabel,ValLabel,TLabel),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRB7_brfjY3g",
        "colab_type": "code",
        "outputId": "9f179a6a-fb8c-46f0-c06e-d0bce30c2e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "np.save('Dataset.npy', Dataset)\n",
        "np.save('Label.npy', Labels)\n",
        "\n",
        "np.unique(Labels, return_counts=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256)\n",
            "(410,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crX86A3FUZZI",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXWpwOAkOVK",
        "colab_type": "code",
        "outputId": "8e7077f2-b0b4-4cca-db39-b44abbf3ed70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "nb_classes = 2\n",
        "Dataset = Dataset.reshape((410,256,256,1))\n",
        "Dataset_extend = np.zeros((Dataset.shape[0],256, 256,3))\n",
        "for i in range(Dataset.shape[0]):\n",
        "    rex = np.resize(Dataset[i,:,:,:], (256, 256))\n",
        "    Dataset_extend[i,:,:,0] = rex\n",
        "    Dataset_extend[i,:,:,1] = rex\n",
        "    Dataset_extend[i,:,:,2] = rex\n",
        "Dataset = Dataset_extend\n",
        "Labels = np_utils.to_categorical(Labels, nb_classes)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqsOUAXIlSL1",
        "colab_type": "text"
      },
      "source": [
        "# Print Data Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNitNFUBlQXT",
        "colab_type": "code",
        "outputId": "39ffa35a-8d25-409e-93fd-ffaf7426f757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256, 3)\n",
            "(410, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_WtGDWTeiI",
        "colab_type": "code",
        "outputId": "92d5db44-cdf3-47f3-9537-121012739e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.unique(Labels[:,1],return_counts = True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.], dtype=float32), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_SNbuEjS9SP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label Assignment\n",
        "Labels_Assign = np.zeros((410,64))\n",
        "for i in range(0,410):\n",
        "  if(Labels[i,0] == 1):\n",
        "    Labels_Assign[i,:-2] = 1\n",
        "  else:\n",
        "    Labels_Assign[i,62:64] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaqLtUTiUXpJ",
        "colab_type": "code",
        "outputId": "9030b0a8-2517-4c82-a9a8-6e9f2b3c65c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(Labels_Assign[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4qn6IkmSC1",
        "colab_type": "text"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKoXcmbmUCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model1():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLHg7YfSOZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "def model2():\n",
        "\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  \n",
        "  out1 = Lambda(lambda image: tf.image.resize(image, (4, 4)))(model.output)\n",
        "  out2 = Lambda(lambda image: tf.image.resize(image, (2, 2)))(model.output)\n",
        "  out3 = Lambda(lambda image: tf.image.resize(image, (6, 6)))(model.output)\n",
        "\n",
        "  SL1 = Conv2D(1, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "  SL2 = BatchNormalization()\n",
        "  SL3 = Conv2D(1, 1, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "\n",
        "  resize1 = Flatten()(SL3(SL2(SL1(out1))))\n",
        "  resize2 = Flatten()(SL3(SL2(SL1(out2))))\n",
        "  resize3 = Flatten()(SL3(SL2(SL1(out3))))\n",
        "\n",
        "  M1 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize1)\n",
        "  M2 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize2)\n",
        "  M3 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize3)\n",
        "\n",
        "  added10 = concatenate([M1,M2,M3], axis=1)\n",
        "  final = Dense(2, activation='softmax')(added10)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSm1LGMzKbeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class KPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(KPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],64))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output = tf.sort(x)\n",
        "        output=K.concatenate([1-output[:,:-2], output[:,62:64]])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 64)\n",
        "\n",
        "def model3():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = KPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN0ILfCIiRLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        max_val = K.max(x, axis=-1,keepdims=True)\n",
        "        self.add_loss(l1(1e-5)(x))\n",
        "        output=K.concatenate([1-max_val,max_val])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model4():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rym-sZ_dxvp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "def model5():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "\n",
        "  logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "  dense_2 = Flatten()(logistic1)\n",
        "\n",
        "  logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "  dense_3 = Flatten()(logistic2)\n",
        "\n",
        "  logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "  dense_4 = Flatten()(logistic3)\n",
        "\n",
        "  final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "  prediction = MaxPool(axis=1)(final)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K922EQst61r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model6():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  dense = Flatten()(model.output)\n",
        "  final = Dense(2, activation='softmax')(dense)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOIFdxJVvqWS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8bc75cc7-acca-40d3-eb99-e718d41766f5"
      },
      "source": [
        "model = model6()\n",
        "print(model.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 9s 0us/step\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 131072)       0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            262146      flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 23,849,858\n",
            "Trainable params: 23,796,738\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssCxcR7lutw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFdleZCluLO",
        "colab_type": "code",
        "outputId": "9ca783a8-1086-477b-ca49-e3d14a172db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Necessary Imports\n",
        "import numpy as np \n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "print('[INFO:Applying CV]')\n",
        "skf = KFold(n_splits=5)\n",
        "skf.get_n_splits(Dataset)\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(Dataset):\n",
        "    print(['FOLD:'], fold)\n",
        "    # Construct data from indexes\n",
        "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
        "    y_train, y_test = Labels[train_index], Labels[test_index]\n",
        "    \n",
        "    # Model Construction\n",
        "    model = model6()\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "    datagen.fit(X_train)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "    Stopping_Criterion = EarlyStopping(patience=50, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint('./SEfold'+str(fold)+'{epoch:08d}{val_accuracy:05f}.hdf5', monitor='val_accuracy',verbose=1,save_best_only=True)\n",
        "    model.fit_generator(datagen.flow(X_train, y_train,batch_size=10),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_test , y_test ),callbacks=[model_checkpoint, Stopping_Criterion])\n",
        "    datafilename = 'TestFold'+str(fold)+'.npy'\n",
        "    labelfilename = 'TestLabelFold'+str(fold)+'.npy'\n",
        "    np.save(datafilename,X_test)\n",
        "    np.save(labelfilename,y_test)\n",
        "    fold+=1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO:Applying CV]\n",
            "['FOLD:'] 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 38s 1s/step - loss: 2.1054 - accuracy: 0.6555 - val_loss: 0.6887 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.75610, saving model to ./SEfold1000000010.756098.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 1.5802 - accuracy: 0.6951 - val_loss: 0.6521 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.75610\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 1.6824 - accuracy: 0.7073 - val_loss: 2.7659 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.75610\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 1.4032 - accuracy: 0.7378 - val_loss: 3.9461 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 1.8245 - accuracy: 0.7256 - val_loss: 4.7656 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 1.6271 - accuracy: 0.7500 - val_loss: 25.6804 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 1.2781 - accuracy: 0.7927 - val_loss: 14.4094 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.9993 - accuracy: 0.7652 - val_loss: 1.4114 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.9828 - accuracy: 0.7774 - val_loss: 1.6159 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.7344 - accuracy: 0.8018 - val_loss: 1.1239 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.6602 - accuracy: 0.8049 - val_loss: 0.5461 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.6315 - accuracy: 0.8384 - val_loss: 0.6132 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.6776 - accuracy: 0.8323 - val_loss: 0.5870 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.3244 - accuracy: 0.8994 - val_loss: 0.5535 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.4100 - accuracy: 0.8750 - val_loss: 2.3932 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.4397 - accuracy: 0.8841 - val_loss: 1.7857 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.3786 - accuracy: 0.8811 - val_loss: 0.8775 - val_accuracy: 0.3902\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.3068 - accuracy: 0.8902 - val_loss: 0.5968 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.3197 - accuracy: 0.8994 - val_loss: 0.8113 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.4004 - accuracy: 0.8933 - val_loss: 1.1791 - val_accuracy: 0.4878\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.3679 - accuracy: 0.8811 - val_loss: 0.8989 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.2731 - accuracy: 0.9177 - val_loss: 0.9164 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.3507 - accuracy: 0.8872 - val_loss: 1.1106 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.2585 - accuracy: 0.8933 - val_loss: 1.1412 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.2626 - accuracy: 0.9268 - val_loss: 1.1555 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.3283 - accuracy: 0.9055 - val_loss: 0.9939 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.4035 - accuracy: 0.8872 - val_loss: 1.0328 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 15s 464ms/step - loss: 0.3843 - accuracy: 0.8780 - val_loss: 0.9268 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.2586 - accuracy: 0.8994 - val_loss: 1.0686 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.3275 - accuracy: 0.9085 - val_loss: 0.9233 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 15s 460ms/step - loss: 0.2394 - accuracy: 0.9238 - val_loss: 0.8431 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.2721 - accuracy: 0.9116 - val_loss: 0.8909 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 15s 465ms/step - loss: 0.3478 - accuracy: 0.9207 - val_loss: 0.7826 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.3685 - accuracy: 0.8994 - val_loss: 0.9488 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.75610 to 0.81707, saving model to ./SEfold1000000340.817073.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.2329 - accuracy: 0.9146 - val_loss: 1.0176 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.81707\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.1014 - accuracy: 0.9604 - val_loss: 0.9393 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.81707\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.1059 - accuracy: 0.9512 - val_loss: 1.0459 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.81707\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.1125 - accuracy: 0.9695 - val_loss: 1.0582 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.81707\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.1690 - accuracy: 0.9451 - val_loss: 0.8335 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.81707\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.1790 - accuracy: 0.9482 - val_loss: 1.0321 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.81707\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.1550 - accuracy: 0.9512 - val_loss: 0.6924 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.81707 to 0.84146, saving model to ./SEfold1000000410.841463.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.1612 - accuracy: 0.9451 - val_loss: 0.8952 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.84146\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 15s 460ms/step - loss: 0.1432 - accuracy: 0.9482 - val_loss: 0.8987 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.84146\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.2225 - accuracy: 0.9299 - val_loss: 0.6632 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.84146\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.1341 - accuracy: 0.9604 - val_loss: 0.5498 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.84146\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.1768 - accuracy: 0.9451 - val_loss: 0.7139 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.84146\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.2582 - accuracy: 0.9451 - val_loss: 1.4005 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.84146\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.2234 - accuracy: 0.9451 - val_loss: 0.9925 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.84146\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.1504 - accuracy: 0.9482 - val_loss: 1.1118 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.84146\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.1463 - accuracy: 0.9543 - val_loss: 0.7404 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.84146\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 15s 462ms/step - loss: 0.1580 - accuracy: 0.9604 - val_loss: 0.8668 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.84146\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.2040 - accuracy: 0.9421 - val_loss: 1.3518 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00052: val_accuracy improved from 0.84146 to 0.85366, saving model to ./SEfold1000000520.853659.hdf5\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.1959 - accuracy: 0.9512 - val_loss: 1.6512 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.85366\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 15s 461ms/step - loss: 0.2585 - accuracy: 0.9482 - val_loss: 1.5266 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.85366\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 15s 463ms/step - loss: 0.1691 - accuracy: 0.9512 - val_loss: 1.3117 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.85366\n",
            "Epoch 56/100\n",
            "26/33 [======================>.......] - ETA: 3s - loss: 0.1153 - accuracy: 0.9690"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0J3YG2DUZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def classifyEval(testX, testy,model):\n",
        "  # predict probabilities for test set  \n",
        "  yhat_probs = model.predict(testX, verbose=0)\n",
        "  yhat = np.max(yhat_probs,axis=1)\n",
        "  yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "  x=1\n",
        "  print(testy[:,x])\n",
        "  print(yhat_classes)\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(testy[:,x], yhat_classes)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(testy[:,x], yhat_classes)\n",
        "  print('Precision: %f' % precision)\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(testy[:,x], yhat_classes)\n",
        "  print('Recall: %f' % recall)\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(testy[:,x], yhat_classes)\n",
        "  print('F1 score: %f' % f1)\n",
        " \n",
        "  # kappa\n",
        "  kappa = cohen_kappa_score(testy[:,x], yhat_classes)\n",
        "  print('Cohens kappa: %f' % kappa)\n",
        "  # ROC AUC\n",
        "  auc = roc_auc_score(testy[:,x], yhat_probs[:,x])\n",
        "  print('ROC AUC: %f' % auc)\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(testy[:,x], yhat_classes)\n",
        "  print(matrix)\n",
        "\n",
        "  fpr_keras, tpr_keras, thresholds_keras = sklearn.metrics.roc_curve(testy[:,x], yhat_probs[:,x])\n",
        "  return fpr_keras, tpr_keras, thresholds_keras\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.plot(fpr_keras, tpr_keras, label='CV1 (area = {:.3f})'.format(auc))\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acURLMezCxA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpr_list = []\n",
        "fpr_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxXwIdDaTncb",
        "colab_type": "code",
        "outputId": "ce8e2c6d-dd5a-434c-bc39-0541c1d6872c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold5.npy')\n",
        "Y_test = np.load('/content/TestLabelFold5.npy')\n",
        "model = model5()\n",
        "model.load_weights('/content/SEfold5000000740.853659.hdf5')\n",
        "fpr, tpr, _ = classifyEval(X_test_extend, Y_test, model)\n",
        "fpr_list.append(fpr)\n",
        "tpr_list.append(tpr)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
            "[0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0\n",
            " 1 0 0 0 0 0 1 1]\n",
            "Accuracy: 0.853659\n",
            "Precision: 0.900000\n",
            "Recall: 0.450000\n",
            "F1 score: 0.600000\n",
            "Cohens kappa: 0.522330\n",
            "ROC AUC: 0.727419\n",
            "[[61  1]\n",
            " [11  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q99ruMRwFMuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auc = [.85,.89,.84,.93,.73]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2ejE2XEHlN",
        "colab_type": "code",
        "outputId": "96d0d210-55c7-4f6a-95a1-90c7f40a984c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "for i in range(5):\n",
        "  plt.plot(fpr_list[i], tpr_list[i],label='CV{} (area = {:.2f})'.format(i,auc[i]))\n",
        "  plt.legend(loc='CV{}'.format(i))\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV0'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV1'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV2'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV3'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV4'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACD81-NjF5Sg",
        "colab_type": "code",
        "outputId": "b691dd51-f7e5-4c50-a60f-598d5264b30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold5.npy')\n",
        "Y_test = np.load('/content/TestLabelFold5.npy')\n",
        "model = model2()\n",
        "model.load_weights('/content/SEfold5000000830.829268.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CavROEcGAv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Model(input=)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuERoQ6CUZZ0",
        "colab_type": "text"
      },
      "source": [
        "## Attention Model for Multi Instance Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtK_lHXrUZZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featureextractor(data):\n",
        "    conv = Conv2D(20, 5, activation = None, padding = 'same')(data)\n",
        "    conv_a = Activation('relu')(BatchNormalization()(conv))\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv_a)\n",
        "    conv1 = Conv2D(50, 5, activation = None, padding = 'same')(pool1)\n",
        "    conv1_a = Activation('relu')(BatchNormalization()(conv1))\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
        "    return pool2\n",
        "\n",
        "def featureextractorx2(data):\n",
        "    dense = Dense(500,activation='relu')(data)\n",
        "    return dense\n",
        "\n",
        "def Attention(data):\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    return dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3miMxRUZZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "l1factor=1e-2\n",
        "l2factor=0\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "prediction = MaxPool(axis=1)(dense_1)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbRDRdVcYkP",
        "colab_type": "code",
        "outputId": "fab69d65-a6a7-48f0-f900-18355494f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIHWT6X6fPv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUaTaZWirf0",
        "colab_type": "code",
        "outputId": "c155dade-a1b1-43bb-cb6f-7c13d55aadec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}\n",
        "Features = []\n",
        "\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "\n",
        "logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "dense_2 = Flatten()(logistic1)\n",
        "\n",
        "logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "dense_3 = Flatten()(logistic2)\n",
        "\n",
        "logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "dense_4 = Flatten()(logistic3)\n",
        "\n",
        "final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "prediction = MaxPool(axis=1)(final)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FwRbn0eiwib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "noises=50\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "datagen.fit(X_train_extend)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bg_PaepkBhP",
        "colab_type": "code",
        "outputId": "9567289f-268b-42bd-9eda-a9d2e29f67ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.00001),\n",
        "              metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(X_train_extend, Y_train,batch_size=2),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_valid_extend, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., epochs=100)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134/134 [==============================] - 35s 264ms/step - loss: 0.6514 - accuracy: 0.7127 - val_loss: 0.8838 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.6267 - accuracy: 0.7425 - val_loss: 0.7636 - val_accuracy: 0.4333\n",
            "Epoch 3/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.6024 - accuracy: 0.7836 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.5672 - accuracy: 0.8507 - val_loss: 0.5682 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5584 - accuracy: 0.8433 - val_loss: 0.5442 - val_accuracy: 0.8167\n",
            "Epoch 6/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.5589 - accuracy: 0.8097 - val_loss: 0.5946 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5288 - accuracy: 0.8545 - val_loss: 0.5800 - val_accuracy: 0.7667\n",
            "Epoch 8/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5151 - accuracy: 0.8582 - val_loss: 0.5377 - val_accuracy: 0.8500\n",
            "Epoch 9/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5261 - accuracy: 0.8358 - val_loss: 0.5270 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5130 - accuracy: 0.8470 - val_loss: 0.5216 - val_accuracy: 0.8500\n",
            "Epoch 11/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4840 - accuracy: 0.8731 - val_loss: 0.5267 - val_accuracy: 0.8167\n",
            "Epoch 12/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4953 - accuracy: 0.8619 - val_loss: 0.5123 - val_accuracy: 0.8167\n",
            "Epoch 13/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5000 - accuracy: 0.8433 - val_loss: 0.4984 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4907 - accuracy: 0.8507 - val_loss: 0.4877 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4814 - accuracy: 0.8694 - val_loss: 0.4697 - val_accuracy: 0.8833\n",
            "Epoch 16/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4705 - accuracy: 0.8806 - val_loss: 0.4655 - val_accuracy: 0.8833\n",
            "Epoch 17/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4634 - accuracy: 0.8843 - val_loss: 0.4711 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4425 - accuracy: 0.9030 - val_loss: 0.4611 - val_accuracy: 0.8833\n",
            "Epoch 19/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4562 - accuracy: 0.8843 - val_loss: 0.4742 - val_accuracy: 0.8500\n",
            "Epoch 20/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4466 - accuracy: 0.8881 - val_loss: 0.4702 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4490 - accuracy: 0.8843 - val_loss: 0.4764 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4406 - accuracy: 0.8993 - val_loss: 0.4772 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9067 - val_loss: 0.4578 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4504 - accuracy: 0.8881 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4518 - accuracy: 0.8806 - val_loss: 0.5093 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4471 - accuracy: 0.8769 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9104 - val_loss: 0.4399 - val_accuracy: 0.9000\n",
            "Epoch 28/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4114 - accuracy: 0.9254 - val_loss: 0.4405 - val_accuracy: 0.8833\n",
            "Epoch 29/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4215 - accuracy: 0.9067 - val_loss: 0.4320 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4280 - accuracy: 0.8918 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4124 - accuracy: 0.9179 - val_loss: 0.4489 - val_accuracy: 0.8833\n",
            "Epoch 32/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4201 - accuracy: 0.9067 - val_loss: 0.4370 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4097 - accuracy: 0.9104 - val_loss: 0.4642 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4228 - accuracy: 0.8993 - val_loss: 0.4591 - val_accuracy: 0.8500\n",
            "Epoch 35/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4080 - accuracy: 0.9179 - val_loss: 0.4386 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4136 - accuracy: 0.8993 - val_loss: 0.4663 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "134/134 [==============================] - 17s 124ms/step - loss: 0.4195 - accuracy: 0.8955 - val_loss: 0.4612 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4086 - accuracy: 0.9067 - val_loss: 0.4633 - val_accuracy: 0.8333\n",
            "Epoch 39/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4060 - accuracy: 0.9179 - val_loss: 0.4412 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4010 - accuracy: 0.9142 - val_loss: 0.4987 - val_accuracy: 0.8167\n",
            "Epoch 41/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4112 - accuracy: 0.9104 - val_loss: 0.4959 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3995 - accuracy: 0.9254 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3979 - accuracy: 0.9142 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4013 - accuracy: 0.9216 - val_loss: 0.4608 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3892 - accuracy: 0.9254 - val_loss: 0.4543 - val_accuracy: 0.8833\n",
            "Epoch 46/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3986 - accuracy: 0.9216 - val_loss: 0.4524 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3882 - accuracy: 0.9328 - val_loss: 0.4398 - val_accuracy: 0.8833\n",
            "Epoch 48/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3867 - accuracy: 0.9366 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 49/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3776 - accuracy: 0.9440 - val_loss: 0.4824 - val_accuracy: 0.8333\n",
            "Epoch 50/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3888 - accuracy: 0.9291 - val_loss: 0.4923 - val_accuracy: 0.8167\n",
            "Epoch 51/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3899 - accuracy: 0.9291 - val_loss: 0.4734 - val_accuracy: 0.8333\n",
            "Epoch 52/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4009 - accuracy: 0.9179 - val_loss: 0.4710 - val_accuracy: 0.8333\n",
            "Epoch 53/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3773 - accuracy: 0.9440 - val_loss: 0.5002 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3923 - accuracy: 0.9291 - val_loss: 0.4546 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3771 - accuracy: 0.9403 - val_loss: 0.4748 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3930 - accuracy: 0.9291 - val_loss: 0.4804 - val_accuracy: 0.8167\n",
            "Epoch 57/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3837 - accuracy: 0.9366 - val_loss: 0.4670 - val_accuracy: 0.8333\n",
            "Epoch 58/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3695 - accuracy: 0.9515 - val_loss: 0.4369 - val_accuracy: 0.8833\n",
            "Epoch 59/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3830 - accuracy: 0.9328 - val_loss: 0.4340 - val_accuracy: 0.8833\n",
            "Epoch 60/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3775 - accuracy: 0.9403 - val_loss: 0.4542 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3674 - accuracy: 0.9515 - val_loss: 0.4400 - val_accuracy: 0.8833\n",
            "Epoch 62/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3747 - accuracy: 0.9403 - val_loss: 0.4501 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3693 - accuracy: 0.9478 - val_loss: 0.4781 - val_accuracy: 0.8167\n",
            "Epoch 64/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3625 - accuracy: 0.9552 - val_loss: 0.4351 - val_accuracy: 0.8833\n",
            "Epoch 65/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3838 - accuracy: 0.9291 - val_loss: 0.4327 - val_accuracy: 0.8833\n",
            "Epoch 66/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9478 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
            "Epoch 67/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9515 - val_loss: 0.4767 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9440 - val_loss: 0.4532 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3763 - accuracy: 0.9366 - val_loss: 0.4743 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3589 - accuracy: 0.9552 - val_loss: 0.4877 - val_accuracy: 0.8167\n",
            "Epoch 71/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3560 - accuracy: 0.9664 - val_loss: 0.4616 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3646 - accuracy: 0.9552 - val_loss: 0.4886 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3543 - accuracy: 0.9627 - val_loss: 0.5030 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3562 - accuracy: 0.9627 - val_loss: 0.5077 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3523 - accuracy: 0.9664 - val_loss: 0.4707 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3576 - accuracy: 0.9590 - val_loss: 0.4801 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3603 - accuracy: 0.9552 - val_loss: 0.5191 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3669 - accuracy: 0.9440 - val_loss: 0.4686 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3591 - accuracy: 0.9515 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3708 - accuracy: 0.9403 - val_loss: 0.4465 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3495 - accuracy: 0.9701 - val_loss: 0.4657 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9552 - val_loss: 0.4355 - val_accuracy: 0.8833\n",
            "Epoch 83/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3573 - accuracy: 0.9515 - val_loss: 0.4195 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3440 - accuracy: 0.9739 - val_loss: 0.4263 - val_accuracy: 0.8833\n",
            "Epoch 85/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3515 - accuracy: 0.9664 - val_loss: 0.5191 - val_accuracy: 0.7833\n",
            "Epoch 86/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9627 - val_loss: 0.4628 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3695 - accuracy: 0.9440 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3635 - accuracy: 0.9515 - val_loss: 0.4759 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3463 - accuracy: 0.9664 - val_loss: 0.5083 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3585 - accuracy: 0.9515 - val_loss: 0.4583 - val_accuracy: 0.8333\n",
            "Epoch 91/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3517 - accuracy: 0.9590 - val_loss: 0.4490 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3496 - accuracy: 0.9701 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9590 - val_loss: 0.4643 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3498 - accuracy: 0.9590 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3492 - accuracy: 0.9627 - val_loss: 0.4788 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3438 - accuracy: 0.9664 - val_loss: 0.4847 - val_accuracy: 0.8167\n",
            "Epoch 97/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3446 - accuracy: 0.9739 - val_loss: 0.5205 - val_accuracy: 0.7833\n",
            "Epoch 98/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.3442 - accuracy: 0.9664 - val_loss: 0.4866 - val_accuracy: 0.8167\n",
            "Epoch 99/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3369 - accuracy: 0.9813 - val_loss: 0.4866 - val_accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3466 - accuracy: 0.9701 - val_loss: 0.4513 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb84798d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeGlE_Oz-RjS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f36f1173-8550-4982-e5fe-20d4af123438"
      },
      "source": [
        "model = model5()\n",
        "print(model.summary())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 64, 64, 1)    257         conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 32, 32, 1)    513         conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 1)    1025        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 1)      2049        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_53 (Flatten)            (None, 4096)         0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_54 (Flatten)            (None, 1024)         0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_55 (Flatten)            (None, 256)          0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_56 (Flatten)            (None, 64)           0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 5440)         0           flatten_53[0][0]                 \n",
            "                                                                 flatten_54[0][0]                 \n",
            "                                                                 flatten_55[0][0]                 \n",
            "                                                                 flatten_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_14 (MaxPool)           (None, 2)            0           concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 2)            0           max_pool_14[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 23,591,556\n",
            "Trainable params: 23,538,436\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfcXjhymHnt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
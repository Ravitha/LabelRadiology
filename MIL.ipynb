{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravitha/LabelRadiology/blob/master/MIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arW_6_PrkIlZ",
        "colab_type": "text"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthF7sh_UZX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dicom # some machines not install pydicom\n",
        "import scipy.misc\n",
        "import numpy as np \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle as cPickle\n",
        "#import matplotlib\n",
        "#import matplotlib.pyplot as plt \n",
        "from skimage.filters import threshold_otsu\n",
        "import os\n",
        "from os.path import join as join\n",
        "import csv\n",
        "import scipy.ndimage\n",
        "#import pydicom as dicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82DTupaeUZYI",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evR_WImPaUN0",
        "colab_type": "code",
        "outputId": "3199e4c7-d037-458e-c0f5-8170bbbf2126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcQkq4ShkC-6",
        "colab_type": "text"
      },
      "source": [
        "# Compose Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzRMqmRi0fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/drive/My Drive/CO-PO/INBreast'\n",
        "TrDataset = np.load(join(path,'Train.npy'))\n",
        "TrLabel = np.load(join(path,'TrainL.npy'))\n",
        "ValDataset= np.load(join(path,'valid.npy'))\n",
        "ValLabel = np.load(join(path,'validL.npy'))\n",
        "TDataset = np.load(join(path,'Test.npy'))\n",
        "TLabel = np.load(join(path,'TestL.npy'))\n",
        "\n",
        "Dataset = np.concatenate((TrDataset,ValDataset,TDataset),axis = 0)\n",
        "Labels = np.concatenate((TrLabel,ValLabel,TLabel),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRB7_brfjY3g",
        "colab_type": "code",
        "outputId": "9f179a6a-fb8c-46f0-c06e-d0bce30c2e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "np.save('Dataset.npy', Dataset)\n",
        "np.save('Label.npy', Labels)\n",
        "\n",
        "np.unique(Labels, return_counts=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256)\n",
            "(410,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crX86A3FUZZI",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXWpwOAkOVK",
        "colab_type": "code",
        "outputId": "92a543de-cf96-41ee-b9cf-2a9cf56f3489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "nb_classes = 2\n",
        "Dataset = Dataset.reshape((410,256,256,1))\n",
        "Dataset_extend = np.zeros((Dataset.shape[0],256, 256,3))\n",
        "for i in range(Dataset.shape[0]):\n",
        "    rex = np.resize(Dataset[i,:,:,:], (256, 256))\n",
        "    Dataset_extend[i,:,:,0] = rex\n",
        "    Dataset_extend[i,:,:,1] = rex\n",
        "    Dataset_extend[i,:,:,2] = rex\n",
        "Dataset = Dataset_extend\n",
        "Labels = np_utils.to_categorical(Labels, nb_classes)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqsOUAXIlSL1",
        "colab_type": "text"
      },
      "source": [
        "# Print Data Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNitNFUBlQXT",
        "colab_type": "code",
        "outputId": "02afd032-43b5-40d1-cbf0-0ed0940becd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256, 3)\n",
            "(410, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_WtGDWTeiI",
        "colab_type": "code",
        "outputId": "92d5db44-cdf3-47f3-9537-121012739e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.unique(Labels[:,1],return_counts = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.], dtype=float32), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4qn6IkmSC1",
        "colab_type": "text"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKoXcmbmUCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model1():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_PMVBLi_l1q",
        "colab_type": "text"
      },
      "source": [
        "## Using K Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSm1LGMzKbeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class KPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(KPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],64))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output = tf.sort(x)\n",
        "        output=K.concatenate([1-output[:,:-2], output[:,62:64]])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 64)\n",
        "\n",
        "def model3():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = KPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXNmazY__4v4",
        "colab_type": "text"
      },
      "source": [
        "## Use Sparsity Constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN0ILfCIiRLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        max_val = K.max(x, axis=-1,keepdims=True)\n",
        "        self.add_loss(l1(1e-5)(x))\n",
        "        output=K.concatenate([1-max_val,max_val])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model4():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9SHG1mD_a08",
        "colab_type": "text"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLHg7YfSOZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "def model2():\n",
        "\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  \n",
        "  out1 = Lambda(lambda image: tf.image.resize(image, (4, 4)))(model.output)\n",
        "  out2 = Lambda(lambda image: tf.image.resize(image, (2, 2)))(model.output)\n",
        "  out3 = Lambda(lambda image: tf.image.resize(image, (6, 6)))(model.output)\n",
        "\n",
        "  SL1 = Conv2D(1, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "  SL2 = BatchNormalization()\n",
        "  SL3 = Conv2D(1, 1, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "\n",
        "  resize1 = Flatten()(SL3(SL2(SL1(out1))))\n",
        "  resize2 = Flatten()(SL3(SL2(SL1(out2))))\n",
        "  resize3 = Flatten()(SL3(SL2(SL1(out3))))\n",
        "\n",
        "  M1 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize1)\n",
        "  M2 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize2)\n",
        "  M3 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize3)\n",
        "\n",
        "  added10 = concatenate([M1,M2,M3], axis=1)\n",
        "  final = Dense(2, activation='softmax')(added10)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIfSI5K___Z6",
        "colab_type": "text"
      },
      "source": [
        "# Model 3 (ResNet Multiscale)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rym-sZ_dxvp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "def model5():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "\n",
        "  logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "  dense_2 = Flatten()(logistic1)\n",
        "\n",
        "  logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "  dense_3 = Flatten()(logistic2)\n",
        "\n",
        "  logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "  dense_4 = Flatten()(logistic3)\n",
        "\n",
        "  final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "  prediction = MaxPool(axis=1)(final)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTMerT0GAKAF",
        "colab_type": "text"
      },
      "source": [
        "## ResNet 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K922EQst61r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model6():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  dense = Flatten()(model.output)\n",
        "  final = Dense(2, activation='softmax')(dense)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7mepqnFBBQl",
        "colab_type": "text"
      },
      "source": [
        "# Model 4 (Attention with ResNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-O8n7VBBJor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten,UpSampling2D, Multiply\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "def model6():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  G = model.get_layer('conv5_block3_out').output\n",
        "  G = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(G)\n",
        "\n",
        "  L1 = model.get_layer('conv4_block6_out').output\n",
        "  L1 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L1)\n",
        "  G_modified = UpSampling2D(size = (2,2), interpolation='bilinear')(G)\n",
        "  L1_modified = Multiply()([L1,G_modified])\n",
        "  L1_modified = Activation('softmax')(L1_modified)\n",
        "  L1_modified = Multiply()([L1_modified,G_modified])\n",
        "  L1_modified = Flatten()(L1_modified)\n",
        "  L2 = model.get_layer('conv3_block4_out').output\n",
        "  L2 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L2)\n",
        "  G_modified = UpSampling2D(size = (4,4), interpolation='bilinear')(G)\n",
        "  L2_modified = Multiply()([L2,G_modified])\n",
        "  L2_modified = Activation('softmax')(L2_modified)\n",
        "  L2_modified = Multiply()([L2_modified,G_modified])\n",
        "  L2_modified = Flatten()(L2_modified)\n",
        "  L3 = model.get_layer('conv2_block3_out').output\n",
        "  L3 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L3)\n",
        "  G_modified = UpSampling2D(size = (8,8), interpolation='bilinear')(G)\n",
        "  L3_modified = Multiply()([L3,G_modified])\n",
        "  L3_modified = Activation('softmax')(L3_modified)\n",
        "  L3_modified = Multiply()([L3_modified,G_modified])\n",
        "  L3_modified = Flatten()(L3_modified)\n",
        "  final = concatenate([L1_modified, L2_modified,L3_modified])\n",
        "  prediction = MaxPool(axis=1)(final)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssCxcR7lutw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFdleZCluLO",
        "colab_type": "code",
        "outputId": "c25ed1b8-0ceb-4011-b0bb-be8261524950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Necessary Imports\n",
        "import numpy as np \n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "print('[INFO:Applying CV]')\n",
        "skf = KFold(n_splits=5)\n",
        "skf.get_n_splits(Dataset)\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(Dataset):\n",
        "    print(['FOLD:'], fold)\n",
        "    # Construct data from indexes\n",
        "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
        "    y_train, y_test = Labels[train_index], Labels[test_index]\n",
        "    \n",
        "    # Model Construction\n",
        "    model = model6()\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "    datagen.fit(X_train)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "    Stopping_Criterion = EarlyStopping(patience=50, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint('./SEfold'+str(fold)+'{epoch:08d}{val_accuracy:05f}.hdf5', monitor='val_accuracy',verbose=1,save_best_only=True)\n",
        "    model.fit_generator(datagen.flow(X_train, y_train,batch_size=10),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_test , y_test ),callbacks=[model_checkpoint, Stopping_Criterion])\n",
        "    datafilename = 'TestFold'+str(fold)+'.npy'\n",
        "    labelfilename = 'TestLabelFold'+str(fold)+'.npy'\n",
        "    np.save(datafilename,X_test)\n",
        "    np.save(labelfilename,y_test)\n",
        "    fold+=1\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO:Applying CV]\n",
            "['FOLD:'] 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33/33 [==============================] - 22s 660ms/step - loss: 1.0003 - accuracy: 0.2713 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold1000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.8683 - accuracy: 0.4329 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.6558 - accuracy: 0.6707 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.24390\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.6893 - accuracy: 0.6220 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.24390\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6326 - accuracy: 0.6829 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.24390\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.6662 - accuracy: 0.6402 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.24390\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6156 - accuracy: 0.7012 - val_loss: 0.5752 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.24390 to 0.75610, saving model to ./SEfold1000000070.756098.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.6269 - accuracy: 0.6799 - val_loss: 0.5912 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.5956 - accuracy: 0.7134 - val_loss: 0.5782 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5892 - accuracy: 0.7287 - val_loss: 0.5959 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5969 - accuracy: 0.7317 - val_loss: 0.6613 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6303 - accuracy: 0.6768 - val_loss: 0.5888 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.6088 - accuracy: 0.7073 - val_loss: 0.5900 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.5735 - accuracy: 0.7409 - val_loss: 0.5953 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.5618 - accuracy: 0.7470 - val_loss: 0.5922 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.5818 - accuracy: 0.7256 - val_loss: 0.5846 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5795 - accuracy: 0.7317 - val_loss: 0.5879 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5870 - accuracy: 0.7439 - val_loss: 0.6319 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.6437 - accuracy: 0.6768 - val_loss: 0.6510 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6482 - accuracy: 0.6768 - val_loss: 0.6790 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5873 - accuracy: 0.7378 - val_loss: 0.6554 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.5966 - accuracy: 0.7104 - val_loss: 0.6459 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4826 - accuracy: 0.8323 - val_loss: 0.6274 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4716 - accuracy: 0.8445 - val_loss: 0.6173 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.5112 - accuracy: 0.7988 - val_loss: 0.6110 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4880 - accuracy: 0.8171 - val_loss: 0.5997 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4951 - accuracy: 0.8110 - val_loss: 0.5901 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4966 - accuracy: 0.8140 - val_loss: 0.5914 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4784 - accuracy: 0.8293 - val_loss: 0.5884 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4797 - accuracy: 0.8262 - val_loss: 0.5774 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4696 - accuracy: 0.8384 - val_loss: 0.5669 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4693 - accuracy: 0.8354 - val_loss: 0.5655 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4724 - accuracy: 0.8354 - val_loss: 0.5335 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00033: val_accuracy improved from 0.75610 to 0.79268, saving model to ./SEfold1000000330.792683.hdf5\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4603 - accuracy: 0.8506 - val_loss: 0.5196 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.79268 to 0.80488, saving model to ./SEfold1000000340.804878.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4514 - accuracy: 0.8567 - val_loss: 0.5177 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.80488\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4504 - accuracy: 0.8628 - val_loss: 0.4967 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.80488 to 0.82927, saving model to ./SEfold1000000360.829268.hdf5\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4474 - accuracy: 0.8567 - val_loss: 0.5055 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.82927\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4830 - accuracy: 0.8232 - val_loss: 0.5322 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.82927\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4531 - accuracy: 0.8567 - val_loss: 0.4819 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.82927\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4518 - accuracy: 0.8567 - val_loss: 0.4926 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.82927\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4434 - accuracy: 0.8659 - val_loss: 0.4646 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.82927 to 0.84146, saving model to ./SEfold1000000410.841463.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.4818 - accuracy: 0.8262 - val_loss: 0.4741 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.84146\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4328 - accuracy: 0.8811 - val_loss: 0.4507 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.84146 to 0.86585, saving model to ./SEfold1000000430.865854.hdf5\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4246 - accuracy: 0.8841 - val_loss: 0.4803 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.86585\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4310 - accuracy: 0.8811 - val_loss: 0.4574 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.86585\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4247 - accuracy: 0.8902 - val_loss: 0.4401 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.86585\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4337 - accuracy: 0.8750 - val_loss: 0.4084 - val_accuracy: 0.9024\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.86585 to 0.90244, saving model to ./SEfold1000000470.902439.hdf5\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4238 - accuracy: 0.8933 - val_loss: 0.4190 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90244\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4265 - accuracy: 0.8933 - val_loss: 0.4734 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90244\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4518 - accuracy: 0.8506 - val_loss: 0.4467 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90244\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4307 - accuracy: 0.8841 - val_loss: 0.5106 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90244\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4359 - accuracy: 0.8720 - val_loss: 0.5172 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90244\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4476 - accuracy: 0.8598 - val_loss: 0.4722 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90244\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4699 - accuracy: 0.8415 - val_loss: 0.4729 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90244\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4356 - accuracy: 0.8720 - val_loss: 0.4632 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90244\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4400 - accuracy: 0.8659 - val_loss: 0.5036 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90244\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4342 - accuracy: 0.8780 - val_loss: 0.4354 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90244\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4310 - accuracy: 0.8811 - val_loss: 0.4909 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90244\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4446 - accuracy: 0.8628 - val_loss: 0.5212 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90244\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4267 - accuracy: 0.8841 - val_loss: 0.5158 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90244\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4539 - accuracy: 0.8537 - val_loss: 0.5368 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90244\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4371 - accuracy: 0.8780 - val_loss: 0.5148 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90244\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4346 - accuracy: 0.8780 - val_loss: 0.4202 - val_accuracy: 0.9024\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90244\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4613 - accuracy: 0.8506 - val_loss: 0.4617 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90244\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4306 - accuracy: 0.8841 - val_loss: 0.4534 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.90244\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4074 - accuracy: 0.8994 - val_loss: 0.7004 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.90244\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4142 - accuracy: 0.8963 - val_loss: 0.7668 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.90244\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4196 - accuracy: 0.8841 - val_loss: 0.4994 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.90244\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4198 - accuracy: 0.8963 - val_loss: 0.4770 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.90244\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4349 - accuracy: 0.8750 - val_loss: 0.4721 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.90244\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4313 - accuracy: 0.8780 - val_loss: 0.4609 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.90244\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4293 - accuracy: 0.8841 - val_loss: 0.4765 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.90244\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4265 - accuracy: 0.8872 - val_loss: 0.4847 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.90244\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4321 - accuracy: 0.8750 - val_loss: 0.4980 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.90244\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4005 - accuracy: 0.9085 - val_loss: 0.4898 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.90244\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4214 - accuracy: 0.8841 - val_loss: 0.4902 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.90244\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4081 - accuracy: 0.9024 - val_loss: 0.4759 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.90244\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4108 - accuracy: 0.8994 - val_loss: 0.4789 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.90244\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4025 - accuracy: 0.9055 - val_loss: 0.4755 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.90244\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4368 - accuracy: 0.8750 - val_loss: 0.4889 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.90244\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4506 - accuracy: 0.8689 - val_loss: 0.5078 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.90244\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4233 - accuracy: 0.8841 - val_loss: 0.4350 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.90244\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.4207 - accuracy: 0.8963 - val_loss: 0.4700 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.90244\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4230 - accuracy: 0.8872 - val_loss: 0.4558 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.90244\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4235 - accuracy: 0.8872 - val_loss: 0.5163 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.90244\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4527 - accuracy: 0.8506 - val_loss: 0.5843 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.90244\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4199 - accuracy: 0.8902 - val_loss: 0.5152 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.90244\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4406 - accuracy: 0.8689 - val_loss: 0.5023 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.90244\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4466 - accuracy: 0.8720 - val_loss: 0.5284 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.90244\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4216 - accuracy: 0.8872 - val_loss: 0.4777 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.90244\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4210 - accuracy: 0.8872 - val_loss: 0.5231 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.90244\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4519 - accuracy: 0.8567 - val_loss: 0.5079 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.90244\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4176 - accuracy: 0.8902 - val_loss: 0.5614 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.90244\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4164 - accuracy: 0.8933 - val_loss: 0.4791 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.90244\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4319 - accuracy: 0.8811 - val_loss: 0.5392 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.90244\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4140 - accuracy: 0.8963 - val_loss: 0.6711 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.90244\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4232 - accuracy: 0.8841 - val_loss: 0.5239 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.90244\n",
            "Epoch 00097: early stopping\n",
            "['FOLD:'] 2\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 22s 677ms/step - loss: 1.0297 - accuracy: 0.2439 - val_loss: 1.0693 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold2000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 193ms/step - loss: 0.8013 - accuracy: 0.5030 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.7133 - accuracy: 0.6128 - val_loss: 0.9366 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.24390\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.7464 - accuracy: 0.5610 - val_loss: 1.0690 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.24390\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6443 - accuracy: 0.6707 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.24390\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.6477 - accuracy: 0.6738 - val_loss: 0.5680 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.24390 to 0.75610, saving model to ./SEfold2000000060.756098.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.5929 - accuracy: 0.7134 - val_loss: 0.5671 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5403 - accuracy: 0.7805 - val_loss: 0.5890 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5076 - accuracy: 0.8079 - val_loss: 0.6028 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4998 - accuracy: 0.8079 - val_loss: 0.5932 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4778 - accuracy: 0.8323 - val_loss: 0.5807 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4899 - accuracy: 0.8201 - val_loss: 0.5709 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4663 - accuracy: 0.8537 - val_loss: 0.5643 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4805 - accuracy: 0.8323 - val_loss: 0.5674 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4462 - accuracy: 0.8659 - val_loss: 0.5686 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4649 - accuracy: 0.8506 - val_loss: 0.5625 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4726 - accuracy: 0.8354 - val_loss: 0.5624 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4807 - accuracy: 0.8293 - val_loss: 0.5604 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4511 - accuracy: 0.8598 - val_loss: 0.5671 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4600 - accuracy: 0.8506 - val_loss: 0.5661 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4546 - accuracy: 0.8567 - val_loss: 0.5712 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4375 - accuracy: 0.8750 - val_loss: 0.5621 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.75610 to 0.78049, saving model to ./SEfold2000000220.780488.hdf5\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4307 - accuracy: 0.8841 - val_loss: 0.5523 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.78049\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 208ms/step - loss: 0.4593 - accuracy: 0.8506 - val_loss: 0.5542 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.78049\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4399 - accuracy: 0.8750 - val_loss: 0.5606 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.78049\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4790 - accuracy: 0.8201 - val_loss: 0.5474 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.78049\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5203 - accuracy: 0.7835 - val_loss: 0.5654 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.78049\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4672 - accuracy: 0.8415 - val_loss: 0.5608 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.78049\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4437 - accuracy: 0.8628 - val_loss: 0.5619 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.78049\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4456 - accuracy: 0.8689 - val_loss: 0.5587 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.78049\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4132 - accuracy: 0.9024 - val_loss: 0.5506 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.78049\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4248 - accuracy: 0.8933 - val_loss: 0.5515 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.78049\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4272 - accuracy: 0.8811 - val_loss: 0.5504 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.78049\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4252 - accuracy: 0.8872 - val_loss: 0.5481 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.78049\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4141 - accuracy: 0.8963 - val_loss: 0.5497 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.78049\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4021 - accuracy: 0.9085 - val_loss: 0.5445 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.78049\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3960 - accuracy: 0.9207 - val_loss: 0.5275 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.78049 to 0.79268, saving model to ./SEfold2000000370.792683.hdf5\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4359 - accuracy: 0.8750 - val_loss: 0.5371 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.79268\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4285 - accuracy: 0.8872 - val_loss: 0.5564 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.79268\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4628 - accuracy: 0.8476 - val_loss: 0.5165 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.79268\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4345 - accuracy: 0.8720 - val_loss: 0.5265 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.79268\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4213 - accuracy: 0.8902 - val_loss: 0.4924 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00042: val_accuracy improved from 0.79268 to 0.81707, saving model to ./SEfold2000000420.817073.hdf5\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 6s 194ms/step - loss: 0.4200 - accuracy: 0.8933 - val_loss: 0.5064 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.81707\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4337 - accuracy: 0.8720 - val_loss: 0.5125 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.81707\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4311 - accuracy: 0.8750 - val_loss: 0.4919 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.81707\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4331 - accuracy: 0.8841 - val_loss: 0.5990 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.81707\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3993 - accuracy: 0.9116 - val_loss: 0.5163 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.81707\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4273 - accuracy: 0.8780 - val_loss: 0.5328 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.81707\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4616 - accuracy: 0.8384 - val_loss: 0.5535 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.81707\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4602 - accuracy: 0.8476 - val_loss: 0.5572 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.81707\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4298 - accuracy: 0.8872 - val_loss: 0.5296 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.81707\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4131 - accuracy: 0.9055 - val_loss: 0.5268 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.81707\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4285 - accuracy: 0.8811 - val_loss: 0.5079 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.81707\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4165 - accuracy: 0.8933 - val_loss: 0.5288 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.81707\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3957 - accuracy: 0.9177 - val_loss: 0.5176 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.81707\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4117 - accuracy: 0.8963 - val_loss: 0.5389 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.81707\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4325 - accuracy: 0.8750 - val_loss: 0.5338 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.81707\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4131 - accuracy: 0.8963 - val_loss: 0.4896 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.81707\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4222 - accuracy: 0.8841 - val_loss: 0.5134 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.81707\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4220 - accuracy: 0.8841 - val_loss: 0.5047 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.81707\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4195 - accuracy: 0.8872 - val_loss: 0.5142 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.81707\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4039 - accuracy: 0.9055 - val_loss: 0.5177 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.81707\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4198 - accuracy: 0.8872 - val_loss: 0.5536 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.81707\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3899 - accuracy: 0.9238 - val_loss: 0.5507 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.81707\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4014 - accuracy: 0.9055 - val_loss: 0.5284 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.81707\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4125 - accuracy: 0.9024 - val_loss: 0.4688 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00066: val_accuracy improved from 0.81707 to 0.84146, saving model to ./SEfold2000000660.841463.hdf5\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.3997 - accuracy: 0.9116 - val_loss: 0.4686 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.84146\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6458 - accuracy: 0.6646 - val_loss: 0.6868 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.84146\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4746 - accuracy: 0.8384 - val_loss: 0.5321 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.84146\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4214 - accuracy: 0.8933 - val_loss: 0.4910 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.84146\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4113 - accuracy: 0.8963 - val_loss: 0.4825 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.84146\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4219 - accuracy: 0.8841 - val_loss: 0.4906 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.84146\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3908 - accuracy: 0.9177 - val_loss: 0.4889 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.84146\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4082 - accuracy: 0.9055 - val_loss: 0.4937 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.84146\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4010 - accuracy: 0.9116 - val_loss: 0.5001 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.84146\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3944 - accuracy: 0.9177 - val_loss: 0.4968 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.84146\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4107 - accuracy: 0.8994 - val_loss: 0.4949 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.84146\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4136 - accuracy: 0.8963 - val_loss: 0.5141 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.84146\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4001 - accuracy: 0.9116 - val_loss: 0.5362 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.84146\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4218 - accuracy: 0.8872 - val_loss: 0.4746 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.84146\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4079 - accuracy: 0.9055 - val_loss: 0.5104 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.84146\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4225 - accuracy: 0.8902 - val_loss: 0.4981 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.84146\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4011 - accuracy: 0.9055 - val_loss: 0.5094 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.84146\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4140 - accuracy: 0.8963 - val_loss: 0.5105 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.84146\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3924 - accuracy: 0.9146 - val_loss: 0.4764 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.84146\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4049 - accuracy: 0.9024 - val_loss: 0.4657 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.84146\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4376 - accuracy: 0.8720 - val_loss: 0.5410 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.84146\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4140 - accuracy: 0.9024 - val_loss: 0.5278 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.84146\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4155 - accuracy: 0.8963 - val_loss: 0.4916 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.84146\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3977 - accuracy: 0.9146 - val_loss: 0.4849 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.84146\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4217 - accuracy: 0.8902 - val_loss: 0.5090 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.84146\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4234 - accuracy: 0.8902 - val_loss: 0.5215 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.84146\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4041 - accuracy: 0.9116 - val_loss: 0.4999 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.84146\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.3923 - accuracy: 0.9207 - val_loss: 0.5060 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.84146\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3983 - accuracy: 0.9177 - val_loss: 0.5626 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.84146\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4134 - accuracy: 0.9024 - val_loss: 0.5562 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.84146\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3864 - accuracy: 0.9268 - val_loss: 0.4947 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.84146\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3846 - accuracy: 0.9329 - val_loss: 0.4862 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.84146\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3804 - accuracy: 0.9329 - val_loss: 0.5037 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.84146\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4009 - accuracy: 0.9085 - val_loss: 0.5025 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.84146\n",
            "['FOLD:'] 3\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 23s 686ms/step - loss: 1.0155 - accuracy: 0.2439 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold3000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.7950 - accuracy: 0.5061 - val_loss: 1.0686 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.7554 - accuracy: 0.5671 - val_loss: 0.8773 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.24390\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6215 - accuracy: 0.6921 - val_loss: 0.7264 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.24390\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6627 - accuracy: 0.6585 - val_loss: 0.7401 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.24390\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.6212 - accuracy: 0.6951 - val_loss: 0.6936 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.24390 to 0.52439, saving model to ./SEfold3000000060.524390.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.6518 - accuracy: 0.6585 - val_loss: 0.6639 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.52439 to 0.74390, saving model to ./SEfold3000000070.743902.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.6499 - accuracy: 0.6677 - val_loss: 0.6241 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.74390 to 0.75610, saving model to ./SEfold3000000080.756098.hdf5\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.6417 - accuracy: 0.6677 - val_loss: 0.5953 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.6652 - accuracy: 0.6280 - val_loss: 0.6078 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.6452 - accuracy: 0.6616 - val_loss: 0.6033 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6128 - accuracy: 0.6921 - val_loss: 0.5880 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6312 - accuracy: 0.6829 - val_loss: 0.7025 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.6009 - accuracy: 0.7073 - val_loss: 0.7160 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5159 - accuracy: 0.7957 - val_loss: 0.6594 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4953 - accuracy: 0.8323 - val_loss: 0.9045 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4510 - accuracy: 0.8659 - val_loss: 1.0563 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4470 - accuracy: 0.8598 - val_loss: 1.0607 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4594 - accuracy: 0.8537 - val_loss: 1.0667 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4490 - accuracy: 0.8689 - val_loss: 1.0687 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4847 - accuracy: 0.8201 - val_loss: 1.0688 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4343 - accuracy: 0.8811 - val_loss: 1.0688 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4750 - accuracy: 0.8293 - val_loss: 1.0693 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4958 - accuracy: 0.8079 - val_loss: 1.0684 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4656 - accuracy: 0.8476 - val_loss: 1.0672 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4666 - accuracy: 0.8476 - val_loss: 1.0668 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4665 - accuracy: 0.8415 - val_loss: 1.0615 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4628 - accuracy: 0.8476 - val_loss: 1.0649 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4590 - accuracy: 0.8506 - val_loss: 1.0586 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4436 - accuracy: 0.8689 - val_loss: 1.0543 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4361 - accuracy: 0.8750 - val_loss: 0.7971 - val_accuracy: 0.4146\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4486 - accuracy: 0.8689 - val_loss: 0.8383 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4511 - accuracy: 0.8567 - val_loss: 1.0333 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4422 - accuracy: 0.8720 - val_loss: 0.8315 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.75610\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4388 - accuracy: 0.8628 - val_loss: 0.7935 - val_accuracy: 0.4634\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.75610\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4629 - accuracy: 0.8445 - val_loss: 0.7812 - val_accuracy: 0.4878\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.75610\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4452 - accuracy: 0.8659 - val_loss: 0.7022 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.75610\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4414 - accuracy: 0.8689 - val_loss: 0.6682 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.75610\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4434 - accuracy: 0.8720 - val_loss: 0.7005 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.75610\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4753 - accuracy: 0.8262 - val_loss: 0.7607 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.75610\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4439 - accuracy: 0.8598 - val_loss: 0.5838 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.75610\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4467 - accuracy: 0.8659 - val_loss: 0.5949 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.75610\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4240 - accuracy: 0.8933 - val_loss: 0.5567 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.75610\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4389 - accuracy: 0.8720 - val_loss: 0.4944 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.75610 to 0.81707, saving model to ./SEfold3000000440.817073.hdf5\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4331 - accuracy: 0.8780 - val_loss: 0.5132 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.81707\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4278 - accuracy: 0.8841 - val_loss: 0.4641 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.81707 to 0.85366, saving model to ./SEfold3000000460.853659.hdf5\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4207 - accuracy: 0.8872 - val_loss: 0.4943 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.85366\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4365 - accuracy: 0.8750 - val_loss: 0.4809 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.85366\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4728 - accuracy: 0.8293 - val_loss: 0.4936 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.85366\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4784 - accuracy: 0.8354 - val_loss: 0.5023 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.85366\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.4441 - accuracy: 0.8659 - val_loss: 0.6856 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.85366\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4605 - accuracy: 0.8537 - val_loss: 0.6041 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.85366\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.4288 - accuracy: 0.8872 - val_loss: 0.5734 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.85366\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4462 - accuracy: 0.8537 - val_loss: 0.5163 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.85366\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4197 - accuracy: 0.8902 - val_loss: 0.4752 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.85366\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4242 - accuracy: 0.8872 - val_loss: 0.4635 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.85366\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4272 - accuracy: 0.8933 - val_loss: 0.4627 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.85366\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4291 - accuracy: 0.8811 - val_loss: 0.4597 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.85366\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4244 - accuracy: 0.8902 - val_loss: 0.5107 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.85366\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4309 - accuracy: 0.8780 - val_loss: 0.4711 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.85366\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4142 - accuracy: 0.8994 - val_loss: 0.4615 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.85366\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4142 - accuracy: 0.9024 - val_loss: 0.4677 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.85366\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4013 - accuracy: 0.9085 - val_loss: 0.4578 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.85366\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4138 - accuracy: 0.9024 - val_loss: 0.4806 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.85366\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.4305 - accuracy: 0.8811 - val_loss: 0.4368 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00065: val_accuracy improved from 0.85366 to 0.87805, saving model to ./SEfold3000000650.878049.hdf5\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4841 - accuracy: 0.8262 - val_loss: 0.5343 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.87805\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4538 - accuracy: 0.8537 - val_loss: 0.5116 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.87805\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4465 - accuracy: 0.8628 - val_loss: 0.5033 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.87805\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4280 - accuracy: 0.8872 - val_loss: 0.5552 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.87805\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4384 - accuracy: 0.8720 - val_loss: 0.4582 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.87805\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3930 - accuracy: 0.9238 - val_loss: 0.5349 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.87805\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4079 - accuracy: 0.9055 - val_loss: 0.4726 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.87805\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4098 - accuracy: 0.9024 - val_loss: 0.4859 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.87805\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4054 - accuracy: 0.9024 - val_loss: 0.4752 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.87805\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4045 - accuracy: 0.9085 - val_loss: 0.4895 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.87805\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4163 - accuracy: 0.8933 - val_loss: 0.4964 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.87805\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3960 - accuracy: 0.9146 - val_loss: 0.5208 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.87805\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4083 - accuracy: 0.9024 - val_loss: 0.4792 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.87805\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3834 - accuracy: 0.9329 - val_loss: 0.4969 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.87805\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4114 - accuracy: 0.8994 - val_loss: 0.5618 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.87805\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4222 - accuracy: 0.8872 - val_loss: 0.8272 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.87805\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4298 - accuracy: 0.8720 - val_loss: 0.4793 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.87805\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4232 - accuracy: 0.8841 - val_loss: 0.4754 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.87805\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4589 - accuracy: 0.8506 - val_loss: 0.4797 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.87805\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.4236 - accuracy: 0.8872 - val_loss: 0.4870 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.87805\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4258 - accuracy: 0.8811 - val_loss: 0.4875 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.87805\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4241 - accuracy: 0.8811 - val_loss: 0.4962 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.87805\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4258 - accuracy: 0.8811 - val_loss: 0.4692 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.87805\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4027 - accuracy: 0.9116 - val_loss: 0.4946 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.87805\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4146 - accuracy: 0.8963 - val_loss: 0.4853 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.87805\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4145 - accuracy: 0.8994 - val_loss: 0.4997 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.87805\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4234 - accuracy: 0.8902 - val_loss: 0.5063 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.87805\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4099 - accuracy: 0.8994 - val_loss: 0.4669 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.87805\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4486 - accuracy: 0.8659 - val_loss: 0.4915 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.87805\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.4269 - accuracy: 0.8841 - val_loss: 0.5075 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.87805\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3954 - accuracy: 0.9177 - val_loss: 0.4443 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.87805\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4368 - accuracy: 0.8811 - val_loss: 0.4938 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.87805\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4239 - accuracy: 0.8811 - val_loss: 0.5383 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.87805\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4232 - accuracy: 0.8841 - val_loss: 0.5451 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.87805\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4079 - accuracy: 0.9055 - val_loss: 0.4703 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.87805\n",
            "['FOLD:'] 4\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 22s 672ms/step - loss: 1.0129 - accuracy: 0.2470 - val_loss: 1.0691 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold4000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.7333 - accuracy: 0.5274 - val_loss: 1.0122 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.5564 - accuracy: 0.7713 - val_loss: 0.9600 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.24390\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5118 - accuracy: 0.7988 - val_loss: 0.9278 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.24390\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5377 - accuracy: 0.7652 - val_loss: 0.7596 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.24390\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 7s 207ms/step - loss: 0.5341 - accuracy: 0.7805 - val_loss: 0.6931 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.24390 to 0.52439, saving model to ./SEfold4000000060.524390.hdf5\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 8s 236ms/step - loss: 0.4851 - accuracy: 0.8323 - val_loss: 0.6391 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.52439 to 0.75610, saving model to ./SEfold4000000070.756098.hdf5\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4984 - accuracy: 0.8140 - val_loss: 0.6218 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4681 - accuracy: 0.8445 - val_loss: 0.7331 - val_accuracy: 0.4146\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4791 - accuracy: 0.8323 - val_loss: 1.0100 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4668 - accuracy: 0.8445 - val_loss: 1.0326 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4948 - accuracy: 0.8140 - val_loss: 0.8002 - val_accuracy: 0.2805\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4550 - accuracy: 0.8598 - val_loss: 1.0509 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.4942 - accuracy: 0.8140 - val_loss: 1.0673 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4682 - accuracy: 0.8445 - val_loss: 1.0689 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5544 - accuracy: 0.7591 - val_loss: 1.0688 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.5002 - accuracy: 0.8049 - val_loss: 1.0648 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4889 - accuracy: 0.8262 - val_loss: 1.0547 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.5271 - accuracy: 0.7774 - val_loss: 1.0632 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.5045 - accuracy: 0.8018 - val_loss: 1.0633 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4897 - accuracy: 0.8201 - val_loss: 0.8976 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4751 - accuracy: 0.8415 - val_loss: 0.9998 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.5714 - accuracy: 0.7317 - val_loss: 1.0152 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.5751 - accuracy: 0.7317 - val_loss: 0.9854 - val_accuracy: 0.2317\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5544 - accuracy: 0.7591 - val_loss: 1.0687 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5224 - accuracy: 0.7896 - val_loss: 1.0691 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5046 - accuracy: 0.8079 - val_loss: 1.0692 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4751 - accuracy: 0.8415 - val_loss: 1.0686 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4994 - accuracy: 0.8079 - val_loss: 1.0676 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.5234 - accuracy: 0.7866 - val_loss: 1.0680 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.5211 - accuracy: 0.7835 - val_loss: 1.0571 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.75610\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.5150 - accuracy: 0.7988 - val_loss: 1.0369 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.75610\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 7s 207ms/step - loss: 0.4887 - accuracy: 0.8323 - val_loss: 1.0223 - val_accuracy: 0.2683\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.75610\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4741 - accuracy: 0.8415 - val_loss: 0.9008 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.75610\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5097 - accuracy: 0.8018 - val_loss: 0.6944 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.75610\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5243 - accuracy: 0.7866 - val_loss: 0.6280 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.75610\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4950 - accuracy: 0.8140 - val_loss: 0.6706 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.75610\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.5595 - accuracy: 0.7500 - val_loss: 0.9414 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.75610\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5684 - accuracy: 0.7439 - val_loss: 0.7056 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.75610\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.5237 - accuracy: 0.7896 - val_loss: 0.5616 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.75610\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.5046 - accuracy: 0.7988 - val_loss: 0.5153 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.75610 to 0.79268, saving model to ./SEfold4000000410.792683.hdf5\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.5002 - accuracy: 0.8110 - val_loss: 0.5547 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.79268\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.5018 - accuracy: 0.8140 - val_loss: 0.4758 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.79268 to 0.84146, saving model to ./SEfold4000000430.841463.hdf5\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.4672 - accuracy: 0.8415 - val_loss: 0.4953 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.84146\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4923 - accuracy: 0.8232 - val_loss: 0.4823 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.84146\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4561 - accuracy: 0.8628 - val_loss: 0.4826 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.84146\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4429 - accuracy: 0.8750 - val_loss: 0.4782 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.84146\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4372 - accuracy: 0.8750 - val_loss: 0.5009 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.84146\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4657 - accuracy: 0.8354 - val_loss: 0.5044 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.84146\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.4401 - accuracy: 0.8628 - val_loss: 0.4943 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.84146\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4617 - accuracy: 0.8445 - val_loss: 0.5493 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.84146\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4677 - accuracy: 0.8445 - val_loss: 0.4929 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.84146\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4406 - accuracy: 0.8750 - val_loss: 0.4803 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.84146\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4442 - accuracy: 0.8598 - val_loss: 0.4704 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.84146\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4465 - accuracy: 0.8628 - val_loss: 0.4950 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.84146\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4372 - accuracy: 0.8780 - val_loss: 0.4662 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00056: val_accuracy improved from 0.84146 to 0.85366, saving model to ./SEfold4000000560.853659.hdf5\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4190 - accuracy: 0.8963 - val_loss: 0.4757 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.85366\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4229 - accuracy: 0.8933 - val_loss: 0.4649 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.85366\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4321 - accuracy: 0.8811 - val_loss: 0.4581 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.85366\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4483 - accuracy: 0.8659 - val_loss: 0.4735 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.85366\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4611 - accuracy: 0.8506 - val_loss: 0.4719 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.85366\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4366 - accuracy: 0.8780 - val_loss: 0.4777 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.85366\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4406 - accuracy: 0.8720 - val_loss: 0.5028 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.85366\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4383 - accuracy: 0.8750 - val_loss: 0.4746 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.85366\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.4624 - accuracy: 0.8445 - val_loss: 0.5034 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.85366\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4409 - accuracy: 0.8659 - val_loss: 0.4894 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.85366\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4352 - accuracy: 0.8811 - val_loss: 0.4837 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.85366\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4411 - accuracy: 0.8720 - val_loss: 0.5072 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.85366\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 7s 206ms/step - loss: 0.4111 - accuracy: 0.9024 - val_loss: 0.4980 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.85366\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4558 - accuracy: 0.8567 - val_loss: 0.4968 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.85366\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4593 - accuracy: 0.8506 - val_loss: 0.5126 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.85366\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4527 - accuracy: 0.8598 - val_loss: 0.5201 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.85366\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4333 - accuracy: 0.8689 - val_loss: 0.5002 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.85366\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4473 - accuracy: 0.8659 - val_loss: 0.4721 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.85366\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4164 - accuracy: 0.8963 - val_loss: 0.5201 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.85366\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4324 - accuracy: 0.8780 - val_loss: 0.5086 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.85366\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 207ms/step - loss: 0.4157 - accuracy: 0.8963 - val_loss: 0.5074 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.85366\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4224 - accuracy: 0.8902 - val_loss: 0.4741 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.85366\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4132 - accuracy: 0.8994 - val_loss: 0.5077 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.85366\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4215 - accuracy: 0.8933 - val_loss: 0.4682 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.85366\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4106 - accuracy: 0.9055 - val_loss: 0.4675 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.85366\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4078 - accuracy: 0.9055 - val_loss: 0.4609 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.85366\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4094 - accuracy: 0.9055 - val_loss: 0.4479 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00083: val_accuracy improved from 0.85366 to 0.86585, saving model to ./SEfold4000000830.865854.hdf5\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 6s 197ms/step - loss: 0.4017 - accuracy: 0.9116 - val_loss: 0.4806 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.86585\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4183 - accuracy: 0.8902 - val_loss: 0.5044 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.86585\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3946 - accuracy: 0.9207 - val_loss: 0.4671 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.86585\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4098 - accuracy: 0.9024 - val_loss: 0.4619 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.86585\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4168 - accuracy: 0.8963 - val_loss: 0.4612 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.86585\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4440 - accuracy: 0.8689 - val_loss: 0.5208 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.86585\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4403 - accuracy: 0.8720 - val_loss: 0.6883 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.86585\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4113 - accuracy: 0.9024 - val_loss: 0.5012 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.86585\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4453 - accuracy: 0.8598 - val_loss: 0.5401 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.86585\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4616 - accuracy: 0.8506 - val_loss: 0.4959 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.86585\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4315 - accuracy: 0.8872 - val_loss: 0.5312 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.86585\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5722 - accuracy: 0.7348 - val_loss: 0.7846 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.86585\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.5360 - accuracy: 0.7774 - val_loss: 0.6917 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.86585\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5386 - accuracy: 0.7713 - val_loss: 0.5568 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.86585\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.5320 - accuracy: 0.7835 - val_loss: 0.5467 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.86585\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.5343 - accuracy: 0.7744 - val_loss: 0.5399 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.86585\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.5527 - accuracy: 0.7561 - val_loss: 0.5566 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.86585\n",
            "['FOLD:'] 5\n",
            "Epoch 1/100\n",
            "33/33 [==============================] - 23s 698ms/step - loss: 1.0600 - accuracy: 0.2439 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold5000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "33/33 [==============================] - 6s 196ms/step - loss: 0.9448 - accuracy: 0.3476 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.7631 - accuracy: 0.5488 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.24390\n",
            "Epoch 4/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.6478 - accuracy: 0.6646 - val_loss: 1.0694 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.24390\n",
            "Epoch 5/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.6830 - accuracy: 0.6463 - val_loss: 0.6008 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.24390 to 0.75610, saving model to ./SEfold5000000050.756098.hdf5\n",
            "Epoch 6/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.6043 - accuracy: 0.7104 - val_loss: 0.5845 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6138 - accuracy: 0.7073 - val_loss: 0.5799 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.5958 - accuracy: 0.7165 - val_loss: 0.5890 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6574 - accuracy: 0.6646 - val_loss: 0.6025 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.6036 - accuracy: 0.7134 - val_loss: 0.6017 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5679 - accuracy: 0.7500 - val_loss: 0.5965 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.5960 - accuracy: 0.7134 - val_loss: 0.6012 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.5902 - accuracy: 0.7287 - val_loss: 0.6116 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.5590 - accuracy: 0.7561 - val_loss: 0.6141 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4604 - accuracy: 0.8567 - val_loss: 0.6328 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4514 - accuracy: 0.8689 - val_loss: 0.6394 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4679 - accuracy: 0.8445 - val_loss: 0.6393 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4619 - accuracy: 0.8506 - val_loss: 0.6314 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4751 - accuracy: 0.8384 - val_loss: 0.6185 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4610 - accuracy: 0.8476 - val_loss: 0.6081 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.75610\n",
            "Epoch 21/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.4351 - accuracy: 0.8780 - val_loss: 0.6076 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.75610\n",
            "Epoch 22/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4536 - accuracy: 0.8598 - val_loss: 0.6110 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.75610\n",
            "Epoch 23/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4360 - accuracy: 0.8841 - val_loss: 0.6195 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.75610\n",
            "Epoch 24/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4440 - accuracy: 0.8689 - val_loss: 0.6122 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.75610\n",
            "Epoch 25/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4324 - accuracy: 0.8780 - val_loss: 0.6001 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.75610\n",
            "Epoch 26/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4316 - accuracy: 0.8811 - val_loss: 0.5810 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.75610\n",
            "Epoch 27/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4564 - accuracy: 0.8598 - val_loss: 0.5759 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.75610\n",
            "Epoch 28/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4633 - accuracy: 0.8415 - val_loss: 0.5807 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.75610\n",
            "Epoch 29/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4454 - accuracy: 0.8659 - val_loss: 0.5784 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.75610\n",
            "Epoch 30/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4360 - accuracy: 0.8720 - val_loss: 0.5766 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.75610\n",
            "Epoch 31/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4214 - accuracy: 0.8902 - val_loss: 0.5739 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.75610 to 0.76829, saving model to ./SEfold5000000310.768293.hdf5\n",
            "Epoch 32/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4477 - accuracy: 0.8689 - val_loss: 0.9773 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.76829\n",
            "Epoch 33/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4483 - accuracy: 0.8628 - val_loss: 0.5792 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.76829\n",
            "Epoch 34/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4291 - accuracy: 0.8811 - val_loss: 0.5758 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.76829 to 0.79268, saving model to ./SEfold5000000340.792683.hdf5\n",
            "Epoch 35/100\n",
            "33/33 [==============================] - 6s 195ms/step - loss: 0.4454 - accuracy: 0.8628 - val_loss: 0.5737 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.79268\n",
            "Epoch 36/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4386 - accuracy: 0.8689 - val_loss: 0.5988 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.79268\n",
            "Epoch 37/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4321 - accuracy: 0.8750 - val_loss: 0.5335 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.79268\n",
            "Epoch 38/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4347 - accuracy: 0.8750 - val_loss: 0.5468 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.79268\n",
            "Epoch 39/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4431 - accuracy: 0.8689 - val_loss: 0.7801 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.79268\n",
            "Epoch 40/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4336 - accuracy: 0.8780 - val_loss: 0.5206 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.79268\n",
            "Epoch 41/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4302 - accuracy: 0.8872 - val_loss: 0.5219 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.79268\n",
            "Epoch 42/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4113 - accuracy: 0.9055 - val_loss: 0.5381 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.79268\n",
            "Epoch 43/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4188 - accuracy: 0.8963 - val_loss: 0.5351 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.79268\n",
            "Epoch 44/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4275 - accuracy: 0.8811 - val_loss: 0.5357 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.79268\n",
            "Epoch 45/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4360 - accuracy: 0.8780 - val_loss: 0.5129 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.79268 to 0.80488, saving model to ./SEfold5000000450.804878.hdf5\n",
            "Epoch 46/100\n",
            "33/33 [==============================] - 7s 197ms/step - loss: 0.4225 - accuracy: 0.8811 - val_loss: 0.5513 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.80488\n",
            "Epoch 47/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4172 - accuracy: 0.8963 - val_loss: 0.5569 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.80488\n",
            "Epoch 48/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4128 - accuracy: 0.8963 - val_loss: 0.5298 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.80488\n",
            "Epoch 49/100\n",
            "33/33 [==============================] - 7s 204ms/step - loss: 0.4244 - accuracy: 0.8841 - val_loss: 0.5176 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.80488\n",
            "Epoch 50/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4203 - accuracy: 0.8933 - val_loss: 0.5155 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.80488\n",
            "Epoch 51/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4194 - accuracy: 0.8933 - val_loss: 0.4991 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.80488 to 0.81707, saving model to ./SEfold5000000510.817073.hdf5\n",
            "Epoch 52/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4071 - accuracy: 0.8994 - val_loss: 0.5200 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.81707\n",
            "Epoch 53/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4630 - accuracy: 0.8567 - val_loss: 0.5240 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.81707\n",
            "Epoch 54/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4141 - accuracy: 0.8963 - val_loss: 0.5201 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.81707\n",
            "Epoch 55/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4658 - accuracy: 0.8415 - val_loss: 0.5299 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.81707\n",
            "Epoch 56/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4464 - accuracy: 0.8598 - val_loss: 0.5559 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.81707\n",
            "Epoch 57/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4369 - accuracy: 0.8750 - val_loss: 0.6959 - val_accuracy: 0.5976\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.81707\n",
            "Epoch 58/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4481 - accuracy: 0.8567 - val_loss: 0.5612 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.81707\n",
            "Epoch 59/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.5065 - accuracy: 0.7896 - val_loss: 0.5573 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.81707\n",
            "Epoch 60/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4540 - accuracy: 0.8598 - val_loss: 0.5578 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.81707\n",
            "Epoch 61/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4137 - accuracy: 0.8994 - val_loss: 0.4985 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.81707\n",
            "Epoch 62/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4051 - accuracy: 0.9085 - val_loss: 0.5397 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.81707\n",
            "Epoch 63/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4151 - accuracy: 0.8963 - val_loss: 0.4956 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00063: val_accuracy improved from 0.81707 to 0.82927, saving model to ./SEfold5000000630.829268.hdf5\n",
            "Epoch 64/100\n",
            "33/33 [==============================] - 7s 198ms/step - loss: 0.4022 - accuracy: 0.9116 - val_loss: 0.5309 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.82927\n",
            "Epoch 65/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4222 - accuracy: 0.8902 - val_loss: 0.5220 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.82927\n",
            "Epoch 66/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4044 - accuracy: 0.9116 - val_loss: 0.5349 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.82927\n",
            "Epoch 67/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4156 - accuracy: 0.8933 - val_loss: 0.4962 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.82927\n",
            "Epoch 68/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4056 - accuracy: 0.9055 - val_loss: 0.5117 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.82927\n",
            "Epoch 69/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3897 - accuracy: 0.9268 - val_loss: 0.5337 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.82927\n",
            "Epoch 70/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4087 - accuracy: 0.9024 - val_loss: 0.5459 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.82927\n",
            "Epoch 71/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4219 - accuracy: 0.8872 - val_loss: 0.5334 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.82927\n",
            "Epoch 72/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3965 - accuracy: 0.9177 - val_loss: 0.5328 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.82927\n",
            "Epoch 73/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3861 - accuracy: 0.9268 - val_loss: 0.5333 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.82927\n",
            "Epoch 74/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4521 - accuracy: 0.8567 - val_loss: 0.5332 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.82927\n",
            "Epoch 75/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4408 - accuracy: 0.8720 - val_loss: 0.5097 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.82927\n",
            "Epoch 76/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3798 - accuracy: 0.9329 - val_loss: 0.5012 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.82927\n",
            "Epoch 77/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4008 - accuracy: 0.9116 - val_loss: 0.5192 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.82927\n",
            "Epoch 78/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4433 - accuracy: 0.8689 - val_loss: 0.5239 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.82927\n",
            "Epoch 79/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.3969 - accuracy: 0.9146 - val_loss: 0.5304 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.82927\n",
            "Epoch 80/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3927 - accuracy: 0.9207 - val_loss: 0.5094 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.82927\n",
            "Epoch 81/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3940 - accuracy: 0.9177 - val_loss: 0.5034 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.82927\n",
            "Epoch 82/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.4052 - accuracy: 0.9085 - val_loss: 0.5213 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.82927\n",
            "Epoch 83/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4106 - accuracy: 0.8994 - val_loss: 0.5393 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.82927\n",
            "Epoch 84/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4097 - accuracy: 0.8994 - val_loss: 0.5567 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.82927\n",
            "Epoch 85/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4092 - accuracy: 0.9055 - val_loss: 0.5569 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.82927\n",
            "Epoch 86/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4102 - accuracy: 0.9024 - val_loss: 0.5784 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.82927\n",
            "Epoch 87/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3916 - accuracy: 0.9207 - val_loss: 0.5202 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.82927\n",
            "Epoch 88/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.4011 - accuracy: 0.9116 - val_loss: 0.6199 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.82927\n",
            "Epoch 89/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4040 - accuracy: 0.9116 - val_loss: 0.5724 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.82927\n",
            "Epoch 90/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4265 - accuracy: 0.8872 - val_loss: 0.5357 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.82927\n",
            "Epoch 91/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3831 - accuracy: 0.9268 - val_loss: 0.5185 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.82927\n",
            "Epoch 92/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4103 - accuracy: 0.9024 - val_loss: 0.5586 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.82927\n",
            "Epoch 93/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.4143 - accuracy: 0.8963 - val_loss: 0.5397 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.82927\n",
            "Epoch 94/100\n",
            "33/33 [==============================] - 7s 205ms/step - loss: 0.3912 - accuracy: 0.9207 - val_loss: 0.6164 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.82927\n",
            "Epoch 95/100\n",
            "33/33 [==============================] - 7s 199ms/step - loss: 0.3903 - accuracy: 0.9177 - val_loss: 0.5204 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.82927\n",
            "Epoch 96/100\n",
            "33/33 [==============================] - 7s 201ms/step - loss: 0.3917 - accuracy: 0.9177 - val_loss: 0.4877 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.82927\n",
            "Epoch 97/100\n",
            "33/33 [==============================] - 7s 200ms/step - loss: 0.3867 - accuracy: 0.9268 - val_loss: 0.5225 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.82927\n",
            "Epoch 98/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.4263 - accuracy: 0.8872 - val_loss: 0.6925 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.82927\n",
            "Epoch 99/100\n",
            "33/33 [==============================] - 7s 202ms/step - loss: 0.4063 - accuracy: 0.9024 - val_loss: 0.5095 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.82927\n",
            "Epoch 100/100\n",
            "33/33 [==============================] - 7s 203ms/step - loss: 0.3910 - accuracy: 0.9268 - val_loss: 0.5651 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.82927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0J3YG2DUZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def classifyEval(testX, testy,model):\n",
        "  # predict probabilities for test set  \n",
        "  yhat_probs = model.predict(testX, verbose=0)\n",
        "  yhat = np.max(yhat_probs,axis=1)\n",
        "  yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "  x=1\n",
        "  print(testy[:,x])\n",
        "  print(yhat_classes)\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(testy[:,x], yhat_classes)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(testy[:,x], yhat_classes)\n",
        "  print('Precision: %f' % precision)\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(testy[:,x], yhat_classes)\n",
        "  print('Recall: %f' % recall)\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(testy[:,x], yhat_classes)\n",
        "  print('F1 score: %f' % f1)\n",
        " \n",
        "  # kappa\n",
        "  kappa = cohen_kappa_score(testy[:,x], yhat_classes)\n",
        "  print('Cohens kappa: %f' % kappa)\n",
        "  # ROC AUC\n",
        "  auc = roc_auc_score(testy[:,x], yhat_probs[:,x])\n",
        "  print('ROC AUC: %f' % auc)\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(testy[:,x], yhat_classes)\n",
        "  print(matrix)\n",
        "\n",
        "  fpr_keras, tpr_keras, thresholds_keras = sklearn.metrics.roc_curve(testy[:,x], yhat_probs[:,x])\n",
        "  return fpr_keras, tpr_keras, thresholds_keras\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.plot(fpr_keras, tpr_keras, label='CV1 (area = {:.3f})'.format(auc))\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acURLMezCxA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpr_list = []\n",
        "fpr_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxXwIdDaTncb",
        "colab_type": "code",
        "outputId": "a44488ed-7f8b-4818-d533-785ce26aafc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold5.npy')\n",
        "Y_test = np.load('/content/TestLabelFold5.npy')\n",
        "model = model5()\n",
        "model.load_weights('/content/SEfold5000000630.829268.hdf5')\n",
        "fpr, tpr, _ = classifyEval(X_test_extend, Y_test, model)\n",
        "fpr_list.append(fpr)\n",
        "tpr_list.append(tpr)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1.]\n",
            "[0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0\n",
            " 0 0 0 1 0 0 1 1]\n",
            "Accuracy: 0.817073\n",
            "Precision: 0.727273\n",
            "Recall: 0.400000\n",
            "F1 score: 0.516129\n",
            "Cohens kappa: 0.414843\n",
            "ROC AUC: 0.691935\n",
            "[[59  3]\n",
            " [12  8]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q99ruMRwFMuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auc = [.95,.80,.73,.84,.69]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2ejE2XEHlN",
        "colab_type": "code",
        "outputId": "8fad6c41-dc6f-4747-da18-152a13bd537c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "for i in range(5):\n",
        "  plt.plot(fpr_list[i], tpr_list[i],label='CV{} (area = {:.2f})'.format(i,auc[i]))\n",
        "  plt.legend(loc='CV{}'.format(i))\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV0'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV1'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV2'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV3'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV4'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuERoQ6CUZZ0",
        "colab_type": "text"
      },
      "source": [
        "## Attention Model for Multi Instance Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtK_lHXrUZZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featureextractor(data):\n",
        "    conv = Conv2D(20, 5, activation = None, padding = 'same')(data)\n",
        "    conv_a = Activation('relu')(BatchNormalization()(conv))\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv_a)\n",
        "    conv1 = Conv2D(50, 5, activation = None, padding = 'same')(pool1)\n",
        "    conv1_a = Activation('relu')(BatchNormalization()(conv1))\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
        "    return pool2\n",
        "\n",
        "def featureextractorx2(data):\n",
        "    dense = Dense(500,activation='relu')(data)\n",
        "    return dense\n",
        "\n",
        "def Attention(data):\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    return dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3miMxRUZZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "l1factor=1e-2\n",
        "l2factor=0\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "prediction = MaxPool(axis=1)(dense_1)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbRDRdVcYkP",
        "colab_type": "code",
        "outputId": "fab69d65-a6a7-48f0-f900-18355494f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIHWT6X6fPv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUaTaZWirf0",
        "colab_type": "code",
        "outputId": "c155dade-a1b1-43bb-cb6f-7c13d55aadec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}\n",
        "Features = []\n",
        "\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "\n",
        "logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "dense_2 = Flatten()(logistic1)\n",
        "\n",
        "logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "dense_3 = Flatten()(logistic2)\n",
        "\n",
        "logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "dense_4 = Flatten()(logistic3)\n",
        "\n",
        "final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "prediction = MaxPool(axis=1)(final)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FwRbn0eiwib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "noises=50\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "datagen.fit(X_train_extend)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bg_PaepkBhP",
        "colab_type": "code",
        "outputId": "9567289f-268b-42bd-9eda-a9d2e29f67ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.00001),\n",
        "              metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(X_train_extend, Y_train,batch_size=2),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_valid_extend, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., epochs=100)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134/134 [==============================] - 35s 264ms/step - loss: 0.6514 - accuracy: 0.7127 - val_loss: 0.8838 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.6267 - accuracy: 0.7425 - val_loss: 0.7636 - val_accuracy: 0.4333\n",
            "Epoch 3/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.6024 - accuracy: 0.7836 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.5672 - accuracy: 0.8507 - val_loss: 0.5682 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5584 - accuracy: 0.8433 - val_loss: 0.5442 - val_accuracy: 0.8167\n",
            "Epoch 6/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.5589 - accuracy: 0.8097 - val_loss: 0.5946 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5288 - accuracy: 0.8545 - val_loss: 0.5800 - val_accuracy: 0.7667\n",
            "Epoch 8/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5151 - accuracy: 0.8582 - val_loss: 0.5377 - val_accuracy: 0.8500\n",
            "Epoch 9/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5261 - accuracy: 0.8358 - val_loss: 0.5270 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5130 - accuracy: 0.8470 - val_loss: 0.5216 - val_accuracy: 0.8500\n",
            "Epoch 11/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4840 - accuracy: 0.8731 - val_loss: 0.5267 - val_accuracy: 0.8167\n",
            "Epoch 12/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4953 - accuracy: 0.8619 - val_loss: 0.5123 - val_accuracy: 0.8167\n",
            "Epoch 13/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5000 - accuracy: 0.8433 - val_loss: 0.4984 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4907 - accuracy: 0.8507 - val_loss: 0.4877 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4814 - accuracy: 0.8694 - val_loss: 0.4697 - val_accuracy: 0.8833\n",
            "Epoch 16/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4705 - accuracy: 0.8806 - val_loss: 0.4655 - val_accuracy: 0.8833\n",
            "Epoch 17/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4634 - accuracy: 0.8843 - val_loss: 0.4711 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4425 - accuracy: 0.9030 - val_loss: 0.4611 - val_accuracy: 0.8833\n",
            "Epoch 19/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4562 - accuracy: 0.8843 - val_loss: 0.4742 - val_accuracy: 0.8500\n",
            "Epoch 20/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4466 - accuracy: 0.8881 - val_loss: 0.4702 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4490 - accuracy: 0.8843 - val_loss: 0.4764 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4406 - accuracy: 0.8993 - val_loss: 0.4772 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9067 - val_loss: 0.4578 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4504 - accuracy: 0.8881 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4518 - accuracy: 0.8806 - val_loss: 0.5093 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4471 - accuracy: 0.8769 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9104 - val_loss: 0.4399 - val_accuracy: 0.9000\n",
            "Epoch 28/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4114 - accuracy: 0.9254 - val_loss: 0.4405 - val_accuracy: 0.8833\n",
            "Epoch 29/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4215 - accuracy: 0.9067 - val_loss: 0.4320 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4280 - accuracy: 0.8918 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4124 - accuracy: 0.9179 - val_loss: 0.4489 - val_accuracy: 0.8833\n",
            "Epoch 32/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4201 - accuracy: 0.9067 - val_loss: 0.4370 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4097 - accuracy: 0.9104 - val_loss: 0.4642 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4228 - accuracy: 0.8993 - val_loss: 0.4591 - val_accuracy: 0.8500\n",
            "Epoch 35/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4080 - accuracy: 0.9179 - val_loss: 0.4386 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4136 - accuracy: 0.8993 - val_loss: 0.4663 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "134/134 [==============================] - 17s 124ms/step - loss: 0.4195 - accuracy: 0.8955 - val_loss: 0.4612 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4086 - accuracy: 0.9067 - val_loss: 0.4633 - val_accuracy: 0.8333\n",
            "Epoch 39/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4060 - accuracy: 0.9179 - val_loss: 0.4412 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4010 - accuracy: 0.9142 - val_loss: 0.4987 - val_accuracy: 0.8167\n",
            "Epoch 41/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4112 - accuracy: 0.9104 - val_loss: 0.4959 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3995 - accuracy: 0.9254 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3979 - accuracy: 0.9142 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4013 - accuracy: 0.9216 - val_loss: 0.4608 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3892 - accuracy: 0.9254 - val_loss: 0.4543 - val_accuracy: 0.8833\n",
            "Epoch 46/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3986 - accuracy: 0.9216 - val_loss: 0.4524 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3882 - accuracy: 0.9328 - val_loss: 0.4398 - val_accuracy: 0.8833\n",
            "Epoch 48/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3867 - accuracy: 0.9366 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 49/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3776 - accuracy: 0.9440 - val_loss: 0.4824 - val_accuracy: 0.8333\n",
            "Epoch 50/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3888 - accuracy: 0.9291 - val_loss: 0.4923 - val_accuracy: 0.8167\n",
            "Epoch 51/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3899 - accuracy: 0.9291 - val_loss: 0.4734 - val_accuracy: 0.8333\n",
            "Epoch 52/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4009 - accuracy: 0.9179 - val_loss: 0.4710 - val_accuracy: 0.8333\n",
            "Epoch 53/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3773 - accuracy: 0.9440 - val_loss: 0.5002 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3923 - accuracy: 0.9291 - val_loss: 0.4546 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3771 - accuracy: 0.9403 - val_loss: 0.4748 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3930 - accuracy: 0.9291 - val_loss: 0.4804 - val_accuracy: 0.8167\n",
            "Epoch 57/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3837 - accuracy: 0.9366 - val_loss: 0.4670 - val_accuracy: 0.8333\n",
            "Epoch 58/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3695 - accuracy: 0.9515 - val_loss: 0.4369 - val_accuracy: 0.8833\n",
            "Epoch 59/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3830 - accuracy: 0.9328 - val_loss: 0.4340 - val_accuracy: 0.8833\n",
            "Epoch 60/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3775 - accuracy: 0.9403 - val_loss: 0.4542 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3674 - accuracy: 0.9515 - val_loss: 0.4400 - val_accuracy: 0.8833\n",
            "Epoch 62/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3747 - accuracy: 0.9403 - val_loss: 0.4501 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3693 - accuracy: 0.9478 - val_loss: 0.4781 - val_accuracy: 0.8167\n",
            "Epoch 64/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3625 - accuracy: 0.9552 - val_loss: 0.4351 - val_accuracy: 0.8833\n",
            "Epoch 65/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3838 - accuracy: 0.9291 - val_loss: 0.4327 - val_accuracy: 0.8833\n",
            "Epoch 66/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9478 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
            "Epoch 67/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9515 - val_loss: 0.4767 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9440 - val_loss: 0.4532 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3763 - accuracy: 0.9366 - val_loss: 0.4743 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3589 - accuracy: 0.9552 - val_loss: 0.4877 - val_accuracy: 0.8167\n",
            "Epoch 71/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3560 - accuracy: 0.9664 - val_loss: 0.4616 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3646 - accuracy: 0.9552 - val_loss: 0.4886 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3543 - accuracy: 0.9627 - val_loss: 0.5030 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3562 - accuracy: 0.9627 - val_loss: 0.5077 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3523 - accuracy: 0.9664 - val_loss: 0.4707 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3576 - accuracy: 0.9590 - val_loss: 0.4801 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3603 - accuracy: 0.9552 - val_loss: 0.5191 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3669 - accuracy: 0.9440 - val_loss: 0.4686 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3591 - accuracy: 0.9515 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3708 - accuracy: 0.9403 - val_loss: 0.4465 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3495 - accuracy: 0.9701 - val_loss: 0.4657 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9552 - val_loss: 0.4355 - val_accuracy: 0.8833\n",
            "Epoch 83/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3573 - accuracy: 0.9515 - val_loss: 0.4195 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3440 - accuracy: 0.9739 - val_loss: 0.4263 - val_accuracy: 0.8833\n",
            "Epoch 85/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3515 - accuracy: 0.9664 - val_loss: 0.5191 - val_accuracy: 0.7833\n",
            "Epoch 86/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9627 - val_loss: 0.4628 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3695 - accuracy: 0.9440 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3635 - accuracy: 0.9515 - val_loss: 0.4759 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3463 - accuracy: 0.9664 - val_loss: 0.5083 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3585 - accuracy: 0.9515 - val_loss: 0.4583 - val_accuracy: 0.8333\n",
            "Epoch 91/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3517 - accuracy: 0.9590 - val_loss: 0.4490 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3496 - accuracy: 0.9701 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9590 - val_loss: 0.4643 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3498 - accuracy: 0.9590 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3492 - accuracy: 0.9627 - val_loss: 0.4788 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3438 - accuracy: 0.9664 - val_loss: 0.4847 - val_accuracy: 0.8167\n",
            "Epoch 97/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3446 - accuracy: 0.9739 - val_loss: 0.5205 - val_accuracy: 0.7833\n",
            "Epoch 98/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.3442 - accuracy: 0.9664 - val_loss: 0.4866 - val_accuracy: 0.8167\n",
            "Epoch 99/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3369 - accuracy: 0.9813 - val_loss: 0.4866 - val_accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3466 - accuracy: 0.9701 - val_loss: 0.4513 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb84798d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeGlE_Oz-RjS",
        "colab_type": "code",
        "outputId": "f36f1173-8550-4982-e5fe-20d4af123438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = model5()\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 64, 64, 1)    257         conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 32, 32, 1)    513         conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 1)    1025        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 1)      2049        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_53 (Flatten)            (None, 4096)         0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_54 (Flatten)            (None, 1024)         0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_55 (Flatten)            (None, 256)          0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_56 (Flatten)            (None, 64)           0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 5440)         0           flatten_53[0][0]                 \n",
            "                                                                 flatten_54[0][0]                 \n",
            "                                                                 flatten_55[0][0]                 \n",
            "                                                                 flatten_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_14 (MaxPool)           (None, 2)            0           concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 2)            0           max_pool_14[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 23,591,556\n",
            "Trainable params: 23,538,436\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfcXjhymHnt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
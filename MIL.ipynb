{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "MIL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravitha/LabelRadiology/blob/master/MIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arW_6_PrkIlZ",
        "colab_type": "text"
      },
      "source": [
        "# Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthF7sh_UZX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dicom # some machines not install pydicom\n",
        "import scipy.misc\n",
        "import numpy as np \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pickle as cPickle\n",
        "#import matplotlib\n",
        "#import matplotlib.pyplot as plt \n",
        "from skimage.filters import threshold_otsu\n",
        "import os\n",
        "from os.path import join as join\n",
        "import csv\n",
        "import scipy.ndimage\n",
        "#import pydicom as dicom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82DTupaeUZYI",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evR_WImPaUN0",
        "colab_type": "code",
        "outputId": "c902fba3-0a72-4cd0-a832-f62f5c2abdc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcQkq4ShkC-6",
        "colab_type": "text"
      },
      "source": [
        "# Compose Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvzRMqmRi0fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/drive/My Drive/CO-PO/INBreast'\n",
        "TrDataset = np.load(join(path,'Train.npy'))\n",
        "TrLabel = np.load(join(path,'TrainL.npy'))\n",
        "ValDataset= np.load(join(path,'valid.npy'))\n",
        "ValLabel = np.load(join(path,'validL.npy'))\n",
        "TDataset = np.load(join(path,'Test.npy'))\n",
        "TLabel = np.load(join(path,'TestL.npy'))\n",
        "\n",
        "Dataset = np.concatenate((TrDataset,ValDataset,TDataset),axis = 0)\n",
        "Labels = np.concatenate((TrLabel,ValLabel,TLabel),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRB7_brfjY3g",
        "colab_type": "code",
        "outputId": "6797f0e9-9199-417b-b7d0-78f6ac91001c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)\n",
        "np.save('Dataset.npy', Dataset)\n",
        "np.save('Label.npy', Labels)\n",
        "\n",
        "np.unique(Labels, return_counts=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256)\n",
            "(410,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.]), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crX86A3FUZZI",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXWpwOAkOVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "nb_classes = 2\n",
        "Dataset = Dataset.reshape((410,256,256,1))\n",
        "Dataset_extend = np.zeros((Dataset.shape[0],256, 256,3))\n",
        "for i in range(Dataset.shape[0]):\n",
        "    rex = np.resize(Dataset[i,:,:,:], (256, 256))\n",
        "    Dataset_extend[i,:,:,0] = rex\n",
        "    Dataset_extend[i,:,:,1] = rex\n",
        "    Dataset_extend[i,:,:,2] = rex\n",
        "Dataset = Dataset_extend\n",
        "Labels = np_utils.to_categorical(Labels, nb_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqsOUAXIlSL1",
        "colab_type": "text"
      },
      "source": [
        "# Print Data Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNitNFUBlQXT",
        "colab_type": "code",
        "outputId": "596839b5-8852-4d3c-976e-71545e345296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(Dataset.shape)\n",
        "print(Labels.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(410, 256, 256, 3)\n",
            "(410, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw_WtGDWTeiI",
        "colab_type": "code",
        "outputId": "92d5db44-cdf3-47f3-9537-121012739e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.unique(Labels[:,1],return_counts = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.], dtype=float32), array([310, 100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR4qn6IkmSC1",
        "colab_type": "text"
      },
      "source": [
        "# Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKoXcmbmUCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model1():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_PMVBLi_l1q",
        "colab_type": "text"
      },
      "source": [
        "## Using K Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSm1LGMzKbeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class KPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(KPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],64))\n",
        "        #output = K.max(x, axis=-1,keepdims=True)\n",
        "        output = tf.sort(x)\n",
        "        output=K.concatenate([1-output[:,:-2], output[:,62:64]])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 64)\n",
        "\n",
        "def model3():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = KPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXNmazY__4v4",
        "colab_type": "text"
      },
      "source": [
        "## Use Sparsity Constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN0ILfCIiRLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.regularizers import l1\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1, **kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        max_val = K.max(x, axis=-1,keepdims=True)\n",
        "        self.add_loss(l1(1e-5)(x))\n",
        "        output=K.concatenate([1-max_val,max_val])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model4():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "  prediction = MaxPool(axis=1)(dense_1)\n",
        "  #dense_3 = Dense(2,name='dense_3',W_regularizer=l1_l2(l1=l1factor, l2=l2factor))(dense_1)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9SHG1mD_a08",
        "colab_type": "text"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aLHg7YfSOZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Lambda\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "\n",
        "def model2():\n",
        "\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  \n",
        "  out1 = Lambda(lambda image: tf.image.resize(image, (4, 4)))(model.output)\n",
        "  out2 = Lambda(lambda image: tf.image.resize(image, (2, 2)))(model.output)\n",
        "  out3 = Lambda(lambda image: tf.image.resize(image, (6, 6)))(model.output)\n",
        "\n",
        "  SL1 = Conv2D(1, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "  SL2 = BatchNormalization()\n",
        "  SL3 = Conv2D(1, 1, activation = None, padding = 'same', kernel_initializer = 'he_normal')\n",
        "\n",
        "  resize1 = Flatten()(SL3(SL2(SL1(out1))))\n",
        "  resize2 = Flatten()(SL3(SL2(SL1(out2))))\n",
        "  resize3 = Flatten()(SL3(SL2(SL1(out3))))\n",
        "\n",
        "  M1 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize1)\n",
        "  M2 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize2)\n",
        "  M3 = Lambda(lambda image: tf.keras.backend.max(image,axis=-1,keepdims=True))(resize3)\n",
        "\n",
        "  added10 = concatenate([M1,M2,M3], axis=1)\n",
        "  final = Dense(2, activation='softmax')(added10)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIfSI5K___Z6",
        "colab_type": "text"
      },
      "source": [
        "# Model 3 (ResNet Multiscale)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rym-sZ_dxvp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten, Lambda\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "def model5():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "\n",
        "  logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "  dense_2 = Flatten()(logistic1)\n",
        "\n",
        "  logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "  dense_3 = Flatten()(logistic2)\n",
        "\n",
        "  logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "  dense_4 = Flatten()(logistic3)\n",
        "\n",
        "\n",
        "  final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "  prediction = MaxPool(axis=1)(final)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTMerT0GAKAF",
        "colab_type": "text"
      },
      "source": [
        "## ResNet 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K922EQst61r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model6():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  dense = Flatten()(model.output)\n",
        "  final = Dense(2, activation='softmax')(dense)\n",
        "  model = Model(input=model.input, output=final)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7mepqnFBBQl",
        "colab_type": "text"
      },
      "source": [
        "# Model 4 (Attention with ResNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-O8n7VBBJor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten,UpSampling2D, Multiply\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "def model6():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  G = model.get_layer('conv5_block3_out').output\n",
        "  G = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(G)\n",
        "  G_flatten = Flatten()(G)\n",
        "  L1 = model.get_layer('conv4_block6_out').output\n",
        "  L1 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L1)\n",
        "  G_modified = UpSampling2D(size = (2,2), interpolation='bilinear')(G)\n",
        "  L1_modified = Multiply()([L1,G_modified])\n",
        "  L1_modified = Activation('softmax')(L1_modified)\n",
        "  L1_modified = Multiply()([L1_modified,G_modified])\n",
        "  L1_modified = Flatten()(L1_modified)\n",
        "\n",
        "  L2 = model.get_layer('conv3_block4_out').output\n",
        "  L2 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L2)\n",
        "  G_modified = UpSampling2D(size = (4,4), interpolation='bilinear')(G)\n",
        "  L2_modified = Multiply()([L2,G_modified])\n",
        "  L2_modified = Activation('softmax')(L2_modified)\n",
        "  L2_modified = Multiply()([L2_modified,G_modified])\n",
        "  L2_modified = Flatten()(L2_modified)\n",
        "\n",
        "  L3 = model.get_layer('conv2_block3_out').output\n",
        "  L3 = Conv2D(1,1,activation = 'sigmoid', padding = 'same')(L3)\n",
        "  G_modified = UpSampling2D(size = (8,8), interpolation='bilinear')(G)\n",
        "  L3_modified = Multiply()([L3,G_modified])\n",
        "  L3_modified = Activation('softmax')(L3_modified)\n",
        "  L3_modified = Multiply()([L3_modified,G_modified])\n",
        "  L3_modified = Flatten()(L3_modified)\n",
        "\n",
        "  final = concatenate([L1_modified, L2_modified,L3_modified])\n",
        "  prediction = MaxPool(axis=1)(final)\n",
        "  prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "  model = Model(input=model.input, output=prediction)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_sGFuWPLRUp",
        "colab_type": "text"
      },
      "source": [
        "# Multi Scale ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCWu0I9OK_RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "class MaxPool1(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool1, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "def model7():\n",
        "  model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "  Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}  \n",
        "\n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "  dense_1 = Flatten()(logistic)\n",
        "\n",
        "  logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "  dense_2 = Flatten()(logistic1)\n",
        "\n",
        "  logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "  dense_3 = Flatten()(logistic2)\n",
        "\n",
        "  logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "  dense_4 = Flatten()(logistic3)\n",
        "\n",
        "  model1 = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(128,128,3))\n",
        "  for layer in model1.layers:\n",
        "      layer.trainable =False\n",
        "      layer.name = '1_' + layer.name\n",
        "  \n",
        "  logistic = Conv2D(1, 1, activation = 'sigmoid')(model1.get_layer('1_conv2_block3_out').output)\n",
        "  dense_1a = Flatten()(logistic)\n",
        "\n",
        "  logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model1.get_layer('1_conv3_block4_out').output)\n",
        "  dense_2a = Flatten()(logistic1)\n",
        "\n",
        "  logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model1.get_layer('1_conv4_block6_out').output)\n",
        "  dense_3a = Flatten()(logistic2)\n",
        "\n",
        "  logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model1.get_layer('1_conv5_block3_out').output)\n",
        "  dense_4a = Flatten()(logistic3)\n",
        "\n",
        "  final1 = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "  final2 = concatenate([dense_1a,dense_2a,dense_3a,dense_4a])\n",
        "  prediction1 = MaxPool(axis=1)(final1)\n",
        "  prediction2 = MaxPool1(axis=1)(final2)\n",
        " \n",
        "  final = Add()([prediction1,prediction2])\n",
        "  #final = Dense(2, activation='softmax')(pred)\n",
        "  final= Activation(\"softmax\",name=\"softmax\")(final)\n",
        "  model3 = Model(input=[model.input, model1.input], output=final)\n",
        "  return model3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssCxcR7lutw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzFdleZCluLO",
        "colab_type": "code",
        "outputId": "b5a8a0b7-cf19-4c89-b10d-fbd40353a3bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Necessary Imports\n",
        "import numpy as np \n",
        "from sklearn.model_selection import KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam\n",
        "from skimage.transform import resize\n",
        "print('[INFO:Applying CV]')\n",
        "skf = KFold(n_splits=5)\n",
        "skf.get_n_splits(Dataset)\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(Dataset):\n",
        "    print(['FOLD:'], fold)\n",
        "    # Construct data from indexes\n",
        "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
        "    y_train, y_test = Labels[train_index], Labels[test_index]\n",
        "    \n",
        "    # Model Construction\n",
        "    model = model7()\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "    datagen.fit(X_train)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "    Stopping_Criterion = EarlyStopping(patience=50, verbose=1)\n",
        "    model_checkpoint = ModelCheckpoint('./SEfold'+str(fold)+'{epoch:08d}{val_accuracy:05f}.hdf5', monitor='val_accuracy',verbose=1,save_best_only=True)\n",
        "    X_train_resized = resize(X_train, (X_train.shape[0] , X_train.shape[1] // 2,X_train.shape[2] // 2,X_train.shape[3]),\n",
        "                       anti_aliasing=True)\n",
        "    X_test_resized = resize(X_train, (X_test.shape[0] , X_test.shape[1] // 2,X_test.shape[2] // 2,X_test.shape[3]),\n",
        "                       anti_aliasing=True)\n",
        "    model.fit_generator(datagen.flow([X_train, X_train_resized], y_train,batch_size=4),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=([X_test, X_test_resized] , y_test ),callbacks=[model_checkpoint, Stopping_Criterion])\n",
        "    datafilename = 'TestFold'+str(fold)+'.npy'\n",
        "    labelfilename = 'TestLabelFold'+str(fold)+'.npy'\n",
        "    np.save(datafilename,X_test)\n",
        "    np.save(labelfilename,y_test)\n",
        "    fold+=1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO:Applying CV]\n",
            "['FOLD:'] 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"so...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=([array([[..., callbacks=[<keras.ca..., epochs=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "82/82 [==============================] - 43s 528ms/step - loss: 1.5119 - accuracy: 0.2439 - val_loss: 1.2303 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold1000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.8784 - accuracy: 0.3750 - val_loss: 1.1738 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.5971 - accuracy: 0.7439 - val_loss: 0.6199 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.24390 to 0.75610, saving model to ./SEfold1000000030.756098.hdf5\n",
            "Epoch 4/100\n",
            "82/82 [==============================] - 23s 274ms/step - loss: 0.5600 - accuracy: 0.7530 - val_loss: 0.5983 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.5516 - accuracy: 0.7439 - val_loss: 0.5786 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.5393 - accuracy: 0.7713 - val_loss: 0.5839 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.5238 - accuracy: 0.7866 - val_loss: 0.5879 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4941 - accuracy: 0.7713 - val_loss: 0.5840 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4869 - accuracy: 0.7957 - val_loss: 0.5836 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.5028 - accuracy: 0.7774 - val_loss: 0.5825 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4740 - accuracy: 0.8110 - val_loss: 0.5746 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4911 - accuracy: 0.7866 - val_loss: 0.5723 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4733 - accuracy: 0.7835 - val_loss: 0.5668 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4339 - accuracy: 0.8323 - val_loss: 0.5570 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4651 - accuracy: 0.8079 - val_loss: 0.5478 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.75610 to 0.76829, saving model to ./SEfold1000000150.768293.hdf5\n",
            "Epoch 16/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4639 - accuracy: 0.8018 - val_loss: 0.5547 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.76829\n",
            "Epoch 17/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4415 - accuracy: 0.8232 - val_loss: 0.4887 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.76829 to 0.81707, saving model to ./SEfold1000000170.817073.hdf5\n",
            "Epoch 18/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4367 - accuracy: 0.8140 - val_loss: 0.8711 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.81707\n",
            "Epoch 19/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4145 - accuracy: 0.8506 - val_loss: 0.7073 - val_accuracy: 0.4878\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.81707\n",
            "Epoch 20/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4390 - accuracy: 0.8293 - val_loss: 0.5291 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.81707\n",
            "Epoch 21/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4041 - accuracy: 0.8476 - val_loss: 0.4426 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.81707 to 0.86585, saving model to ./SEfold1000000210.865854.hdf5\n",
            "Epoch 22/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4852 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.86585\n",
            "Epoch 23/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3989 - accuracy: 0.8567 - val_loss: 0.4770 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.86585\n",
            "Epoch 24/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4073 - accuracy: 0.8384 - val_loss: 0.4612 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.86585\n",
            "Epoch 25/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4246 - accuracy: 0.8201 - val_loss: 0.4911 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.86585\n",
            "Epoch 26/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3747 - accuracy: 0.8872 - val_loss: 0.5022 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.86585\n",
            "Epoch 27/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3907 - accuracy: 0.8689 - val_loss: 0.4553 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.86585\n",
            "Epoch 28/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3862 - accuracy: 0.8537 - val_loss: 0.5255 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.86585\n",
            "Epoch 29/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3873 - accuracy: 0.8659 - val_loss: 0.4946 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.86585\n",
            "Epoch 30/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3699 - accuracy: 0.8780 - val_loss: 0.4774 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.86585\n",
            "Epoch 31/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.4032 - accuracy: 0.8476 - val_loss: 0.4256 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.86585 to 0.89024, saving model to ./SEfold1000000310.890244.hdf5\n",
            "Epoch 32/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3624 - accuracy: 0.8689 - val_loss: 0.4630 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89024\n",
            "Epoch 33/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3736 - accuracy: 0.8780 - val_loss: 0.4603 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89024\n",
            "Epoch 34/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3872 - accuracy: 0.8689 - val_loss: 0.6455 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89024\n",
            "Epoch 35/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3460 - accuracy: 0.8841 - val_loss: 0.4259 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89024\n",
            "Epoch 36/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3617 - accuracy: 0.8872 - val_loss: 0.4182 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89024\n",
            "Epoch 37/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3517 - accuracy: 0.8902 - val_loss: 0.4371 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89024\n",
            "Epoch 38/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3481 - accuracy: 0.8902 - val_loss: 0.5112 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89024\n",
            "Epoch 39/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3593 - accuracy: 0.8750 - val_loss: 0.4412 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89024\n",
            "Epoch 40/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3461 - accuracy: 0.8902 - val_loss: 0.5049 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89024\n",
            "Epoch 41/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3299 - accuracy: 0.9146 - val_loss: 0.8493 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89024\n",
            "Epoch 42/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3643 - accuracy: 0.8811 - val_loss: 0.5892 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89024\n",
            "Epoch 43/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3225 - accuracy: 0.9177 - val_loss: 0.5014 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89024\n",
            "Epoch 44/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3388 - accuracy: 0.8902 - val_loss: 0.4441 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89024\n",
            "Epoch 45/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3438 - accuracy: 0.8780 - val_loss: 0.4559 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89024\n",
            "Epoch 46/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3408 - accuracy: 0.8872 - val_loss: 0.5357 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89024\n",
            "Epoch 47/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3600 - accuracy: 0.8902 - val_loss: 0.4645 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89024\n",
            "Epoch 48/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3294 - accuracy: 0.8963 - val_loss: 0.4430 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89024\n",
            "Epoch 49/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3195 - accuracy: 0.9085 - val_loss: 0.5422 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89024\n",
            "Epoch 50/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3252 - accuracy: 0.8963 - val_loss: 0.6029 - val_accuracy: 0.6463\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89024\n",
            "Epoch 51/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3374 - accuracy: 0.9055 - val_loss: 0.4072 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89024\n",
            "Epoch 52/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3066 - accuracy: 0.9177 - val_loss: 0.6665 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89024\n",
            "Epoch 53/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3149 - accuracy: 0.9055 - val_loss: 0.4160 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89024\n",
            "Epoch 54/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3114 - accuracy: 0.9207 - val_loss: 0.4981 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89024\n",
            "Epoch 55/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3259 - accuracy: 0.8994 - val_loss: 0.6921 - val_accuracy: 0.4512\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89024\n",
            "Epoch 56/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3182 - accuracy: 0.8994 - val_loss: 0.6272 - val_accuracy: 0.5610\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89024\n",
            "Epoch 57/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3214 - accuracy: 0.9055 - val_loss: 0.4913 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89024\n",
            "Epoch 58/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3036 - accuracy: 0.9055 - val_loss: 0.6383 - val_accuracy: 0.5854\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89024\n",
            "Epoch 59/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3201 - accuracy: 0.8933 - val_loss: 0.5308 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89024\n",
            "Epoch 60/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3170 - accuracy: 0.8933 - val_loss: 0.4632 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89024\n",
            "Epoch 61/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3089 - accuracy: 0.9055 - val_loss: 0.4692 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89024\n",
            "Epoch 62/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3107 - accuracy: 0.9177 - val_loss: 0.4356 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89024\n",
            "Epoch 63/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2836 - accuracy: 0.9329 - val_loss: 0.4304 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89024\n",
            "Epoch 64/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2895 - accuracy: 0.9329 - val_loss: 0.4297 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89024\n",
            "Epoch 65/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2921 - accuracy: 0.9116 - val_loss: 0.6296 - val_accuracy: 0.5732\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.89024\n",
            "Epoch 66/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2956 - accuracy: 0.9207 - val_loss: 0.5911 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.89024\n",
            "Epoch 67/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3029 - accuracy: 0.9116 - val_loss: 0.5032 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.89024\n",
            "Epoch 68/100\n",
            "82/82 [==============================] - 22s 270ms/step - loss: 0.2934 - accuracy: 0.9177 - val_loss: 0.4523 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.89024\n",
            "Epoch 69/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2957 - accuracy: 0.9238 - val_loss: 0.4211 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.89024\n",
            "Epoch 70/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3075 - accuracy: 0.9055 - val_loss: 0.4628 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.89024\n",
            "Epoch 71/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.3007 - accuracy: 0.9024 - val_loss: 0.5699 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.89024\n",
            "Epoch 72/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2853 - accuracy: 0.9238 - val_loss: 0.5124 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.89024\n",
            "Epoch 73/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2872 - accuracy: 0.9177 - val_loss: 0.4023 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.89024\n",
            "Epoch 74/100\n",
            "82/82 [==============================] - 22s 270ms/step - loss: 0.2903 - accuracy: 0.9146 - val_loss: 0.4115 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.89024\n",
            "Epoch 75/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2867 - accuracy: 0.9207 - val_loss: 0.4261 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.89024\n",
            "Epoch 76/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2731 - accuracy: 0.9268 - val_loss: 0.4501 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.89024\n",
            "Epoch 77/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2618 - accuracy: 0.9390 - val_loss: 0.4204 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.89024\n",
            "Epoch 78/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2659 - accuracy: 0.9451 - val_loss: 0.4118 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.89024\n",
            "Epoch 79/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2597 - accuracy: 0.9451 - val_loss: 0.3991 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.89024\n",
            "Epoch 80/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2856 - accuracy: 0.9238 - val_loss: 0.4145 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.89024\n",
            "Epoch 81/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2896 - accuracy: 0.9085 - val_loss: 0.3876 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.89024\n",
            "Epoch 82/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2821 - accuracy: 0.9207 - val_loss: 0.6790 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.89024\n",
            "Epoch 83/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2603 - accuracy: 0.9421 - val_loss: 0.4190 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.89024\n",
            "Epoch 84/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2704 - accuracy: 0.9268 - val_loss: 0.4417 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.89024\n",
            "Epoch 85/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2726 - accuracy: 0.9268 - val_loss: 0.4020 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.89024\n",
            "Epoch 86/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2581 - accuracy: 0.9390 - val_loss: 0.4520 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.89024\n",
            "Epoch 87/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2590 - accuracy: 0.9390 - val_loss: 0.4586 - val_accuracy: 0.8537\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.89024\n",
            "Epoch 88/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2466 - accuracy: 0.9482 - val_loss: 0.5250 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.89024\n",
            "Epoch 89/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2833 - accuracy: 0.9238 - val_loss: 0.5418 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.89024\n",
            "Epoch 90/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2652 - accuracy: 0.9268 - val_loss: 0.4667 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.89024\n",
            "Epoch 91/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2604 - accuracy: 0.9421 - val_loss: 0.4045 - val_accuracy: 0.8780\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.89024\n",
            "Epoch 92/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.2716 - accuracy: 0.9268 - val_loss: 0.4515 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.89024\n",
            "Epoch 93/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2570 - accuracy: 0.9512 - val_loss: 0.5525 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.89024\n",
            "Epoch 94/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2693 - accuracy: 0.9299 - val_loss: 0.5400 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.89024\n",
            "Epoch 95/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2621 - accuracy: 0.9360 - val_loss: 0.4083 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.89024\n",
            "Epoch 96/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2574 - accuracy: 0.9329 - val_loss: 0.4059 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.89024\n",
            "Epoch 97/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2363 - accuracy: 0.9543 - val_loss: 0.5137 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.89024\n",
            "Epoch 98/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2597 - accuracy: 0.9360 - val_loss: 0.4235 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.89024\n",
            "Epoch 99/100\n",
            "82/82 [==============================] - 22s 271ms/step - loss: 0.2557 - accuracy: 0.9390 - val_loss: 0.4986 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.89024\n",
            "Epoch 100/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.2589 - accuracy: 0.9329 - val_loss: 0.4075 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.89024\n",
            "['FOLD:'] 2\n",
            "Epoch 1/100\n",
            "82/82 [==============================] - 40s 490ms/step - loss: 1.5771 - accuracy: 0.2439 - val_loss: 1.3532 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold2000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 1.1497 - accuracy: 0.2805 - val_loss: 1.1246 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.7777 - accuracy: 0.5457 - val_loss: 1.0903 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.24390\n",
            "Epoch 4/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.6884 - accuracy: 0.5793 - val_loss: 1.0640 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.24390\n",
            "Epoch 5/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.6681 - accuracy: 0.5366 - val_loss: 0.5825 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.24390 to 0.75610, saving model to ./SEfold2000000050.756098.hdf5\n",
            "Epoch 6/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.6879 - accuracy: 0.4573 - val_loss: 0.5729 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.6903 - accuracy: 0.4543 - val_loss: 0.5782 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.6162 - accuracy: 0.5884 - val_loss: 0.6015 - val_accuracy: 0.7073\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.6043 - accuracy: 0.6250 - val_loss: 0.9540 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "82/82 [==============================] - 23s 274ms/step - loss: 0.6105 - accuracy: 0.5488 - val_loss: 0.6028 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.6028 - accuracy: 0.6280 - val_loss: 0.6053 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5412 - accuracy: 0.7530 - val_loss: 0.6220 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.4964 - accuracy: 0.7988 - val_loss: 0.6199 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.4763 - accuracy: 0.8171 - val_loss: 0.6109 - val_accuracy: 0.7317\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.5017 - accuracy: 0.7378 - val_loss: 0.8200 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.5225 - accuracy: 0.7134 - val_loss: 0.5988 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.4765 - accuracy: 0.8018 - val_loss: 0.5617 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.4771 - accuracy: 0.7988 - val_loss: 0.5570 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.75610 to 0.76829, saving model to ./SEfold2000000180.768293.hdf5\n",
            "Epoch 19/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.4487 - accuracy: 0.8262 - val_loss: 0.5266 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.76829 to 0.79268, saving model to ./SEfold2000000190.792683.hdf5\n",
            "Epoch 20/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.4310 - accuracy: 0.8384 - val_loss: 0.5149 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.79268 to 0.80488, saving model to ./SEfold2000000200.804878.hdf5\n",
            "Epoch 21/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.4449 - accuracy: 0.8293 - val_loss: 0.5056 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.80488\n",
            "Epoch 22/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.5264 - accuracy: 0.6677 - val_loss: 0.8405 - val_accuracy: 0.4146\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.80488\n",
            "Epoch 23/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.5549 - accuracy: 0.6311 - val_loss: 0.8395 - val_accuracy: 0.4024\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.80488\n",
            "Epoch 24/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.5284 - accuracy: 0.6860 - val_loss: 0.5501 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.80488\n",
            "Epoch 25/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.4528 - accuracy: 0.8384 - val_loss: 0.5412 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.80488\n",
            "Epoch 26/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.4372 - accuracy: 0.8506 - val_loss: 0.5090 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.80488 to 0.81707, saving model to ./SEfold2000000260.817073.hdf5\n",
            "Epoch 27/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4761 - accuracy: 0.7470 - val_loss: 0.5527 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.81707\n",
            "Epoch 28/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4589 - accuracy: 0.7927 - val_loss: 0.5447 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.81707\n",
            "Epoch 29/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4578 - accuracy: 0.7896 - val_loss: 0.5477 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.81707\n",
            "Epoch 30/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4222 - accuracy: 0.8354 - val_loss: 0.5310 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.81707\n",
            "Epoch 31/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4079 - accuracy: 0.8415 - val_loss: 0.4929 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.81707\n",
            "Epoch 32/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4126 - accuracy: 0.8415 - val_loss: 0.5237 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.81707\n",
            "Epoch 33/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4055 - accuracy: 0.8506 - val_loss: 0.4984 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.81707\n",
            "Epoch 34/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4200 - accuracy: 0.8354 - val_loss: 0.5397 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.81707\n",
            "Epoch 35/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.5275 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.81707\n",
            "Epoch 36/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4058 - accuracy: 0.8415 - val_loss: 0.5001 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.81707\n",
            "Epoch 37/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4038 - accuracy: 0.8506 - val_loss: 0.4690 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.81707\n",
            "Epoch 38/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3838 - accuracy: 0.8659 - val_loss: 0.5240 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.81707\n",
            "Epoch 39/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3877 - accuracy: 0.8689 - val_loss: 0.5208 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.81707\n",
            "Epoch 40/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3709 - accuracy: 0.8750 - val_loss: 0.4917 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.81707\n",
            "Epoch 41/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3582 - accuracy: 0.8780 - val_loss: 0.5431 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.81707\n",
            "Epoch 42/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3754 - accuracy: 0.8720 - val_loss: 0.5399 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.81707\n",
            "Epoch 43/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3693 - accuracy: 0.8689 - val_loss: 0.5004 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.81707\n",
            "Epoch 44/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.4161 - accuracy: 0.8018 - val_loss: 0.5169 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.81707\n",
            "Epoch 45/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3836 - accuracy: 0.8567 - val_loss: 0.5149 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.81707\n",
            "Epoch 46/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3963 - accuracy: 0.8445 - val_loss: 0.5261 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.81707\n",
            "Epoch 47/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3723 - accuracy: 0.8537 - val_loss: 0.5245 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.81707\n",
            "Epoch 48/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3723 - accuracy: 0.8567 - val_loss: 0.5223 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.81707\n",
            "Epoch 49/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3490 - accuracy: 0.8872 - val_loss: 0.5373 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.81707\n",
            "Epoch 50/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3554 - accuracy: 0.8872 - val_loss: 0.4963 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.81707\n",
            "Epoch 51/100\n",
            "82/82 [==============================] - 22s 272ms/step - loss: 0.3352 - accuracy: 0.9024 - val_loss: 0.5286 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.81707\n",
            "Epoch 52/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3640 - accuracy: 0.8750 - val_loss: 0.5072 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.81707\n",
            "Epoch 53/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3425 - accuracy: 0.8902 - val_loss: 0.5117 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.81707\n",
            "Epoch 54/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3545 - accuracy: 0.8628 - val_loss: 0.5915 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.81707\n",
            "Epoch 55/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3517 - accuracy: 0.8659 - val_loss: 0.5950 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.81707\n",
            "Epoch 56/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.3765 - accuracy: 0.8323 - val_loss: 0.5237 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.81707\n",
            "Epoch 57/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3577 - accuracy: 0.8659 - val_loss: 0.5275 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.81707\n",
            "Epoch 58/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3545 - accuracy: 0.8720 - val_loss: 0.5668 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.81707\n",
            "Epoch 59/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3838 - accuracy: 0.8537 - val_loss: 0.5241 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.81707\n",
            "Epoch 60/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3359 - accuracy: 0.8872 - val_loss: 0.5273 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.81707\n",
            "Epoch 61/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.3749 - accuracy: 0.8567 - val_loss: 0.4901 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.81707\n",
            "Epoch 62/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3843 - accuracy: 0.8476 - val_loss: 0.4707 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.81707\n",
            "Epoch 63/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3628 - accuracy: 0.8476 - val_loss: 0.5438 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.81707\n",
            "Epoch 64/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3652 - accuracy: 0.8598 - val_loss: 0.5402 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.81707\n",
            "Epoch 65/100\n",
            "82/82 [==============================] - 22s 273ms/step - loss: 0.3725 - accuracy: 0.8506 - val_loss: 0.5000 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.81707\n",
            "Epoch 66/100\n",
            "82/82 [==============================] - 23s 274ms/step - loss: 0.3644 - accuracy: 0.8445 - val_loss: 0.4932 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.81707\n",
            "Epoch 67/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.3419 - accuracy: 0.8750 - val_loss: 0.5175 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.81707\n",
            "Epoch 68/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.3377 - accuracy: 0.8841 - val_loss: 0.5555 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.81707\n",
            "Epoch 69/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.3651 - accuracy: 0.8445 - val_loss: 0.5493 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.81707\n",
            "Epoch 70/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.3399 - accuracy: 0.8902 - val_loss: 0.5189 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.81707\n",
            "Epoch 71/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.3012 - accuracy: 0.9177 - val_loss: 0.5173 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.81707\n",
            "Epoch 72/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.3193 - accuracy: 0.8902 - val_loss: 0.5424 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.81707\n",
            "Epoch 73/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.3224 - accuracy: 0.8933 - val_loss: 0.5150 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.81707\n",
            "Epoch 74/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.3357 - accuracy: 0.8811 - val_loss: 0.5317 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.81707\n",
            "Epoch 75/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.3573 - accuracy: 0.8537 - val_loss: 0.5289 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.81707\n",
            "Epoch 76/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.3446 - accuracy: 0.8720 - val_loss: 0.4954 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.81707\n",
            "Epoch 77/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.3346 - accuracy: 0.8750 - val_loss: 0.5447 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.81707\n",
            "Epoch 78/100\n",
            "82/82 [==============================] - 23s 275ms/step - loss: 0.3252 - accuracy: 0.8750 - val_loss: 0.5260 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.81707\n",
            "Epoch 79/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.3226 - accuracy: 0.8841 - val_loss: 0.5535 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.81707\n",
            "Epoch 80/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.3908 - accuracy: 0.8079 - val_loss: 0.5300 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.81707\n",
            "Epoch 81/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.3476 - accuracy: 0.8659 - val_loss: 0.5421 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.81707\n",
            "Epoch 82/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.3357 - accuracy: 0.8811 - val_loss: 0.5295 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.81707\n",
            "Epoch 83/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.3442 - accuracy: 0.8567 - val_loss: 0.5366 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.81707\n",
            "Epoch 84/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.3188 - accuracy: 0.8933 - val_loss: 0.5383 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.81707\n",
            "Epoch 85/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.3750 - accuracy: 0.8323 - val_loss: 0.5065 - val_accuracy: 0.7927\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.81707\n",
            "Epoch 86/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.3187 - accuracy: 0.9055 - val_loss: 0.5030 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.81707\n",
            "Epoch 87/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.3345 - accuracy: 0.8933 - val_loss: 0.4957 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.81707\n",
            "Epoch 00087: early stopping\n",
            "['FOLD:'] 3\n",
            "Epoch 1/100\n",
            "82/82 [==============================] - 42s 514ms/step - loss: 1.5855 - accuracy: 0.2439 - val_loss: 1.6358 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.24390, saving model to ./SEfold3000000010.243902.hdf5\n",
            "Epoch 2/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 1.2569 - accuracy: 0.2500 - val_loss: 1.3181 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.24390\n",
            "Epoch 3/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.8367 - accuracy: 0.4878 - val_loss: 0.6248 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.24390 to 0.75610, saving model to ./SEfold3000000030.756098.hdf5\n",
            "Epoch 4/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.6854 - accuracy: 0.5762 - val_loss: 0.5611 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.75610\n",
            "Epoch 5/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.6360 - accuracy: 0.6067 - val_loss: 0.5660 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.75610\n",
            "Epoch 6/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.6058 - accuracy: 0.6067 - val_loss: 0.5673 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.75610\n",
            "Epoch 7/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.6133 - accuracy: 0.6006 - val_loss: 0.5602 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.75610\n",
            "Epoch 8/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.6024 - accuracy: 0.5823 - val_loss: 0.5566 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.75610\n",
            "Epoch 9/100\n",
            "82/82 [==============================] - 23s 279ms/step - loss: 0.5935 - accuracy: 0.5884 - val_loss: 0.5546 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.75610\n",
            "Epoch 10/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.6054 - accuracy: 0.5854 - val_loss: 0.5573 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.75610\n",
            "Epoch 11/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5888 - accuracy: 0.5488 - val_loss: 0.5552 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.75610\n",
            "Epoch 12/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5940 - accuracy: 0.5335 - val_loss: 0.5487 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75610\n",
            "Epoch 13/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.6096 - accuracy: 0.5305 - val_loss: 0.5409 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.75610\n",
            "Epoch 14/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5973 - accuracy: 0.5549 - val_loss: 0.5531 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.75610\n",
            "Epoch 15/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5566 - accuracy: 0.6372 - val_loss: 0.6701 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.75610\n",
            "Epoch 16/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.6116 - accuracy: 0.5366 - val_loss: 0.6901 - val_accuracy: 0.5122\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.75610\n",
            "Epoch 17/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.6079 - accuracy: 0.5274 - val_loss: 0.6654 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.75610\n",
            "Epoch 18/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5615 - accuracy: 0.6616 - val_loss: 0.6636 - val_accuracy: 0.5610\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.75610\n",
            "Epoch 19/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5287 - accuracy: 0.6829 - val_loss: 0.5767 - val_accuracy: 0.7195\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.75610\n",
            "Epoch 20/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5571 - accuracy: 0.6250 - val_loss: 0.5317 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.75610 to 0.78049, saving model to ./SEfold3000000200.780488.hdf5\n",
            "Epoch 21/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.6202 - accuracy: 0.5152 - val_loss: 0.5473 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.78049\n",
            "Epoch 22/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5788 - accuracy: 0.5823 - val_loss: 0.8303 - val_accuracy: 0.2439\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.78049\n",
            "Epoch 23/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5909 - accuracy: 0.5610 - val_loss: 0.6167 - val_accuracy: 0.6585\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.78049\n",
            "Epoch 24/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5796 - accuracy: 0.6006 - val_loss: 0.6141 - val_accuracy: 0.6341\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.78049\n",
            "Epoch 25/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5559 - accuracy: 0.6463 - val_loss: 0.8162 - val_accuracy: 0.2561\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.78049\n",
            "Epoch 26/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5513 - accuracy: 0.6494 - val_loss: 0.5470 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.78049\n",
            "Epoch 27/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5572 - accuracy: 0.6524 - val_loss: 0.6728 - val_accuracy: 0.5366\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.78049\n",
            "Epoch 28/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5662 - accuracy: 0.6037 - val_loss: 0.7673 - val_accuracy: 0.3415\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.78049\n",
            "Epoch 29/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5353 - accuracy: 0.6616 - val_loss: 0.7141 - val_accuracy: 0.4268\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.78049\n",
            "Epoch 30/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5327 - accuracy: 0.6616 - val_loss: 0.6807 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.78049\n",
            "Epoch 31/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5453 - accuracy: 0.6037 - val_loss: 0.6486 - val_accuracy: 0.5488\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.78049\n",
            "Epoch 32/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5467 - accuracy: 0.6037 - val_loss: 0.6397 - val_accuracy: 0.5244\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.78049\n",
            "Epoch 33/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.6086 - accuracy: 0.5030 - val_loss: 0.7738 - val_accuracy: 0.3171\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.78049\n",
            "Epoch 34/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5995 - accuracy: 0.5244 - val_loss: 0.5147 - val_accuracy: 0.7805\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.78049\n",
            "Epoch 35/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5486 - accuracy: 0.6250 - val_loss: 0.6219 - val_accuracy: 0.6098\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.78049\n",
            "Epoch 36/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.5363 - accuracy: 0.6372 - val_loss: 0.7388 - val_accuracy: 0.3780\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.78049\n",
            "Epoch 37/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5456 - accuracy: 0.6311 - val_loss: 0.6894 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.78049\n",
            "Epoch 38/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5428 - accuracy: 0.6189 - val_loss: 0.6081 - val_accuracy: 0.6220\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.78049\n",
            "Epoch 39/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5602 - accuracy: 0.5976 - val_loss: 0.5960 - val_accuracy: 0.6951\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.78049\n",
            "Epoch 40/100\n",
            "82/82 [==============================] - 23s 277ms/step - loss: 0.5720 - accuracy: 0.5671 - val_loss: 0.5543 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.78049\n",
            "Epoch 41/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5414 - accuracy: 0.5976 - val_loss: 0.7021 - val_accuracy: 0.4390\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.78049\n",
            "Epoch 42/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5592 - accuracy: 0.6037 - val_loss: 0.7822 - val_accuracy: 0.2927\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.78049\n",
            "Epoch 43/100\n",
            "82/82 [==============================] - 22s 274ms/step - loss: 0.5328 - accuracy: 0.6524 - val_loss: 0.6747 - val_accuracy: 0.4756\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.78049\n",
            "Epoch 44/100\n",
            "82/82 [==============================] - 23s 276ms/step - loss: 0.5120 - accuracy: 0.6524 - val_loss: 0.5788 - val_accuracy: 0.6707\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.78049\n",
            "Epoch 45/100\n",
            "82/82 [==============================] - 24s 289ms/step - loss: 0.5383 - accuracy: 0.6280 - val_loss: 0.6839 - val_accuracy: 0.4634\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.78049\n",
            "Epoch 46/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.4674 - accuracy: 0.8262 - val_loss: 0.5015 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.78049 to 0.80488, saving model to ./SEfold3000000460.804878.hdf5\n",
            "Epoch 47/100\n",
            "82/82 [==============================] - 23s 280ms/step - loss: 0.4209 - accuracy: 0.8445 - val_loss: 0.4984 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.80488 to 0.82927, saving model to ./SEfold3000000470.829268.hdf5\n",
            "Epoch 48/100\n",
            "82/82 [==============================] - 23s 280ms/step - loss: 0.4088 - accuracy: 0.8537 - val_loss: 0.4747 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.82927 to 0.86585, saving model to ./SEfold3000000480.865854.hdf5\n",
            "Epoch 49/100\n",
            "82/82 [==============================] - 23s 279ms/step - loss: 0.4165 - accuracy: 0.8323 - val_loss: 0.4737 - val_accuracy: 0.8659\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.86585\n",
            "Epoch 50/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.3996 - accuracy: 0.8354 - val_loss: 0.4780 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.86585\n",
            "Epoch 51/100\n",
            "82/82 [==============================] - 23s 279ms/step - loss: 0.4070 - accuracy: 0.8232 - val_loss: 0.4836 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.86585\n",
            "Epoch 52/100\n",
            "82/82 [==============================] - 23s 279ms/step - loss: 0.3998 - accuracy: 0.8506 - val_loss: 0.4734 - val_accuracy: 0.8415\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.86585\n",
            "Epoch 53/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.3794 - accuracy: 0.8567 - val_loss: 0.4887 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.86585\n",
            "Epoch 54/100\n",
            "82/82 [==============================] - 23s 278ms/step - loss: 0.3667 - accuracy: 0.8811 - val_loss: 0.4972 - val_accuracy: 0.8049\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.86585\n",
            "Epoch 55/100\n",
            "82/82 [==============================] - 23s 279ms/step - loss: 0.3859 - accuracy: 0.8628 - val_loss: 0.5371 - val_accuracy: 0.7561\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.86585\n",
            "Epoch 56/100\n",
            "82/82 [==============================] - 23s 280ms/step - loss: 0.3821 - accuracy: 0.8659 - val_loss: 0.4601 - val_accuracy: 0.8293\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.86585\n",
            "Epoch 57/100\n",
            "82/82 [==============================] - 23s 279ms/step - loss: 0.3623 - accuracy: 0.8720 - val_loss: 0.4681 - val_accuracy: 0.8171\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.86585\n",
            "Epoch 58/100\n",
            " 3/82 [>.............................] - ETA: 20s - loss: 0.3518 - accuracy: 0.9167"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0J3YG2DUZZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def classifyEval(testX, testy,model):\n",
        "  # predict probabilities for test set  \n",
        "  yhat_probs = model.predict(testX, verbose=0)\n",
        "  yhat = np.max(yhat_probs,axis=1)\n",
        "  yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "  x=1\n",
        "  print(testy[:,x])\n",
        "  print(yhat_classes)\n",
        "  # accuracy: (tp + tn) / (p + n)\n",
        "  accuracy = accuracy_score(testy[:,x], yhat_classes)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  # precision tp / (tp + fp)\n",
        "  precision = precision_score(testy[:,x], yhat_classes)\n",
        "  print('Precision: %f' % precision)\n",
        "  # recall: tp / (tp + fn)\n",
        "  recall = recall_score(testy[:,x], yhat_classes)\n",
        "  print('Recall: %f' % recall)\n",
        "  # f1: 2 tp / (2 tp + fp + fn)\n",
        "  f1 = f1_score(testy[:,x], yhat_classes)\n",
        "  print('F1 score: %f' % f1)\n",
        " \n",
        "  # kappa\n",
        "  kappa = cohen_kappa_score(testy[:,x], yhat_classes)\n",
        "  print('Cohens kappa: %f' % kappa)\n",
        "  # ROC AUC\n",
        "  auc = roc_auc_score(testy[:,x], yhat_probs[:,x])\n",
        "  print('ROC AUC: %f' % auc)\n",
        "  # confusion matrix\n",
        "  matrix = confusion_matrix(testy[:,x], yhat_classes)\n",
        "  print(matrix)\n",
        "\n",
        "  fpr_keras, tpr_keras, thresholds_keras = sklearn.metrics.roc_curve(testy[:,x], yhat_probs[:,x])\n",
        "  return fpr_keras, tpr_keras, thresholds_keras\n",
        "  '''\n",
        "  plt.figure(1)\n",
        "  plt.plot([0, 1], [0, 1], 'k--')\n",
        "  plt.plot(fpr_keras, tpr_keras, label='CV1 (area = {:.3f})'.format(auc))\n",
        "  plt.xlabel('False positive rate')\n",
        "  plt.ylabel('True positive rate')\n",
        "  plt.title('ROC curve')\n",
        "  plt.legend(loc='best')\n",
        "  plt.show()\n",
        "  '''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acURLMezCxA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpr_list = []\n",
        "fpr_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxXwIdDaTncb",
        "colab_type": "code",
        "outputId": "7776d07c-5526-4c1b-ecc7-65c6f2498799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "X_test_extend = np.load('/content/TestFold1.npy')\n",
        "Y_test = np.load('/content/TestLabelFold1.npy')\n",
        "model = model6()\n",
        "model.load_weights('/content/SEfold1000000530.853659.hdf5')\n",
        "fpr, tpr, _ = classifyEval(X_test_extend, Y_test, model)\n",
        "fpr_list.append(fpr)\n",
        "tpr_list.append(tpr)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0.\n",
            " 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 1 0 0 1 0 0 0]\n",
            "Accuracy: 0.853659\n",
            "Precision: 0.900000\n",
            "Recall: 0.450000\n",
            "F1 score: 0.600000\n",
            "Cohens kappa: 0.522330\n",
            "ROC AUC: 0.761290\n",
            "[[61  1]\n",
            " [11  9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q99ruMRwFMuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auc = [.95,.80,.73,.84,.69]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT2ejE2XEHlN",
        "colab_type": "code",
        "outputId": "8fad6c41-dc6f-4747-da18-152a13bd537c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "for i in range(5):\n",
        "  plt.plot(fpr_list[i], tpr_list[i],label='CV{} (area = {:.2f})'.format(i,auc[i]))\n",
        "  plt.legend(loc='CV{}'.format(i))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV0'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV1'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV2'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV3'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Unrecognized location 'CV4'. Falling back on 'best'; valid locations are\n",
            "\tbest\n",
            "\tupper right\n",
            "\tupper left\n",
            "\tlower left\n",
            "\tlower right\n",
            "\tright\n",
            "\tcenter left\n",
            "\tcenter right\n",
            "\tlower center\n",
            "\tupper center\n",
            "\tcenter\n",
            "This will raise an exception in 3.3.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuERoQ6CUZZ0",
        "colab_type": "text"
      },
      "source": [
        "## Attention Model for Multi Instance Learner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtK_lHXrUZZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def featureextractor(data):\n",
        "    conv = Conv2D(20, 5, activation = None, padding = 'same')(data)\n",
        "    conv_a = Activation('relu')(BatchNormalization()(conv))\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv_a)\n",
        "    conv1 = Conv2D(50, 5, activation = None, padding = 'same')(pool1)\n",
        "    conv1_a = Activation('relu')(BatchNormalization()(conv1))\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv1_a)\n",
        "    return pool2\n",
        "\n",
        "def featureextractorx2(data):\n",
        "    dense = Dense(500,activation='relu')(data)\n",
        "    return dense\n",
        "\n",
        "def Attention(data):\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    dense = Dense(128,activation='tanh')(data)\n",
        "    return dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3miMxRUZZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import keras\n",
        "from keras.engine import Layer\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.layers import Dense,Dropout, Activation,Flatten,Conv2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "l1factor=1e-2\n",
        "l2factor=0\n",
        "\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "prediction = MaxPool(axis=1)(dense_1)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbRDRdVcYkP",
        "colab_type": "code",
        "outputId": "fab69d65-a6a7-48f0-f900-18355494f1f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIHWT6X6fPv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "class MaxPool(Layer):\n",
        "    def __init__(self, axis=-1,**kwargs):\n",
        "        self.axis=axis\n",
        "        super(MaxPool, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        pass\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        output=K.placeholder(shape=(x.shape[0],2))\n",
        "        output=K.concatenate([1-K.max(x, axis=-1,keepdims=True),K.max(x, axis=-1,keepdims=True)])\n",
        "        return output\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpUaTaZWirf0",
        "colab_type": "code",
        "outputId": "c155dade-a1b1-43bb-cb6f-7c13d55aadec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D,Flatten\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "model = keras.applications.resnet.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(256,256,3))\n",
        "Feature_maps = {'conv2_block3_out','conv3_block4_out','conv4_block6_out','conv5_block3_out'}\n",
        "Features = []\n",
        "\n",
        "logistic = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv2_block3_out').output)\n",
        "dense_1 = Flatten()(logistic)\n",
        "\n",
        "logistic1 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv3_block4_out').output)\n",
        "dense_2 = Flatten()(logistic1)\n",
        "\n",
        "logistic2 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv4_block6_out').output)\n",
        "dense_3 = Flatten()(logistic2)\n",
        "\n",
        "logistic3 = Conv2D(1, 1, activation = 'sigmoid')(model.get_layer('conv5_block3_out').output)\n",
        "dense_4 = Flatten()(logistic3)\n",
        "\n",
        "final = concatenate([dense_1,dense_2,dense_3,dense_4])\n",
        "prediction = MaxPool(axis=1)(final)\n",
        "prediction = Activation(\"softmax\",name=\"softmax\")(prediction)\n",
        "model = Model(input=model.input, output=prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FwRbn0eiwib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "noises=50\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=45.0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False) \n",
        "datagen.fit(X_train_extend)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bg_PaepkBhP",
        "colab_type": "code",
        "outputId": "9567289f-268b-42bd-9eda-a9d2e29f67ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(lr=0.00001),\n",
        "              metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(X_train_extend, Y_train,batch_size=2),\n",
        "                        nb_epoch=100,\n",
        "                        validation_data=(X_valid_extend, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=(array([[[..., epochs=100)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "134/134 [==============================] - 35s 264ms/step - loss: 0.6514 - accuracy: 0.7127 - val_loss: 0.8838 - val_accuracy: 0.2667\n",
            "Epoch 2/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.6267 - accuracy: 0.7425 - val_loss: 0.7636 - val_accuracy: 0.4333\n",
            "Epoch 3/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.6024 - accuracy: 0.7836 - val_loss: 0.6158 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.5672 - accuracy: 0.8507 - val_loss: 0.5682 - val_accuracy: 0.8500\n",
            "Epoch 5/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5584 - accuracy: 0.8433 - val_loss: 0.5442 - val_accuracy: 0.8167\n",
            "Epoch 6/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.5589 - accuracy: 0.8097 - val_loss: 0.5946 - val_accuracy: 0.8333\n",
            "Epoch 7/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5288 - accuracy: 0.8545 - val_loss: 0.5800 - val_accuracy: 0.7667\n",
            "Epoch 8/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5151 - accuracy: 0.8582 - val_loss: 0.5377 - val_accuracy: 0.8500\n",
            "Epoch 9/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5261 - accuracy: 0.8358 - val_loss: 0.5270 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5130 - accuracy: 0.8470 - val_loss: 0.5216 - val_accuracy: 0.8500\n",
            "Epoch 11/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4840 - accuracy: 0.8731 - val_loss: 0.5267 - val_accuracy: 0.8167\n",
            "Epoch 12/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4953 - accuracy: 0.8619 - val_loss: 0.5123 - val_accuracy: 0.8167\n",
            "Epoch 13/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.5000 - accuracy: 0.8433 - val_loss: 0.4984 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4907 - accuracy: 0.8507 - val_loss: 0.4877 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4814 - accuracy: 0.8694 - val_loss: 0.4697 - val_accuracy: 0.8833\n",
            "Epoch 16/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4705 - accuracy: 0.8806 - val_loss: 0.4655 - val_accuracy: 0.8833\n",
            "Epoch 17/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4634 - accuracy: 0.8843 - val_loss: 0.4711 - val_accuracy: 0.8500\n",
            "Epoch 18/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4425 - accuracy: 0.9030 - val_loss: 0.4611 - val_accuracy: 0.8833\n",
            "Epoch 19/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4562 - accuracy: 0.8843 - val_loss: 0.4742 - val_accuracy: 0.8500\n",
            "Epoch 20/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4466 - accuracy: 0.8881 - val_loss: 0.4702 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4490 - accuracy: 0.8843 - val_loss: 0.4764 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4406 - accuracy: 0.8993 - val_loss: 0.4772 - val_accuracy: 0.8500\n",
            "Epoch 23/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9067 - val_loss: 0.4578 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.4504 - accuracy: 0.8881 - val_loss: 0.4631 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4518 - accuracy: 0.8806 - val_loss: 0.5093 - val_accuracy: 0.8000\n",
            "Epoch 26/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4471 - accuracy: 0.8769 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
            "Epoch 27/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4238 - accuracy: 0.9104 - val_loss: 0.4399 - val_accuracy: 0.9000\n",
            "Epoch 28/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4114 - accuracy: 0.9254 - val_loss: 0.4405 - val_accuracy: 0.8833\n",
            "Epoch 29/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.4215 - accuracy: 0.9067 - val_loss: 0.4320 - val_accuracy: 0.9000\n",
            "Epoch 30/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4280 - accuracy: 0.8918 - val_loss: 0.4559 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4124 - accuracy: 0.9179 - val_loss: 0.4489 - val_accuracy: 0.8833\n",
            "Epoch 32/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.4201 - accuracy: 0.9067 - val_loss: 0.4370 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4097 - accuracy: 0.9104 - val_loss: 0.4642 - val_accuracy: 0.8500\n",
            "Epoch 34/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4228 - accuracy: 0.8993 - val_loss: 0.4591 - val_accuracy: 0.8500\n",
            "Epoch 35/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4080 - accuracy: 0.9179 - val_loss: 0.4386 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4136 - accuracy: 0.8993 - val_loss: 0.4663 - val_accuracy: 0.8500\n",
            "Epoch 37/100\n",
            "134/134 [==============================] - 17s 124ms/step - loss: 0.4195 - accuracy: 0.8955 - val_loss: 0.4612 - val_accuracy: 0.8667\n",
            "Epoch 38/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4086 - accuracy: 0.9067 - val_loss: 0.4633 - val_accuracy: 0.8333\n",
            "Epoch 39/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4060 - accuracy: 0.9179 - val_loss: 0.4412 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4010 - accuracy: 0.9142 - val_loss: 0.4987 - val_accuracy: 0.8167\n",
            "Epoch 41/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4112 - accuracy: 0.9104 - val_loss: 0.4959 - val_accuracy: 0.8000\n",
            "Epoch 42/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3995 - accuracy: 0.9254 - val_loss: 0.4676 - val_accuracy: 0.8500\n",
            "Epoch 43/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3979 - accuracy: 0.9142 - val_loss: 0.5070 - val_accuracy: 0.8000\n",
            "Epoch 44/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.4013 - accuracy: 0.9216 - val_loss: 0.4608 - val_accuracy: 0.8667\n",
            "Epoch 45/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3892 - accuracy: 0.9254 - val_loss: 0.4543 - val_accuracy: 0.8833\n",
            "Epoch 46/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3986 - accuracy: 0.9216 - val_loss: 0.4524 - val_accuracy: 0.8667\n",
            "Epoch 47/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3882 - accuracy: 0.9328 - val_loss: 0.4398 - val_accuracy: 0.8833\n",
            "Epoch 48/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3867 - accuracy: 0.9366 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 49/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3776 - accuracy: 0.9440 - val_loss: 0.4824 - val_accuracy: 0.8333\n",
            "Epoch 50/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3888 - accuracy: 0.9291 - val_loss: 0.4923 - val_accuracy: 0.8167\n",
            "Epoch 51/100\n",
            "134/134 [==============================] - 17s 125ms/step - loss: 0.3899 - accuracy: 0.9291 - val_loss: 0.4734 - val_accuracy: 0.8333\n",
            "Epoch 52/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.4009 - accuracy: 0.9179 - val_loss: 0.4710 - val_accuracy: 0.8333\n",
            "Epoch 53/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3773 - accuracy: 0.9440 - val_loss: 0.5002 - val_accuracy: 0.8000\n",
            "Epoch 54/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3923 - accuracy: 0.9291 - val_loss: 0.4546 - val_accuracy: 0.8667\n",
            "Epoch 55/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3771 - accuracy: 0.9403 - val_loss: 0.4748 - val_accuracy: 0.8500\n",
            "Epoch 56/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3930 - accuracy: 0.9291 - val_loss: 0.4804 - val_accuracy: 0.8167\n",
            "Epoch 57/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3837 - accuracy: 0.9366 - val_loss: 0.4670 - val_accuracy: 0.8333\n",
            "Epoch 58/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3695 - accuracy: 0.9515 - val_loss: 0.4369 - val_accuracy: 0.8833\n",
            "Epoch 59/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3830 - accuracy: 0.9328 - val_loss: 0.4340 - val_accuracy: 0.8833\n",
            "Epoch 60/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3775 - accuracy: 0.9403 - val_loss: 0.4542 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3674 - accuracy: 0.9515 - val_loss: 0.4400 - val_accuracy: 0.8833\n",
            "Epoch 62/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3747 - accuracy: 0.9403 - val_loss: 0.4501 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3693 - accuracy: 0.9478 - val_loss: 0.4781 - val_accuracy: 0.8167\n",
            "Epoch 64/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3625 - accuracy: 0.9552 - val_loss: 0.4351 - val_accuracy: 0.8833\n",
            "Epoch 65/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3838 - accuracy: 0.9291 - val_loss: 0.4327 - val_accuracy: 0.8833\n",
            "Epoch 66/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9478 - val_loss: 0.4663 - val_accuracy: 0.8333\n",
            "Epoch 67/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9515 - val_loss: 0.4767 - val_accuracy: 0.8500\n",
            "Epoch 68/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3714 - accuracy: 0.9440 - val_loss: 0.4532 - val_accuracy: 0.8500\n",
            "Epoch 69/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3763 - accuracy: 0.9366 - val_loss: 0.4743 - val_accuracy: 0.8500\n",
            "Epoch 70/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3589 - accuracy: 0.9552 - val_loss: 0.4877 - val_accuracy: 0.8167\n",
            "Epoch 71/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3560 - accuracy: 0.9664 - val_loss: 0.4616 - val_accuracy: 0.8500\n",
            "Epoch 72/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3646 - accuracy: 0.9552 - val_loss: 0.4886 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3543 - accuracy: 0.9627 - val_loss: 0.5030 - val_accuracy: 0.8000\n",
            "Epoch 74/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3562 - accuracy: 0.9627 - val_loss: 0.5077 - val_accuracy: 0.8000\n",
            "Epoch 75/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3523 - accuracy: 0.9664 - val_loss: 0.4707 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3576 - accuracy: 0.9590 - val_loss: 0.4801 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3603 - accuracy: 0.9552 - val_loss: 0.5191 - val_accuracy: 0.8000\n",
            "Epoch 78/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3669 - accuracy: 0.9440 - val_loss: 0.4686 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3591 - accuracy: 0.9515 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3708 - accuracy: 0.9403 - val_loss: 0.4465 - val_accuracy: 0.8500\n",
            "Epoch 81/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3495 - accuracy: 0.9701 - val_loss: 0.4657 - val_accuracy: 0.8500\n",
            "Epoch 82/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3623 - accuracy: 0.9552 - val_loss: 0.4355 - val_accuracy: 0.8833\n",
            "Epoch 83/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3573 - accuracy: 0.9515 - val_loss: 0.4195 - val_accuracy: 0.9000\n",
            "Epoch 84/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3440 - accuracy: 0.9739 - val_loss: 0.4263 - val_accuracy: 0.8833\n",
            "Epoch 85/100\n",
            "134/134 [==============================] - 17s 126ms/step - loss: 0.3515 - accuracy: 0.9664 - val_loss: 0.5191 - val_accuracy: 0.7833\n",
            "Epoch 86/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9627 - val_loss: 0.4628 - val_accuracy: 0.8500\n",
            "Epoch 87/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3695 - accuracy: 0.9440 - val_loss: 0.4856 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3635 - accuracy: 0.9515 - val_loss: 0.4759 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3463 - accuracy: 0.9664 - val_loss: 0.5083 - val_accuracy: 0.8000\n",
            "Epoch 90/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3585 - accuracy: 0.9515 - val_loss: 0.4583 - val_accuracy: 0.8333\n",
            "Epoch 91/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3517 - accuracy: 0.9590 - val_loss: 0.4490 - val_accuracy: 0.8667\n",
            "Epoch 92/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3496 - accuracy: 0.9701 - val_loss: 0.4756 - val_accuracy: 0.8333\n",
            "Epoch 93/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3564 - accuracy: 0.9590 - val_loss: 0.4643 - val_accuracy: 0.8500\n",
            "Epoch 94/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3498 - accuracy: 0.9590 - val_loss: 0.4785 - val_accuracy: 0.8333\n",
            "Epoch 95/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3492 - accuracy: 0.9627 - val_loss: 0.4788 - val_accuracy: 0.8000\n",
            "Epoch 96/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3438 - accuracy: 0.9664 - val_loss: 0.4847 - val_accuracy: 0.8167\n",
            "Epoch 97/100\n",
            "134/134 [==============================] - 17s 129ms/step - loss: 0.3446 - accuracy: 0.9739 - val_loss: 0.5205 - val_accuracy: 0.7833\n",
            "Epoch 98/100\n",
            "134/134 [==============================] - 17s 130ms/step - loss: 0.3442 - accuracy: 0.9664 - val_loss: 0.4866 - val_accuracy: 0.8167\n",
            "Epoch 99/100\n",
            "134/134 [==============================] - 17s 127ms/step - loss: 0.3369 - accuracy: 0.9813 - val_loss: 0.4866 - val_accuracy: 0.8333\n",
            "Epoch 100/100\n",
            "134/134 [==============================] - 17s 128ms/step - loss: 0.3466 - accuracy: 0.9701 - val_loss: 0.4513 - val_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fb84798d4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeGlE_Oz-RjS",
        "colab_type": "code",
        "outputId": "f36f1173-8550-4982-e5fe-20d4af123438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = model5()\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_14 (InputLayer)           (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 262, 262, 3)  0           input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 128, 128, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 128, 128, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 128, 128, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 130, 130, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 64, 64, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 64, 64, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 64, 64, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 64, 64, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 64, 64, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 64, 64, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 64, 64, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 64, 64, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 64, 64, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 64, 64, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 64, 64, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 64, 64, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 64, 64, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 64, 64, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 64, 64, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 64, 64, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 64, 64, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 64, 64, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 64, 64, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 64, 64, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 32, 32, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 32, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 32, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 32, 32, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 32, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 32, 32, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 32, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 32, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 32, 32, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 32, 32, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 32, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 32, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 32, 32, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 32, 32, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 32, 32, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 32, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 32, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 32, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 32, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 32, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 32, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 32, 32, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 32, 32, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 16, 16, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 16, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 16, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 16, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 16, 16, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 16, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 16, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 16, 16, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 16, 16, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 16, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 16, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 16, 16, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 16, 16, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 16, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 16, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 16, 16, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 16, 16, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 16, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 16, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 16, 16, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 16, 16, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 16, 16, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 16, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 16, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 16, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 16, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 16, 16, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 16, 16, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 16, 16, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 16, 16, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 8, 8, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 8, 8, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 8, 8, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 8, 8, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 8, 8, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 8, 8, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 8, 8, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 8, 8, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 8, 8, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 8, 8, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 8, 8, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 8, 8, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 8, 8, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 8, 8, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 8, 8, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 8, 8, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 8, 8, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 8, 8, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 64, 64, 1)    257         conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 32, 32, 1)    513         conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 16, 16, 1)    1025        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 1)      2049        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_53 (Flatten)            (None, 4096)         0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_54 (Flatten)            (None, 1024)         0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_55 (Flatten)            (None, 256)          0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_56 (Flatten)            (None, 64)           0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 5440)         0           flatten_53[0][0]                 \n",
            "                                                                 flatten_54[0][0]                 \n",
            "                                                                 flatten_55[0][0]                 \n",
            "                                                                 flatten_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pool_14 (MaxPool)           (None, 2)            0           concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 2)            0           max_pool_14[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 23,591,556\n",
            "Trainable params: 23,538,436\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:42: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"so...)`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfcXjhymHnt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "9bd07f15-6d30-4b6d-a9f6-724c74505722"
      },
      "source": [
        "from skimage.transform import resize\n",
        "from matplotlib import pyplot as plt\n",
        "image = np.load('TestFold1.npy')\n",
        "plt.imshow(image[0])\n",
        "plt.show()\n",
        "image_resized = resize(image, (image.shape[0] , image.shape[1] // 2,image.shape[2] // 2,image.shape[3]),\n",
        "                       anti_aliasing=True)\n",
        "plt.imshow(image_resized[0])\n",
        "plt.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGMo-r-6SbE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}